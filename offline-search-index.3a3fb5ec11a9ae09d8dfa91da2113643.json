[{"body":"What is a container : https://www.docker.com/resources/what-container\nWhat is container orchestration : https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s) : https://kubernetes.io/\nDocker : https://www.docker.com/\nUnderstanding CSI : https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container : https://www.docker.com/resources/what-container …","ref":"/storage-plugin-docs/docs/grasp/start/","title":"Getting Started"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Driver PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current version v1.4 v1.4 v1.2 v1.3 v1.5   Older Versions v1.3 v1.3 v1.1 v1.2 v1.4    Architecture Features and capabilities Supported Platforms   Features PowerMax PowerFlex/VxFlex OS Unity PowerScale/Isilon PowerStore     Kubernetes 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19   RHEL 7.7, 7.8, 7.9 7.7, 7.8, 7.9 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.7, 7.8, 7.9   Ubuntu 20.04 20.04 20.04 20.04 20.04   CentOS 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8   SLES no 15SP2 no no no   OpenShift 4.5, 4.6 4.5, 4.6 4.5, 4.6 4.5, 4.6 4.5, 4.6   Docker EE 3.1 3.1 3.1 3.1 3.1   Google Anthos 1.5 no no no 1.5    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO/RWX/ROX RWO RWO(FC/iSCSI)\nRWO/RWX/ROX(NFS)\nRWO/RWX/ROX(Raw block FC and iSCSI) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no no yes yes yes   Topology yes yes yes yes yes   Multi-array yes (via Unisphere) no yes (with single driver) no no    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering\nNFS host IO size\nSnapshot retention duration Access Zone\nNFS version (3 or 4) iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI enabled …","ref":"/storage-plugin-docs/docs/dell-csi-driver/","title":"Introduction"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.2 to v1.3 using Helm Steps\n Run git clone https://github.com/dell/csi-vxflexos.git to clone the git repository and get the v1.3 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Update Driver from pre-v1.2 to v1.3 using Helm A direct upgrade of the driver from an older version pre-v1.2 to version 1.3 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command. Delete any VolumeSnapshotClass present in the cluster. Delete all the alpha snapshot CRDs from the cluster by running the following commands: kubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io  Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace vxflexos. Install the driver using the steps described in the Installation Using Helm section for the CSI PowerFlex driver.  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.3 driver is not supported on Kubernetes upstream clusters running version 1.16. You must upgrade your cluster to 1.17, 1.18, or 1.19 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option –upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/storage-plugin-docs/docs/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.4 to v1.5 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.5 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  Update Driver from pre-v1.4 to v1.5 using Helm A rolling upgrade of the driver from an older version to v1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n  Delete any alpha VolumeSnapshot or VolumeSnapshotContent in the cluster.\n  Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command.\n  Delete any VolumeSnapshotClass present in the cluster.\n  Delete all the alpha snapshot CRDs from the cluster by running the following commands:\nkubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io   Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e where driver-namespace is the namespace where driver is installed.\n  Install the driver using the steps described in the Installation Using Helm section for the CSI PowerMax driver.\n  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerMax version 1.5 driver is not supported on Kubernetes upstream clusters running Kubernetes version 1.16 or lower. You must upgrade your cluster to 1.17, 1.18, or 1.19 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option –upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or …","ref":"/storage-plugin-docs/docs/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver from version v1.3.0.1 to v1.4 Steps\n  Upgrade the Kubernetes to Kubernetes 1.17.x version first before upgrading CSI driver.\n  Uninstall CSI Driver for DELL EMC PowerScale v1.3.0.\n  Verify that all pre-requisites to install CSI Driver for DELL EMC PowerScale v1.4.0 are fulfilled.\n  Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the DELL EMC PowerScale cd dell-csi-helm-installer\n  Install the CSI Driver for DELL EMC PowerScale v1.4.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml \n  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/storage-plugin-docs/docs/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.1 to v1.2 using Helm Steps\n Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.2 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/storage-plugin-docs/docs/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpdate Driver from v1.3 to v1.4 using Helm Steps\n Run git clone https://github.com/dell/csi-unity.git to clone the git repository and get the v1.3 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace unity --values ./my-values.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/storage-plugin-docs/docs/upgradation/drivers/unity/","title":"Unity"},{"body":"Installation information for all the drivers can be found in the individual drivers page in this section\n","excerpt":"Installation information for all the drivers can be found in the …","ref":"/storage-plugin-docs/docs/installation/","title":"Installation"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/storage-plugin-docs/docs/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n  Search DellEMC in storage category in Operatorhub.io.   Click DellEMC Operator.   Check the desired version is selected and click Install. Follow the provided instructions.   Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on …","ref":"/storage-plugin-docs/docs/partners/operator/","title":"OperatorHub.io"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n  Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.   Check the version you want to install from the list, you can check the details by clicking it.   Once selected, click “Install” to proceed with installation process.   You can verify the list of available operators by selecting “Installed Operator” section.   Select the Dell CSI Operator to get further description.   Install CSI Drivers via Operator Steps\n  Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.   After clicking “Create CSIUnity” option in above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.   You can check the driver installed and node and controller pods running in the Pods section under Workloads.   ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/storage-plugin-docs/docs/partners/redhat/","title":"Red Hat OpenShift"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the PowerScale driver:\n./csi-uninstall.sh --namespace isilon/\u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall a PowerFlex driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-name and driver-namespace with their respective values $ kubectl delete vxflexos/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, …","ref":"/storage-plugin-docs/docs/uninstall/","title":"Uninstallation"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/upgradation/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/features/","title":"Features"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\n For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page. For installing via OpenShift with the certified Operator, go to the OpenShift page. For installing manually, follow the instructions below.  Manual Installation Pre-requisites Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes cluster v1.17, v1.18, v1.19 OpenShift Clusters 4.5, 4.6 with RHEL 7.x \u0026 RHCOS worker nodes For upstream k8s clusters, make sure to install  Beta VolumeSnapshot CRDs (can be installed using the Operator installation script) External Volume Snapshot Controller     Note- For more insights or detailed pre-requisites refer https://github.com/dell/dell-csi-operator\n Steps  Clone the Dell CSI Operator repository Run ‘bash scripts/install.sh’ to install the operator  Run the command ‘oc get pods’ to validate the install completed  Should be able to see the operator related pod on default namespace     Driver Install via Dell CSI Operator For information on how to install the CSI drivers via the Dell CSI Operator, please refer to the sub-pages below for each driver.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/storage-plugin-docs/docs/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/blog/news/","title":"News About Docsy"},{"body":"The quaterly update for Dell CSI Driver is there !\n New features  Across portfolio Volume Cloning Volume Expansion online and offline Raw Block Support RedHat CoreOS Docker EE 3.1 Dell CSI Operator CSI Driver for PowerMax CSI Driver for PowerStore CSI Driver for PowerFlex   One more thing ; Ansible for PowerStore v1.1 Useful links  New features Across portfolio This release gives for every driver the :\n Support of OpenShift 4.4 as well as Kubernetes 1.17, 1.18, 1.19 Support for Kubernetes Volume Snapshot Beta API New installer !  With Volume Snapshot’s promotion to beta, one significant change is the CSI external-snapshotter sidecar has been split into two controllers, a common snapshot controller and a CSI external-snapshotter sidecar.\nThe new install script available under dell-csi-helm-installer/csi-install.sh will :\n By default, install of the external-snaphotter for CSI driver. Optionally, install the beta snapshot CRD when the option --snapshot-crd is set during the initial installation.  Most recent Kubernetes distributions like OpenShift or GKE come with the common snapshotter controller installed.\nFor Kubernetes vanilla, you have to deploy the common snapshotter manually. The instructions are available here.\n /!\\ The drivers have validated the external-snapshotter version 1.2 and not the bleeding-edge version\n Volume Cloning Volume cloning is now available for every driver, but PowerFlex (that feature is on the roadmap).\nIt never has been easier [to spin a new environement from the production](({% post_url {% post_url note/dell/2020-05-29-gitlab-powermax %})).\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clone-pvc-0spec:accessModes:- ReadWriteOnceresources:requests:storage:5GistorageClassName:powermaxdataSource:kind:PersistentVolumeClaimname:pvc-0In the PVC definition you must make sure the source of the clone has the same storageClassName, request.storage size, namespace and accessModes.\nVolume Expansion online and offline That feature was already present in csi-powerscale ; it is now available for every Dell CSI driver.\nTo expand a volume, you just have to edit the PV size ; blazing fast example below: {: .size-small}\nRaw Block Support The Raw Block Support was already available with csi-powermax ; it is now available in csi-vxflexos and csi-powerstore.\nThat feature can be used if your application needs a filesystem different from xfs or ext4 or applications that can take advantage of a block device (like HDFS, Oracle ASM, etc.).\nRedHat CoreOS But for PowerFlex, every driver has been qualified with OpenShift 4.3 and 4.4 on CoreOS type of nodes !\nDocker EE 3.1 Docker Enterprise Edition (now part of Mirantis) makes his appearance to the list of officially supported by support.dell.com Kubernetes distributions.\nThe first drivers to qualify Docker EE are : csi-powerscale, csi-unity and csi-vxflexos.\nDell CSI Operator The dell-csi-operator adds support for the installation of the csi-powerstore and the multi-array support for csi-unity.\n At the moment of the publication, the new operator is under the RedHat certification process to get official support. The version 1.1 is not available yet in OperatorHub.io or OpenShift UI. Stay tuned for the update.\n CSI Driver for PowerMax Upon installation, we can enable the CSI PowerMax Reverse Proxy service. The CSI PowerMax Reverse Proxy is a reverse proxy that forwards CSI driver requests to Unisphere servers.\nIt can be used to improve reliability by having redundant Unisphere, or scale-up the number of requests to be sent to Unisphere and the managed PowerMax arrays.\nCSI Driver for PowerStore The csi-powerstore adds NFS to the list of supported protocols. It has all the features that iSCSI and Fiber Channel storage classes have.\nIf you need concurrent filesystem access (i.e. ReadWriteMany access mode) you can use the NFS protocol.\nCSI Driver for PowerFlex The csi-vxflexos is the first driver to bring topology support. It avoids the driver tried to mount a volume when the SDC is not installed (I see you non-CoreOS support ;-))\nOne more thing ; Ansible for PowerStore v1.1 The biggest “hot new feature” is the support for file operation in ansible-powerstore; this means we have access to new modules for:\n File system Snapshot File system NAS server NFS export SMB Share Quota  And of course all the modules conform to Ansible Idempotency requirement.\nUseful links For more details you can check :\n The product guides and release notes in the repositories for csi-powermax, csi-powerscale, csi-powerstore, csi-unity, csi-vxflexos and ansible-powerstore. The FAQs on on Dell container community website:  FAQ for CSI Driver for PowerMax FAQ for CSI Driver for PowerScale FAQ for CSI Driver for PowerStore FAQ for CSI Driver for PowerFlex FAQ for CSI Driver for Unity    ","excerpt":"The quaterly update for Dell CSI Driver is there !\n New features …","ref":"/storage-plugin-docs/blog/2020/09/28/csi-drivers-volume-expansion-and-beta-snapshot-support-update/","title":"CSI drivers Volume expansion and beta Snapshot support update !"},{"body":"Every quarter Dell Technologies ships new versions of his CSI Drivers and Ansible modules.\n Dell EMC has anounced new set of CSI Drivers for their storage arrays. Some highliths for these June 2020 releases:\n Qualifications for OpenShift 4.3 and Kubernetes 1.16 for all the drivers Easy upgrade with the CSI Operator for all the drivers Helm 3 support for all the drivers Multi-array support for PowerMax and Unity NFS support for Unity Volume expansion for Isilon Volume cloning for PowerMax CHAP for PowerMax   For the Ansible modules you will have:\n a brand new Ansible module for Unity ! Ansible for Isilon v1.1 brings support for SmartQuotas and is compatible with next OneFS major version.   For more details you can check :\n The product guides and release notes in the repositories for csi-powermax v1.3, csi-isilon v1.2, csi-unity v1.2, csi-vxflexos v1.1.5, ansible-unity v1.0 and ansible-isilon v1.1 The FAQs on on Dell container community website:  FAQ for CSI Driver for PowerMax FAQ for CSI Driver for Unity FAQ for CSI Driver for VxFlexOS FAQ for CSI Driver for Isilon FAQ for Ansible Isilon    ","excerpt":"Every quarter Dell Technologies ships new versions of his CSI Drivers …","ref":"/storage-plugin-docs/blog/2020/06/15/june-2020-dell-storage-enablers-big-update/","title":"June 2020 Dell storage enablers big update !"},{"body":"PowerMax is now supported by Google Anthos 1.3\nKeep in mind the snapshot alpha version is not supported in GKE.\nMore details on the support matrix and installation steps, check the official announcement on Dell container community website.\n","excerpt":"PowerMax is now supported by Google Anthos 1.3\nKeep in mind the …","ref":"/storage-plugin-docs/blog/2020/05/27/anthos-1.3-qualification-for-powermax/","title":"Anthos 1.3 Qualification for PowerMax"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/storage-plugin-docs/docs/archives/","title":"Archives"},{"body":"This is the blog section. It has two categories: News and Releases.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\n","ref":"/storage-plugin-docs/blog/","title":"Blog"},{"body":" Welcome to Dell Technologies CSI documentation! Learn More          ","excerpt":" Welcome to Dell Technologies CSI documentation! Learn More          ","ref":"/storage-plugin-docs/","title":"Dell Technologies"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Docker Universal Control Plane (UCP).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on UCP backed clusters may run any of the OSs which we support with upstream clusters.\nDocker EE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/storage-plugin-docs/docs/partners/docker/","title":"Docker EE"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple linux based systems may be required to create and process an offline bundle for use.\n One linux based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One linux based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repos in order to create and offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking an offline bundle and preparing for installation Perform either a Helm installation or Operator installation  Building an offline bundle This needs to be performed on a linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nThe build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking an offline bundle and preparing for installation This needs to be performed on a linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to a image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing a offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 changing: dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R changing: dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R changing: dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R changing: dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R changing: dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R changing: dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R changing: dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R changing: dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R changing: quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 changing: quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 changing: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 changing: quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 changing: quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 changing: quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 changing: quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Complete Perform either a Helm installation or Operator installation Now that the required images have been made available and the Helm Charts/Operator configuration updated, installation can proceed by following the usual installation procedure as documented.\n","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/storage-plugin-docs/docs/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"Release Notes - Dell CSI Operator 1.2.0  Note: There is a delay in Operator 1.2.0 certification hence it will not be visible in OperatorHub.io and Red Hat OpenShift certified catalogue.\n New Features/Changes  Added support for OpenShift 4.5, 4.6 with RHEL and CoreOS worker nodes Migrated to Operator SDK 1.0 Added support for Ubuntu 20.04 Added support for Controller high availability (multiple-controllers) Added Topology support Added support for CSI Ephemeral Inline Volumes  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no Known issues in this release.\nSupport The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.2.0  Note: There is a delay in …","ref":"/storage-plugin-docs/docs/release/operator/","title":"Operator"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver version 1.2 and later support beta snapshots. Earlier versions of the driver supported alpha snapshots.\nVolume Snapshots feature in Kubernetes has moved to beta in Kubernetes version 1.17. It was an alpha feature in earlier releases (1.13 onwards). The snapshot API version has changed from v1alpha1 to v1beta1 with this migration.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI PowerFlex 1.3 driver, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, that is, when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIn case you are creating more storage classes, make sure that this attribute is set to true if you wish to expand any Persistent Volumes created using these new storage classes.\nFollowing is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the closest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce , ReadWriteMany , and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nTopology Support The CSI PowerFlex driver version 1.2 and later support Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer defined topology, users cannot create their own labels for nodes and storage classed and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that pod scheduling takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If controller count is greater than the number of available nodes, excess controller pods will be stuck in pending state.\n If you’re using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nAutomated SDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Red Hat CoreOS (RHCOS) nodes in an OpenShift cluster. Only RHCOS is supported at this time. The deployment of the SDC kernel module on RHCOS nodes is done via an init container. Automated installation is supported in both via Helm and Dell CSI Operator based installs. The following describes further details of this feature:\n On RHCOS nodes, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the node. If there is a SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On non-RHCOS nodes, the SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC There is no automated uninstall of SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from node.  ","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver version 1.2 and later …","ref":"/storage-plugin-docs/docs/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 Enable Zero Padding on PowerFlex Configure Mount propagation on container runtime (i.e. Docker) Install PowerFlex Storage Data Client Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. See Dell EMC PowerFlex documentation for more information to configure this setting.\nConfigure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerFlex. The following is instruction on how to do this with Docker. If you use another container runtime, follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Note: Some distribution, like Ubuntu, already has MountFlags set by default\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all worker nodes. If installing on Red Hat CoreOS (RHCOS) nodes on OpenShift you can install using the automated SDC deployment feature. If installing on non-RHCOS nodes, you must install SDC manually.\nAutomatic SDC Deployment The automated deployment of the SDC runs by default when installing the driver. It installs an SDC container to faciliate the installation. While the install is automated there are a few configuration options for this feature. Those are referenced in the Install the Driver section. More details on how the automatic SDC deployment works can be found in the Feature section of this site on the PowerFlex page.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC ftp site, which is setup by default. Some users might want to mirror this repository to a local location. The PowerFlex documentation has instructions on how to do this. If a mirror is used, you need to create an SDC repo secret for managing the credentials to the mirror. Details on how to create the secret are in the Install the Driver section.\nManually SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.    Volume Snapshot requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nYou can also install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script. If you are installing the driver using the Dell CSI Operator, there is a helper script provided to install the snapshot CRDs - scripts/install_snap_crds.sh.\nVolume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository. Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one. Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to new image. Edit the helm/secret.yaml, point to correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername \u0026 mypassword are credentials for a user with PowerFlex priviledges.\n Create the secret by running kubectl create -f secret.yaml If installing on OpenShift/RHCOS nodes,  Check the SDC container image is the correct version for your version of PowerFlex. If using the optional SDC repo mirror, you need to create a secret for the repo credentials and provide the URL for the repo.  To create the secret, you must update the details in helm/sdc-repo-secret.yaml file and running kubectl create -f sdc-repo-secret.yaml. To set the repo URL, you must set the repoUrl parameter in the myvalues.yaml file.     Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the myvalues.yaml file.  NOTE: Your SDC might have multiple VxFlex OS systems registered. Ensure that you choose the correct values.   Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:     Parameter Description Required Default     systemName Set to the PowerFlex/VxFlex OS system name or system ID to be used with the driver. Yes “systemname”   restGateway Set to the URL of your system’s REST API Gateway. You can obtain this value from the PowerFlex administrator. Yes “https://123.0.0.1”   storagePool Set to a default (existing) storage pool name in your PowerFlex/VxFlex OS system. Yes “sp”   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No “k8s”   controllerCount Set to deploy multiple controller instances. If controller count is greater than the number of available nodes, excess pods will be left in pending state. You can increase number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   StorageClass Helm charts create a Kubernetes StorageClass while deploying CSI Driver for Dell EMC PowerFlex. This section includes relevant variables. - -   name Defines the name of the Kubernetes storage class that the Helm charts will create. For example, the vxflexos base name will be used to generate names such as vxflexos and vxflexos-xfs. No “vxflexos”   isDefault Sets the newly created storage class as default for Kubernetes. Set this value to true only if you expect PowerFlex to be your principle storage provider, as it will be used in PersitentVolumeClaims where no storageclass is provided. After installation, you can add custom storage classes, if desired. No TRUE   reclaimPolicy Defines whether the volumes will be retained or deleted when the assigned pod is destroyed. The valid values for this variable are Retain or Delete. No “Delete”   controller This section allows configuration of controller specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \" \"   monitor This section allows configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   sdcKernelMirror [RHCOS only] The PowerFlex SDC may need to pull a new module that is known to work with newer Linux kernels. The default location of this mirror os at ftp.emc.com. The PowerFlex documentation has instructions for methods to mirror this repository to a local location if necessary. - -   repoUrl Set the URL of the ftp mirror containing SDC kernel modules. Only ftp locations are allowed. A blank string signifies the default mirror, which is “ftp://ftp.emc.com”. No \" \"   11. Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml       NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script also runs the verify.sh script that is present in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. You can also skip the verification step by specifiying the --skip-verify-node option. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: Automated SDC Deployment for Operator  This applies to OpenShift with RHCOS Nodes Only. This feature deploys the sdc kernel modules on CoreOS nodes with the help of an init container. Required: MDM value need to be provided in CR file for the sdc init container to work. Expect error if not in proper format. To use a specific image from ftp site, pass in repo url, repo password and repo username.  Repo username and repo password are to be encrypted by a secret and passed in. Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml.   Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflexos_v130_ops_46.yaml #sideCars:# Uncomment the following section if you want to run the monitoring sidecar# - name: sdc-monitor# envs:# - name: HOST_PID# value: \"1\"initContainers:- image:dellemc/sdc:3.6.0.176-3.5.1000.176imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.xx.xx.xx,10.xx.xx.xx\"Install Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace. Create PowerFlex credentials: Create a file called vxflexos-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:vxflexos-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f vxflexos-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerFlex using the sample files provided here . Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If controller pods is greater than number of available nodes, excess pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_SYSTEMNAME Defines the name of the PowerFlex system from which volumes will be provisioned. This must either be set to the PowerFlex system name or system ID Yes systemname   X_CSI_VXFLEXOS_ENDPOINT Defines the PowerFlex REST API endpoint, with full URL, typically leveraging HTTPS. You must set this for your PowerFlex installations REST gateway Yes https://127.0.0.1   CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   StorageClass parameters      storagePool Defines the PowerFlex storage pool from which this driver will provision volumes. You must set this for the primary storage pool to be used Yes pool1   allowVolumeExpansion Once the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No true   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the X_CSI_VXFLEXOS_SYSTEMNAME in the key with the actual systemname value No X_CSI_VXFLEXOS_SYSTEMNAME   initContainers:value Set the MDM IP’s here if installing on CoreOS to enable automatic SDC installation Yes (OpenShift) “10.xx.xx.xx,10.xx.xx.xx”     Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/docs/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use one of the pre-defined storage classes created by the CSI Driver for Dell EMC PowerFlex installation process. This step yields a mounted ext4 file system. You can see the storage class definitions in the PowerFlex installation helm chart files storageclass.yaml and storageclass-xfs.yaml. If you compare pvol0.yaml and pvol1.yaml , you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion: snapshot.storage.k8s.io/v1alpha1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: helmtest-vxflexos spec: snapshotClassName: vxflexos-snapclass source: name: pvol kind: PersistentVolumeClaim Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0 , then the created snapshot is named pvol0-snap.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n test.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation creates this class as its default snapshot class. You can see its definition in the installation directory file volumesnapshotclass.yaml.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updateddirectory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim , which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/docs/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.3.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added automatic SDC deployment on OpenShift CoreOS nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for volume cloning Added support for Controller high availability (multiple-controllers)  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+    ","excerpt":"Release Notes - CSI PowerFlex v1.3.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/docs/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that needs to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs shows that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs shows that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on worker node, and ensure your container run time manager is properly configured to be   utilized with SElinux.    Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/storage-plugin-docs/docs/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerMax driver supports beta snapshots. Driver versions prior to version 1.4 supported alpha snapshots.\nThe Volume Snapshots feature in Kubernetes has moved to beta in Kubernetes version 1.17. It was an alpha feature in earlier releases (1.13 onwards). The snapshot API version has changed from v1alpha1 to v1beta1 with this migration.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class Starting CSI PowerMax 1.4 driver, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class you will need and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation (using the default driver name):\napiVersion: snapshot.storage.k8s.io/v1beta1 deletionPolicy: Delete kind: VolumeSnapshotClass metadata: name: powermax-snapclass driver: csi-powermax.dellemc.com Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pmax-snapshot-demo namespace: test spec: volumeSnapshotClassName: powermax-snapclass source: persistentVolumeClaimName: pmax-pvc-demo Once the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-restore-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-snapshot-demo kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Creating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-clone-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-pvc-demo kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP With version 1.3.0, support has been added for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (Experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax , then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions which should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting with v1.4, the CSI PowerMax driver supports expansion of Persistent Volumes (PVs). This expansion is done online, that is, when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThis is a sample manifest for a storage class which allows for Volume Expansion.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-expand-sc annotations: storageclass.beta.kubernetes.io/is-default-class: false provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true #Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: SYMID: \"000000000001\" SRP: \"DEFAULT_SRP\" ServiceLevel: \"Bronze\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5Gi, then you can resize it to 10 Gi by updating the PVC.\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc NOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of volume, it cannot be used to shrink a volume.\nRaw block support Starting v1.4, CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powermax resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere forPowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server which acts as a reverse proxy for the Unisphere forPowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and performance of the CSI PowerMax driver.\nOptionally you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation CSI PowerMax Reverse Proxy is installed as a Kubernetes deployment in the same namespace as the driver.\nIt is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is a HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be a X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1 , then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state\n If you’re using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment \u0026 driver node Daemonset. There are two new sections in the values file - controller \u0026 node - where you can specify these values separately for the controller and node pods.\ncontroller If you want to apply nodeSelectors \u0026 tolerations for the controller pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller pods to worker nodes only (Default):  controller: nodeSelector: tolerations:  Set the following values for controller pods to tolerate the taint NoSchedule on master nodes:  controller: nodeSelector: tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\"  Set the following values for controller pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller: nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node If you want to apply nodeSelectors \u0026 tolerations for the node pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add/remove tolerations to this list\n# \"node\" allows to configure node specific parameters node: # \"node.nodeSelector\" defines what nodes would be selected for pods of node daemonset # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"node.tolerations\" defines tolerations that would be applied to node daemonset # Add/Remove tolerations as per requirement # Leave as blank if you wish to not apply any tolerations tolerations: - key: \"node.kubernetes.io/memory-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/disk-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/network-unavailable\" operator: \"Exists\" effect: \"NoExecute\" Topology Support Starting from version 1.5, the CSI PowerMax driver supports Topology aware Volume Provisioning which helps Kubernetes scheduler place PVCs on worker nodes which have access to backend storage. When used in conjunction with nodeSelectors which can be specified for the driver node pods, it provides an effective way to provision applications on nodes which have access to the PowerMax array.\nAfter a successful installation of the driver, if a node pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer defined topology i.e. users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage In order to utilize the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For e.g. - A PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-fc parameters: SRP: \"SRP_1\" SYMID: \"000000000001\" ServiceLevel: \u003cService Level\u003e #Insert Service Level Name provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true allowedTopologies: - matchLabelExpressions: - key: csi-powermax.dellemc.com/000000000001 values: - csi-powermax.dellemc.com - key: csi-powermax.dellemc.com/000000000001.fc values: - csi-powermax.dellemc.com In the above example if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n NOTE: The storage classes created during the driver installation (via Helm) do not contain any topology keys and have the volumeBindingMode set to Immediate. A set of sample storage class definitions to enable topology aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nIf you are using dell-csi-operator to create storage classes while installing the CSI PowerMax 1.5 driver, you can set the allowedTopologies value appropriately. volumeBindingMode is set to WaitForFirstConsumer if not specified explicitly.\n","excerpt":"Volume Snapshot Feature The CSI PowerMax driver supports beta …","ref":"/storage-plugin-docs/docs/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the powermax namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace powermax:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Configure Mount propagation on container runtime (that is, Docker) Linux multipathing requirements Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port groups names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For information about configuring iSCSI, see Dell EMC PowerMax documentation on Dell EMC Support.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nConfigure Mount Propagation on Container Runtime You must configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax. The following steps explain how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes. Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Linux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  Volume Snapshot requirements Volume Snapshot CRDs The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nAlternately, you can install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script.\nVolume Snapshot Controller Starting with the beta Volume Snapshots, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on the GitHub repository for snapshot controller will install v3.0.2 of the snapshotter controller - (k8s.gcr.io/sig-storage/snapshot-controller:v3.0.2) Dell EMC recommends using the v3.0.2 image of the CSI external snapshotter - (k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2) The CSI external-snapshotter sidecar is still installed with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml, point to the correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax priviledges.\n Create the secret by running kubectl create -f helm/secret.yaml If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds a SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file `cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     unisphere Specifies the URL of the Unisphere for PowerMax server. If using the CSI PowerMax Reverse Proxy, leave this value unchanged at https://127.0.0.1:8443. Yes “https://127.0.0.1:8443”   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   arrayWhitelist List of comma-separated array IDs. If this parameter remains empty, the driver manages all the arrays that are managed by the Unisphere instance that is configured for the driver. Specify the IDs of the arrays that you want to manage, using the driver. No Empty   symmetrixID Specify a Dell EMC PowerMax array that the driver manages. This value is used to create a default storage class. Yes “000000000000”   storageResourcePool Must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template which will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, then leave this value empty. No Empty   csireverseproxy This section refers to configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation No 2222   primary Mandatory section for Reverse Proxy - -   unisphere This must specify the URL of the Unisphere for PowerMax server Yes, if using Reverse Proxy “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   backup Optional section for Reverse Proxy. Specify Unisphere server address which the Reverse Proxy can fall back to if the primary Unisphere is unreachable or unresponsive.\nNOTE: If you do not want to specify a backup Unisphere server, then remove the backup section from the file - -   unisphere Specify the IP address of the Unisphere for PowerMax server which manages the arrays being used by the CSI driver No “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server No Empty    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. Please refer detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller Pods you deploy. If controller Pods are greater than number of available nodes, excess Pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended onto all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_POWERMAX_ARRAYS List of comma-separated array id(s) which will be managed by the driver No -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature review related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: “a-b-c-%foo%-xyz” where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature review the related documentation No false   StorageClass parameters      SYMID Symmetrix ID Yes 000000000001   SRP Storage Resource Pool Name Yes DEFAULT_SRP   ServiceLevel Service Level No Bronze   FsType File System type (xfs/ext4) xfs    allowVolumeExpansion After the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No false   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to specify the PowerMax array ID and append .fc or .iscsi at the end of it to specify a protocol. For more details on this feature review the related documentation No “000000000001”     Execute the following command to create PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component which can be installed along with the CSI PowerMax driver. For more details on this feature review the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator is going to create a Deployment and ClusterIP service as part of the installation\nNote - If you wish to use the ReverseProxy with CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret which holds a SSL certificate and a private key which is required by the reverse proxy server. Use a tool like openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs\nconfig : This section contains the details of the Reverse Proxy configuration\nmode : This value is set to Linked by default. Do not change this value\nlinkConfig : This section contains the configuration of the Linked mode\nprimary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if Primary Unisphere is unreachable\nurl : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion: storage.dell.com/v1 kind: CSIPowerMaxRevProxy metadata: name: powermax-reverseproxy # \u003c- Name of the CSIPowerMaxRevProxy object namespace: test-powermax # \u003c- Set the namespace to where you will install the CSI PowerMax driver spec: # Image for CSI PowerMax ReverseProxy image: dellemc/csipowermax-reverseproxy:v1.0.0.000R # \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy: Always # TLS secret which contains SSL certificate and private key for the Reverse Proxy server tlsSecret: csirevproxy-tls-secret config: # Mode for the proxy - only supported mode for now is \"Linked\" mode: Linked linkConfig: primary: url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true # This setting determines if client side Unisphere certificate validation is to be skipped certSecret: \"\" # Provide this value if skipCertificateValidation is set to false backup: # This is an optional field and lets you configure a backup unisphere which can be used by proxy server url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true Installation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/docs/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. These examples automate the creation of Pods using the default storage classes that were created during installation. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with different number of volumes. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n test\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the ./stoptest.sh -t 2vols -n test script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are using the default storageclass names (powermax and powermax-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates/ directory). You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot on that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass which is created by the Helm-based installer. If you have an operator-based deployment, the name of the snapshot class will differ. You must update the snapshot class name in the file betaSnap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/docs/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.5.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Docker EE 3.1 Added support for Controller high availability (multiple-controllers) Added support for Topology Added support for mount options Changed driver base image to UBI 8.x  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+   Delete Volume fails with error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release    ","excerpt":"Release Notes - CSI PowerMax v1.5.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/docs/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/storage-plugin-docs/docs/troubleshooting/powermax/","title":"PowerMax"},{"body":"Consuming existing volumes with static provisioning You can use existent volumes from PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=SystemclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.3 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:isilon-snapclassdriver:csi-isilon.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:autotestvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:newsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:AccessZone:SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"This manifest will create a pod and attach newly created ephemeral inline csi volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labelled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nTopology Usage In order to utilize the Topology feature the default storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies field as part of default storage class ensures that pod scheduling takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion:v1items:- allowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/XX.XX.XX.XXvalues:- csi-isilon.dellemc.comapiVersion:storage.k8s.io/v1kind:StorageClassname:Isilonparameters:AccessZone:SystemAzServiceIP:XX.XX.XX.XXIsiPath:/ifs/data/csiRootClientEnabled:\"false\"provisioner:csi-isilon.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerFor additional information, see the Kubernetes Topology documentation.\n","excerpt":"Consuming existing volumes with static provisioning You can use …","ref":"/storage-plugin-docs/docs/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a Daemon Set:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for PowerScale, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes. Configure Docker service Install Helm v3 Install volume snapshot components Deploy PowerScale driver using Helm  Note: There is no feature gate that needs to be set explicitly for CSI drivers version 1.17 and later. All the required feature gates are either beta/GA.\nConfigure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.\nProcedure  Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows: [Service] ... MountFlags=shared  Restart the Docker service with systemctl daemon-reload and systemctl daemon-reload systemctl restart docker   Install volume snapshot components Install Snapshot Beta CRDs To install snapshot CRDs specify --snapshot-crd flag to driver installation script dell-csi-helm-installer/csi-install.sh during driver installation.\nInstall Common Snapshot Controller, if not already installed for the cluster.\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml Install CSI Driver for PowerScale Before you begin\n You must clone the source code from git repository. In the dell-csi-helm-installer directory, there should be two shell scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart.  Steps\n  Collect information from the PowerScale Systems like IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and values file.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     isiIP “isiIP” defines the HTTPs endpoint of the PowerScale OneFS API server true -   isiPort “isiPort” defines the HTTPs port number of the PowerScale OneFS API server false 8080   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and host name should be verified. false true   isiAccessZone The name of the access zone a volume can be created in false System   volumeNamePrefix “volumeNamePrefix” defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount “controllerCount” defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableDebug Indicates whether debug level logs should be logged false true   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs false 1   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization false false   isiPath The default base path for the volumes to be created, this will be used if a storage class does not have the IsiPath parameter specified false    /ifs/data/csi      autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used (that is, the mount command will not explicitly specify “-o vers=3” option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify ‘vers=3’ as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish connection to Array. This requires enableCustomTopology to be enabled. false false   Storage Class parameters Following parameters are related to Storage Class     name “storageClass.name” defines the name of the storage class to be defined. false isilon   isDefault “storageClass.isDefault” defines whether the primary storage class should be the default. false true   reclaimPolicy “storageClass.reclaimPolicy” defines what will happen when a volume is removed from the Kubernetes API. Valid values are “Retain” and “Delete”. false Delete   accessZone The Access Zone where the Volume would be created false System   AzServiceIP Access Zone service IP if different from isiIP, specify here and refer in storageClass false    rootClientEnabled When a PVC is being created, it takes the storage class’ value of “storageclass.rootClientEnabled” false false   volumeBindingMode Controls when volume binding and dynamic provisioning should occur true WaitForFirstConsumer   allowedTopologies Controls scheduling of pods on worker nodes matching topology keys true csi-isilon.dellemc.com/{{ IsilonIP }}: csi-isilon.dellemc.com   Controller parameters Set nodeSelector and tolerations for controller     nodeSelector Define nodeSelector for the controllers, if required false    tolerations Define tolerations for the controllers, if required false     Note: User should provide all boolean values with double quotes. This is applicable only for my-isilon-settings.yaml. Example: “true”/“false”\nNote: controllerCount parameter value should not exceed number of nodes in the kubernetes cluster. Otherwise some of the controller pods will be in “Pending” state till new nodes are available for scheduling. The installer will exit with a WARNING on the same.\n  Create namespace Run kubectl create namespace isilon to create the isilon namespace. Specify the same namespace name while installing the driver.\nNote: CSI PowerScale also supports installation of driver in custom namespace.\n  Create a secret file for the OneFS credentials by editing the secret.yaml present under helm directory. Replace the values for the username and password parameters. Use the following command to convert username/password to base64 encoded string:\necho -n 'admin' | base64 echo -n 'password' | base64 Run kubectl create -f secret.yaml to create the secret.\nNote: The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nISI_PRIV_LOGIN_PAPI ISI_PRIV_NFS ISI_PRIV_QUOTA ISI_PRIV_SNAPSHOT ISI_PRIV_IFS_RESTORE ISI_PRIV_NS_IFS_ACCESS ISI_PRIV_LOGIN_SSH   Install OneFS CA certificates by following the instructions from next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and empty secret should be created for the successful CSI Driver for Dell EMC Powerscale installation.\nkubectl create -f emptysecret.yaml   Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/myvalues.yaml\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘isiInsecure’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘isiInsecure’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘isiInsecure’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘isiInsecure’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert.pem To create the secret, run kubectl create secret generic isilon-certs --from-file=ca_cert.pem -n isilon   Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nListing installed drivers with the CSI Isilon CRD User can query for csi-powerscale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver  Create namespace Run kubectl create namespace isilon to create the isilon namespace. Create isilon-creds Create a file called isilon-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:isilon-credsnamespace:isilontype:Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f isilon-creds.yaml command to create the secret.\n Create a CR (Custom Resource) for PowerScale using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:    Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   X_CSI_ISI_ENDPOINT HTTPs endpoint of the PowerScale OneFS API server Yes    X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node     Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver.  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/docs/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing kubectl get pvc. Note: Verify system for the new volume\n  Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to Kubernetes node\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/snap.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for newly created snapshot.\nNote:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. The CSI Driver for PowerScale installation creates this class as its default snapshot class. You can see its definition using kubectl get volumesnapshotclasses isilon-snapclass -o yaml.    Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in spec dataSource field.\nThe sample file for volume creation from snapshot is located under test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot\nkubectl get volumesnapshot kubectl delete volumesnapshot testvolclaim1-snap1   Create new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for new created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to unattach the volume from host\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/docs/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.4.0 New Features/Changes  Added support for OpenShift 4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Controller high availability (multiple-controllers) Added Topology support Added support for CSI Ephemeral Inline Volumes Added support for mount options Enhancements to volume creation from data source Enhanced support for Docker EE 3.1  Fixed Issues    Problem summary Found in version Resolved in version     POD creation fails in OpenShift and Kubernetes environments, if hostname is not an FQDN v1.3.0 v1.4.0   When creating volume from a snapshot or volume from volume, the owner of the new files or folders that are copied from the source snapshot is the Isilon user who is specified in secret.yaml. So the original owner of a file or folder might not be the owner of the newly created file or folder.  v1.4.0    Known Issues    Issue Resolution or workaround, if known     Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class are not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\n* ISI_PRIV_LOGIN_SSH\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.4.0 New Features/Changes …","ref":"/storage-plugin-docs/docs/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in production environment.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/storage-plugin-docs/docs/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command will create a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset will be created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore Pod should be Ready and Running\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller\n Deleting volumes To delete volumes, pod and statefulset run\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 1.1 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-3.0 kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerStore driver version 1.1 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:powerstore-snapshotdriver:csi-powerstore.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:testpowerstorespec:volumeSnapshotClassName:powerstore-snapshotsource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.2.0 adds support for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI protocol.\nTo enable CHAP authentication:\n Create secret powerstore-creds with the key chapsecret and chapuser set to base64 values. chapsecret must be between 12 and 60 symbols. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter connection.enableCHAP in my-powerstore-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating new host on powerstore array driver will populate host chap credentials with provided values. When re-using already existing hosts be sure to check that provided credentials in powerstore-creds match earlier preconfigured host credentials.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"This manifest will create a pod and attach newly created ephemeral inline csi volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assinged in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feaure user needs to create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example will match all nodes where driver has a connection to PowerStore with IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work.\n For any additional information about topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as Host on storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/storage-plugin-docs/docs/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  Linux native multipathing requirements Configure Mount propagation on container runtime (i.e. Docker) Volume Snapshot requirements The nonsecure registries are defined in Docker or other container runtime, for CSI drivers that are hosted in a nonsecure location. You can access your cluster with kubectl and helm.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you will use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.2 supports iSCSI connectivity.\nIf you will use the iSCSI protocol, set up the iSCSI initiators as follows:\n Make sure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Make sure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  Configure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax. The following is instruction on how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Volume Snapshot requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nAlternately, you can install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script. If you are installing the driver using the Dell CSI Operator, there is a helper script provided to install the snapshot CRDs - scripts/install_snap_crds.sh.\nVolume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 onwards, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell EMC recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository Ensure that you’ve created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image. Edit the helm/secret.yaml, point to correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername \u0026 mypassword are credentials that would be used for accessing PowerStore API. NOTE: If you want to use iSCSI CHAP you need fill chapsecret and chapuser fields in similar manner\n Create the secret by running kubectl create -f helm/secret.yaml Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:     Parameter Description Required Default     powerStoreApi Defines the full URL path to the PowerStore API Yes \" \"   volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csi”   nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   connection.scsiProtocol Defines which transport protocol to use (FC, ISCSI, None, or auto). - By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. - A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. - To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. - For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. Yes “auto”   connection.nfs.enable Enables or disables NFS support No FALSE   connection.nfs.nasServerName Points to the NAS server that would be used - If you have nfs.enabled set to true, it will try to use nfs.nasServerName. This will fail if you do not provide nfs.nasServerName. No “nas-server”   connection.nfs.version Defines version of NFS protocol No “v3”   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No FALSE   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator doesn’t use any Helm charts and the installation \u0026 configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerStore credentials: Create a file called powerstore-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powerstore-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f powerstore-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerStore using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If controller pods is greater than number of available nodes, excess pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_POWERSTORE_ENDPOINT Must provide a PowerStore HTTPS API url Yes https://127.0.0.1/api/rest   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provide list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   StorageClass parameters      allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the “127.0.0.1-nfs” portion in the key with PowerStore endpoint IP with its value and append -nfs, -fc or -iscsi at the end of it No “127.0.0.1-nfs”     Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.  ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/docs/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs and nfs storage classes, and automatically mounts them to the pod. Note that nfs storage class is optional and will not be created if you haven’t turned it on in myvalues.yaml.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/storage-plugin-docs/docs/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.2.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Docker EE 3.1 Added support for Controller high availability (multiple-controllers) Added support for Topology Added support for ephemeral volumes Added support for mount options Changed driver base image to UBI 8.x  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+    ","excerpt":"Release Notes - CSI PowerStore v1.2.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/docs/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to PowerStore API. Check if you’ve created secret with correct credentials    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/storage-plugin-docs/docs/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/storage-plugin-docs/docs/grasp/video/","title":"Quick video lessons"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/installation/test/","title":"Test Drivers"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/sample.yaml\nThis command will create a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset will be created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unitycommand. Pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI Unity driver version 1.3 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by copy pasting the following commands (Copy entire thing in one shot and paste it in terminal):\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of the CSI Unity driver version 1.3 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver version 1.3 and later supports managing Raw Block volumes.\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:unitytestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:unityresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nNote: Raw block volume creation supports only for FC and iSCSI protocols\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"This manifest will create a pod and attach newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (Attacher, Provisioner, Resizer and Snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in the similar way in node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feature user can install driver by setting createStorageClassesWithTopology to true in the myvalues.yaml which will create default storage classes by adding topology keys (based on the arrays specified in myvalues.yaml) and with WaitForFirstConsumer binding mode.\nAnother option is the user can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example will match all nodes where driver has a connection to Unity array with array ID mentioned via Fiber Channel. Similarly by replacing fc with iscsi in the key will check for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work properly.\n For any additional information about topology, see the Kubernetes Topology documentation.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/storage-plugin-docs/docs/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes Configure Docker service Install Helm v3 To use FC protocol, host must be zoned with Unity array To use iSCSI and NFS protocol, iSCSI initiator and NFS utility packages need to be installed  Configure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for Unity.\nProcedure   Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows:\n[Service] ... MountFlags=shared   Restart the Docker service with systemctl daemon-reload and\nsystemctl daemon-reload systemctl restart docker   Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart, such as creating Custom Resource Definitions (CRDs), if needed. Make sure “unity” namespace exists in kubernetes cluster. Use kubectl create namespace unity command to create the namespace, if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.json and myvalues.yaml file.\n  Copy the csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (unity-cert-0..unity-cert-n). Minimum value should be 1 false 1   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute false 15   controllerCount Controller replication count to maintain high availability yes 2   volumeNamePrefix String to prepend to any volumes created by the driver false csivol   snapNamePrefix String to prepend to any snapshot created by the driver false csi-snap   csiDebug To set the debug log policy for CSI driver false “false”   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   createStorageClassesWithTopology Flag to enable or disable topology. true false   Storage Array List Following parameters is a list of parameters to provide multiple storage arrays     storageArrayList[i].name Name of the storage class to be defined. A suffix of ArrayId and protocol will be added to the name. No suffix will be added to default array. false unity   storageArrayList[i].isDefaultArray To handle the existing volumes created in csi-unity v1.0, 1.1 and 1.1.0.1. The user needs to provide “isDefaultArray”: true in secret.json. This entry should be present only for one array and that array will be marked default for existing volumes. false “false”   Storage Class parameters Following parameters are not present in values.yaml     storageArrayList[i].storageClass.storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   storageArrayList[i].storageClass.thinProvisioned To set volume thinProvisioned false “true”   storageArrayList[i].storageClass.isDataReductionEnabled To set volume data reduction false “false”   storageArrayList[i].storageClass.volumeTieringPolicy To set volume tiering policy false 0   storageArrayList[i].storageClass.FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   storageArrayList[i].storageClass.hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \"”   storageArrayList[i].storageClass.nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \"”   storageArrayList[i].storageClass.hostIoSize NFS related parameter. To set filesystem host IO Size. false “8192”   storageArrayList[i].storageClass.reclaimPolicy What should happen when a volume is removed false Delete   Snapshot Class parameters Following parameters are not present in values.yaml     storageArrayList[i] .snapshotClass.retentionDuration TO set snapshot retention duration. Format:“1:23:52:50” (number of days:hours:minutes:sec) false \"”    Note: User should provide all boolean values with double quotes. This applicable only for myvalues.yaml. Example: “true”/“false”.\nExample myvalues.yaml\ncsiDebug: \"true\" volumeNamePrefix : csivol snapNamePrefix: csi-snap imagePullPolicy: Always certSecretCount: 1 syncNodeInfoInterval: 5 controllerCount: 2 createStorageClassesWithTopology: true storageClassProtocols: - protocol: \"FC\" - protocol: \"iSCSI\" - protocol: \"NFS\" storageArrayList: - name: \"APM00******1\" isDefaultArray: \"true\" storageClass: storagePool: pool_1 FsType: ext4 nasServer: \"nas_1\" thinProvisioned: \"true\" isDataReductionEnabled: true hostIOLimitName: \"value_from_array\" tieringPolicy: \"2\" snapshotClass: retentionDuration: \"2:2:23:45\" - name: \"APM001******2\" storageClass: storagePool: pool_1 reclaimPolicy: Delete hostIoSize: \"8192\" nasServer: \"nasserver_2\"   Create an empty secret by navigating to helm folder that contains emptysecret.yaml file and running the kubectl create -f emptysecret.yaml command.\n  Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Example: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nNote: “isDefaultArray” parameter in values.yaml and secret.json should match each other.\n  Setup for snapshots\nThe Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17.\n  The following section summarizes the changes in the beta release.\nTo use the Kubernetes Volume Snapshot feature, ensure the following components have been deployed on your Kubernetes cluster.\n Install Snapshot Beta CRDs using the following command  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml  Volume snapshot controller  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml  After executing these commands, a snapshot-controller pod should be up and running.      Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation should emit messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.19 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.18 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying snapshot support | |--\u003e Verifying that beta snapshot CRDs are available Success | |--\u003e Verifying that beta snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u003e Verification Complete ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for statefulset unity-controller to be ready Success | |--\u003e Waiting for daemonset unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results: At the end of the script statefulset unity-controller and daemonset unity-node is ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n unity-controller-xxxx with 5/5 containers ready, and status displayed as Running.      Agent pods with 2/2 containers and the status displayed as Running.\nFinally, the script creates storageclasses such as, “unity”. Additional storage classes can be created for different combinations of file system types and Unity storage pools. The script also creates volumesnapshotclass “unity-snapclass”.\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’ kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)  Note: “unity” is the namespace for helm based installation but namespace can be user defined in operator based installation.\nNote: User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to kubernetes secret size limitation.\nNote: Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/unity/","title":"Unity"},{"body":"Installing Unity CSI Driver via Operator The CSI Driver for Dell EMC Unity can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace run kubectl create namespace test-unity to create the a namespace called test-unity. It can be any user-defined name.\n  Create unity-creds\nCreate secret mentioned in Install csi-driver section. The secret should be created in user-defined namespace (test-unity, in this case)\n  Create certificate secrets\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n in the user-defined namespace (test-unity, in this case) Create certificate procedure explained in the link\nNote: ‘certSecretCount’ parameter is not required for operator. Based on secret name pattern (unity-certs-*) operator reads all the secrets. Secret name suffix should have 0 to N order to read the secrets. Secrets will not be considered, if any number missing in suffix.\nExample: If unity-certs-0, unity-certs-1, unity-certs-3 are present in the namespace, then only first two secrets are considered for SSL verification.\n  Create a CR (Custom Resource) for unity using the sample provided below\n  Create a new file csiunity.yaml by referring the following content. Replace the given sample values according to your environment. You can find may CRDs under deploy/crds folder when you install dell-csi-operator\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v3replicas:2common:image:\"dellemc/csi-unity:v1.4.0.000R\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\"true\"sideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\"]storageClass:- name:virt2016****-fcdefault:truereclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2016****\"protocol:\"FC\"- name:virt2017****-iscsireclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"- name:virt2017****-nfsreclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"NFS\"hostIoSize:\"8192\"nasServer:nas_1- name:virt2017****-iscsi-topologyreclaimPolicy:\"Delete\"allowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/virt2017****-iscsivalues:- \"true\"parameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"snapshotClass:- name:test-snapparameters:retentionDuration:\"\"  Execute the following command to create unity custom resource kubectl create -f csiunity.yaml. This command will deploy the csi-unity driver in the test-unity namespace.\n  Any deployment error can be found out by logging the operator pod which is in default namespace (example, kubectl logs dell-csi-operator-64c58559f6-cbgv7)\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   GOUNITY_DEBUG To enable debug mode for gounity library No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot      ","excerpt":"Installing Unity CSI Driver via Operator The CSI Driver for Dell EMC …","ref":"/storage-plugin-docs/docs/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes, and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/storage-plugin-docs/docs/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.4.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Controller high availability (multiple-controllers) Added support for Ubuntu 20.04 Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Docker EE 3.1 Added support for Topology Added support for ephemeral volumes Added raw-block volume creation capability for iSCSI and FC based volumes. Added support for Mount options Changed driver base image to UBI 8.x  Fixed Issues  Source NFS PVC cannot be deleted if cloned NFS PVC exists.  Known Issues    Issue Workaround     Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released we need to manually remove the node labels mentioned here https://github.com/dell/csi-unity#known-issues (Point 1)   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver should be restarted with the below command only if the topology-based storage classes are used. Otherwise, the driver will automatically detect the newly added or removed arrays https://github.com/dell//csi-unity#known-issues (Point 2)   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in cluster but on array it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from array.    ","excerpt":"Release Notes - CSI Unity v1.4.0 New Features/Changes  Added support …","ref":"/storage-plugin-docs/docs/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to Unity - Authentication failure. Check if you’ve created secret with correct credentials    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/storage-plugin-docs/docs/troubleshooting/unity/","title":"Unity"}]