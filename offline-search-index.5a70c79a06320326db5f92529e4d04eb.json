[{"body":"To upgrade Dell CSI Operator from v1.2.0 or v1.3.0 to v1.4.0, perform the following steps.\nUsing Installation Script Run the following command to upgrade the operator from the v1.2.0 release\n$ bash scripts/install.sh --upgrade Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager. If the InstallPlan for the Operator subscription is set to Automatic, the operator will be automatically upgraded to the new version. If the InstallPlan is set to Manual, then a Cluster Administrator would need to approve the upgrade.\nNOTE: The recommended version of OLM for Upstream Kubernetes is v0.17.0 when upgrading operator to v1.4.0.\n","excerpt":"To upgrade Dell CSI Operator from v1.2.0 or v1.3.0 to v1.4.0, perform …","ref":"/storage-plugin-docs/docs/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"If you are upgrading the Dell CSI Operator from v1.1.0 or v1.2.0 to v1.3.0, then follow the instructions below. If you are trying to upgrade the Operator from an older version, please refer the instructions here\nUsing Installation Script Run the following command to upgrade the operator from v1.2.0 release\n$ bash scripts/install.sh --upgrade Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager. If the InstallPlan for the Operator subscription is set to Automatic, the operator will be automatically upgraded to the new version. If the InstallPlan is set to Manual, then a Cluster Administrator would need to approve the upgrade.\nUpgrade Operator from version older than v1.1.0 to v1.3.0  Uninstall the old version of the Operator If required, upgrade your cluster to a supported version Follow the installation instructions to install the v1.3.0 of the Operator here  ","excerpt":"If you are upgrading the Dell CSI Operator from v1.1.0 or v1.2.0 to …","ref":"/storage-plugin-docs/v1/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"What is a container: https://www.docker.com/resources/what-container\nWhat is container orchestration: https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s): https://kubernetes.io/\nDocker: https://www.docker.com/\nUnderstanding CSI: https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container: https://www.docker.com/resources/what-container …","ref":"/storage-plugin-docs/docs/grasp/start/","title":"Getting Started"},{"body":"What is a container : https://www.docker.com/resources/what-container\nWhat is container orchestration : https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s) : https://kubernetes.io/\nDocker : https://www.docker.com/\nUnderstanding CSI : https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container : https://www.docker.com/resources/what-container …","ref":"/storage-plugin-docs/v1/grasp/start/","title":"Getting Started"},{"body":"What is a container : https://www.docker.com/resources/what-container\nWhat is container orchestration : https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s) : https://kubernetes.io/\nDocker : https://www.docker.com/\nUnderstanding CSI : https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container : https://www.docker.com/resources/what-container …","ref":"/storage-plugin-docs/v2/grasp/start/","title":"Getting Started"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI spec v1.3) enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Release/Drivers PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current v1.6 v1.6 v1.4 v1.5 v1.7   Previous v1.5 v1.5 v1.3 v1.4 v1.6   Older v1.4 v1.4 v1.2 v1.3 v1.5   Archives v1.3 v1.3 v1.1 v1.2 v1.4    Architecture Features and capabilities Supported Platforms    PowerMax PowerFlex Unity PowerScale/Isilon PowerStore     Storage Array 5978.479.479, 5978.669.669, Unisphere 9.2 3.5.x, 3.6.x 5.0.3, 5.0.4, 5.0.5, 5.0.6, 5.0.7 OneFS 8.1, 8.2, 9.0, 9.1 1.0.x, 2.0.x   Kubernetes 1.19, 1.20, 1.21 1.19, 1.20, 1.21 1.19, 1.20, 1.21 1.19, 1.20, 1.21 1.19, 1.20, 1.21   RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x   Ubuntu 20.04 20.04 18.04, 20.04 18.04, 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES no 15SP2 15SP2 15SP2 15SP2   Fedora Core OS no 5.x no no no   OpenShift 4.6, 4.6 EUS, 4.7 4.6, 4.6 EUS, 4.7 4.6, 4.6 EUS, 4.7 4.6, 4.6 EUS, 4.7 4.6, 4.6 EUS, 4.7   MKE 3.4.0 3.4.0 3.4.0 3.4.0 3.4.0   Google Anthos 1.6 1.6 no no 1.7   VMware Tanzu no no yes yes no   RKE no yes yes no no    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO(FC/iSCSI)\nRWO/RWX/ROX(Raw block) RWO\nRWO/RWX/ROX(Raw block) RWO(FC/iSCSI)\nRWO/RWX(RawBlock)\nRWO/RWX/ROX(NFS) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes yes yes yes yes    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI …","ref":"/storage-plugin-docs/docs/dell-csi-driver/","title":"Introduction"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI spec v1.2) enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Release/Drivers PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current v1.5 v1.5 v1.3 v1.4 v1.6   Previous v1.4 v1.4 v1.2 v1.3 v1.5   Older v1.3 v1.3 v1.1 v1.2 v1.4    NOTE: This doc version is no longer supported by us. You can check our latest version\nArchitecture Features and capabilities Supported Platforms    PowerMax PowerFlex/VxFlex OS Unity PowerScale/Isilon PowerStore     Storage Array 5978.479.479, 5978.669.669, Unisphere 9.1, 9.2 3.5.x OE 5.0.2, 5.0.3, 5.0.4, 5.0.5, 5.0.6 OneFS 8.1, 8.2, 9.0, 9.1 1.0.x   Kubernetes 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20   RHEL 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3   Ubuntu 20.04 20.04 20.04 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES no 15SP2 15SP2 no no   Fedora Core OS no 5.10 no no no   OpenShift 4.6, 4.7 4.6, 4.7 4.6, 4.7 4.6, 4.7 4.6, 4.7   Docker EE 3.1 3.1 3.1 3.1 3.1   Google Anthos 1.5 1.6 no no 1.5    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO(FC/iSCSI)\nRWO/RWX/ROX(Raw block) RWO\nRWO/RWX/ROX(Raw block) RWO(FC/iSCSI)\nRWO/RWX(RawBlock)\nRWO/RWX/ROX(NFS) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes (via Unisphere) yes yes yes yes    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI …","ref":"/storage-plugin-docs/v1/dell-csi-driver/","title":"Introduction"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Driver PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current version v1.4 v1.4 v1.2 v1.3 v1.5   Older Versions v1.3 v1.3 v1.1 v1.2 v1.4    NOTE: This doc version is no longer supported by us. You can check our latest version\nArchitecture Features and capabilities Supported Platforms   Features PowerMax PowerFlex/VxFlex OS Unity PowerScale/Isilon PowerStore     Storage Array 5978.479.479, 5978.669.669 3.0.x, 3.5.x 5.0.0, 5.0.1, 5.0.2, 5.0.3 OneFS 8.1, 8.2, 9.0, 9.1 1.0.x   Kubernetes 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19   RHEL 7.7, 7.8, 7.9 7.7, 7.8, 7.9 7.7, 7.8, 7.9 7.7, 7.8, 7.9 7.7, 7.8, 7.9   Ubuntu 20.04 20.04 20.04 20.04 20.04   CentOS 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8   SLES no 15SP2 no no no   OpenShift 4.5, 4.6 4.5, 4.6 4.5, 4.6 4.5, 4.6 4.5, 4.6   Docker EE 3.1 3.1 3.1 3.1 3.1   Google Anthos 1.5 no no no 1.5    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO/RWX/ROX RWO RWO(FC/iSCSI)\nRWO/RWX/ROX(NFS)\nRWO/RWX/ROX(Raw block FC and iSCSI) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no no yes yes yes   Topology yes yes yes yes yes   Multi-array yes (via Unisphere) no yes (with single driver) no no    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering\nNFS host IO size\nSnapshot retention duration Access Zone\nNFS version (3 or 4) iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI enabled …","ref":"/storage-plugin-docs/v2/dell-csi-driver/","title":"Introduction"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.3/v1.4 to v1.5 using Helm Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository and get the v1.5 driver. You need to create config.json with the configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.4/v1.3 driver as default in config.json in v1.5 so that the driver knows the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.5 driver is not supported on Kubernetes upstream clusters running version 1.17. You must upgrade your cluster to 1.19, 1.20, or 1.21 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/storage-plugin-docs/docs/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.2/v1.3 to v1.4 using Helm Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository and get the v1.4 driver. You need to create config.json with configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.3/v1.2 driver as default in config.json in v1.4 so that the driver know the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Update Driver from pre-v1.2 to v1.4 using Helm A direct upgrade of the driver from an older version pre-v1.2 to version 1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command. Delete any VolumeSnapshotClass present in the cluster. Delete all the alpha snapshot CRDs from the cluster by running the following commands: kubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io  Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace vxflexos. Install the driver using the steps described in the Installation Using Helm section for the CSI PowerFlex driver.  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.4 driver is not supported on Kubernetes upstream clusters running version 1.17. You must upgrade your cluster to 1.18, 1.19, or 1.20 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/storage-plugin-docs/v1/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.2 to v1.3 using Helm Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository and get the v1.3 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Update Driver from pre-v1.2 to v1.3 using Helm A direct upgrade of the driver from an older version pre-v1.2 to version 1.3 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command. Delete any VolumeSnapshotClass present in the cluster. Delete all the alpha snapshot CRDs from the cluster by running the following commands: kubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io  Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace vxflexos. Install the driver using the steps described in the Installation Using Helm section for the CSI PowerFlex driver.  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.3 driver is not supported on Kubernetes upstream clusters running version 1.16. You must upgrade your cluster to 1.17, 1.18, or 1.19 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option –upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/storage-plugin-docs/v2/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.6 to v1.7 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.7 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or …","ref":"/storage-plugin-docs/docs/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.5 to v1.6 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.6 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  Update Driver from pre-v1.4 to v1.6 using Helm A rolling upgrade of the driver from an older version to v1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n  Delete any alpha VolumeSnapshot or VolumeSnapshotContent in the cluster.\n  Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command.\n  Delete any VolumeSnapshotClass present in the cluster.\n  Delete all the alpha snapshot CRDs from the cluster by running the following commands:\nkubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io   Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e where driver-namespace is the namespace where driver is installed.\n  Install the driver using the steps described in the Installation Using Helm section for the CSI PowerMax driver.\n  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerMax version 1.6 driver is not supported on Kubernetes upstream clusters running Kubernetes version 1.17 or lower. You must upgrade your cluster to 1.18, 1.19, or 1.20 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or …","ref":"/storage-plugin-docs/v1/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.4 to v1.5 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.5 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  Update Driver from pre-v1.4 to v1.5 using Helm A rolling upgrade of the driver from an older version to v1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n  Delete any alpha VolumeSnapshot or VolumeSnapshotContent in the cluster.\n  Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command.\n  Delete any VolumeSnapshotClass present in the cluster.\n  Delete all the alpha snapshot CRDs from the cluster by running the following commands:\nkubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io   Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e where driver-namespace is the namespace where driver is installed.\n  Install the driver using the steps described in the Installation Using Helm section for the CSI PowerMax driver.\n  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerMax version 1.5 driver is not supported on Kubernetes upstream clusters running Kubernetes version 1.16 or lower. You must upgrade your cluster to 1.17, 1.18, or 1.19 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option –upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or …","ref":"/storage-plugin-docs/v2/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 1.5.0 to 1.6.0 Steps\n  Verify that all pre-requisites to install CSI Driver for Dell EMC PowerScale version 1.6.0 are fulfilled. Note that change in secret format should be taken care.\n Delete the existing secret (isilon-creds and isilon-certs-0) Create new secrets (isilon-creds and isilon-certs-0) Refer Installation section here.    Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell EMC PowerScale version 1.6.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: To upgrade the driver from version 1.5.0 to 1.6.0:\nNote: It is highly recommended to take Backup of existing storage class definition and volumesnapshot class definition, yaml files before the upgrade.\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerScale using Helm or …","ref":"/storage-plugin-docs/docs/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver from version v1.4.0 to v1.5.0 Steps\n  Verify that all pre-requisites to install CSI Driver for Dell EMC PowerScale v1.5.0 are fulfilled (including change in secret formats).\n 1.1 Delete the existing secrets (isilon-creds and isilon-certs) 1.2 Create new secrets (isilon-creds and isilon-certs-0) in the format specified by csi-powerscale 1.5.  Refer Installation section here.\n  Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell EMC PowerScale v1.5.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: To upgrade the driver from csi-PowerScale v1.4 to csi-PowerScale v1.5 (OpenShift 4.6) :\n Clone operator version 1.3.0 Execute bash scripts/install.sh --upgrade .This command will install latest version of operator. Uninstall the existing driver by executing the command kubectl delete -f \u003cdriver.yaml\u003e with appropriate yaml file used for csi-powerscale 1.4 installation. Delete the existing secrets (both isilon-creds and isilon-certs) Create new isilon-creds secret in the latest csi-PowerScale format. For additional information, refer here Create new isilon-certs secret. Make sure the name of new secret is isilon-certs-0. For additional information, refer here Furnish the sample CR yaml according to your environment. Install csi-PowerScale driver 1.5 by executing the following command: kubectl create -f \u003cfurnished-cr.yaml\u003e  The above said steps are for Operator which was deployed in non-olm way.\nFor additional information, refer Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/storage-plugin-docs/v1/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver from version v1.3.0/v1.3.0.1 to v1.4.0 Steps\n  Verify that all pre-requisites to install CSI Driver for DELL EMC PowerScale v1.4.0 are fulfilled.\n  Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the DELL EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for DELL EMC PowerScale v1.4.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/storage-plugin-docs/v2/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.3 to v1.4 using Helm Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.4 driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\n endpoint: defines the full URL path to the PowerStore API. username, password: defines credentials for connecting to array. insecure: defines if we should use insecure connection or not. default: defines if we should treat the current array as a default. block-protocol: defines what SCSI transport protocol we must use (FC, ISCSI, None, or auto). nas-name: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.3 driver will not be deleted, v1.4 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.3 in your cluster then be sure to include the same array you have used for the v1.3 driver and make it default in the config.yaml file.\n   Create the secret by running sed \"s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\" helm/secret.yaml | kubectl apply -f -\n  Update values file as needed.\n  Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the drive, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/storage-plugin-docs/docs/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.2 to v1.3 using Helm Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.3 driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. username, password: defines credentials for connecting to array. insecure: defines if we should use insecure connection or not. default: defines if we should treat the current array as a default. block-protocol: defines what SCSI transport protocol we must use (FC, ISCSI, None, or auto). nas-name: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.2 driver will not be deleted, v1.3 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.2 in your cluster then be sure to include same array you have used for v1.2 driver and make it default in config.yaml file.\n   Create the secret by running sed \"s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\" helm/secret.yaml | kubectl apply -f -\n  Update values file as needed.\n  Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/storage-plugin-docs/v1/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.1 to v1.2 using Helm Steps\n Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.2 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/storage-plugin-docs/v2/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUsing Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v1.5 to csi-unity 1.6 (across K8S 1.19, K8S 1.20, K8S 1.21).\n Get the latest csi-unity 1.6 code from Github. Create myvalues.yaml according to csi-unity 1.6 . Clone the repository https://github.com/dell/csi-unity , copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements. Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade  Note:\n User has to re-create existing custom-storage classes (if any) according to the latest (v1.6) format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.json/Secret.yaml files can be updated according to Multiarray Normalization parameters only after upgrading the driver.  Using Operator Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from csi-unity v1.5 to csi-unity v1.6 (OpenShift 4.6/4.7) :\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/storage-plugin-docs/docs/upgradation/drivers/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUsing Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in install section.\nTo upgrade the driver from csi-unity v1.4 to csi-unity 1.5 (across K8S 1.18, K8S 1.19, K8S 1.20).\n  Get the latest csi-unity 1.5 code from Github.\n  Create myvalues.yaml according to csi-unity 1.5 .\n  Delete the existing default storage classes of csi-unity 1.4 .\n  Clone the repository https://github.com/dell/csi-unity , copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements.\n  Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade\n  If the value of ‘createStorageClassesWithTopology’ is set to “true” in myvalues.yaml , then\n Check the default storage classes, VolumeBindingMode should be ‘WaitForFirstConsumer’.    Note: User has to re-create existing custom-storage classes (if any) according to latest (v1.5) format.\nUsing Operator Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from csi-unity v1.4 to csi-unity v1.5 (OpenShift 4.6) :\n  Clone operator version 1.3.0\n  Execute bash scripts/install.sh --upgrade This command will install latest version of operator.\n  Furnish the sample CR yaml according to your environment.\n  For upgrading the csi-unity driver execute the following command:\nkubectl apply -f \u003cfurnished-cr.yaml\u003e\n  ","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/storage-plugin-docs/v1/upgradation/drivers/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpdate Driver from v1.3 to v1.4 using Helm Steps\n Run git clone https://github.com/dell/csi-unity.git to clone the git repository and get the v1.3 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace unity --values ./my-values.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/storage-plugin-docs/v2/upgradation/drivers/unity/","title":"Unity"},{"body":"Installation information for all the drivers can be found on the individual driver’s page in this section\n","excerpt":"Installation information for all the drivers can be found on the …","ref":"/storage-plugin-docs/docs/installation/","title":"Installation"},{"body":"Installation information for all the drivers can be found in the individual drivers page in this section\n","excerpt":"Installation information for all the drivers can be found in the …","ref":"/storage-plugin-docs/v1/installation/","title":"Installation"},{"body":"Installation information for all the drivers can be found in the individual drivers page in this section\n","excerpt":"Installation information for all the drivers can be found in the …","ref":"/storage-plugin-docs/v2/installation/","title":"Installation"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers).    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/storage-plugin-docs/docs/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisities in worker nodes (in chosen drivers).    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/storage-plugin-docs/v1/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisities in worker nodes (in chosen drivers).    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/storage-plugin-docs/v2/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n Search DellEMC in the storage category in Operatorhub.io.  Click DellEMC Operator.  Check the desired version is selected and click Install. Follow the provided instructions.  Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on …","ref":"/storage-plugin-docs/docs/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n  Search DellEMC in storage category in Operatorhub.io.   Click DellEMC Operator.   Check the desired version is selected and click Install. Follow the provided instructions.   Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on …","ref":"/storage-plugin-docs/v1/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n  Search DellEMC in storage category in Operatorhub.io.   Click DellEMC Operator.   Check the desired version is selected and click Install. Follow the provided instructions.   Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on …","ref":"/storage-plugin-docs/v2/partners/operator/","title":"OperatorHub.io"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.  Check the version you want to install from the list, you can check the details by clicking it.  Once selected, click “Install” to proceed with the installation process.  You can verify the list of available operators by selecting the “Installed Operator” section.  Select the Dell CSI Operator for further details.  Install CSI Drivers via Operator Steps\n Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.  After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.  You can check the driver installed and node and controller pods running in the Pods section under Workloads.  ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/storage-plugin-docs/docs/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n  Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.   Check the version you want to install from the list, you can check the details by clicking it.   Once selected, click “Install” to proceed with installation process.   You can verify the list of available operators by selecting “Installed Operator” section.   Select the Dell CSI Operator to get further description.   Install CSI Drivers via Operator Steps\n  Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.   After clicking “Create CSIUnity” option in above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.   You can check the driver installed and node and controller pods running in the Pods section under Workloads.   ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/storage-plugin-docs/v1/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n  Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.   Check the version you want to install from the list, you can check the details by clicking it.   Once selected, click “Install” to proceed with installation process.   You can verify the list of available operators by selecting “Installed Operator” section.   Select the Dell CSI Operator to get further description.   Install CSI Drivers via Operator Steps\n  Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.   After clicking “Create CSIUnity” option in above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.   You can check the driver installed and node and controller pods running in the Pods section under Workloads.   ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/storage-plugin-docs/v2/partners/redhat/","title":"Red Hat OpenShift"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-type, driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, …","ref":"/storage-plugin-docs/docs/uninstall/","title":"Uninstallation"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, …","ref":"/storage-plugin-docs/v1/uninstall/","title":"Uninstallation"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the PowerScale driver:\n./csi-uninstall.sh --namespace isilon/\u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall a PowerFlex driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-name and driver-namespace with their respective values $ kubectl delete vxflexos/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, …","ref":"/storage-plugin-docs/v2/uninstall/","title":"Uninstallation"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/upgradation/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v1/upgradation/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v2/upgradation/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/features/","title":"Features"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v1/features/","title":"Features"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v2/features/","title":"Features"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nInstallation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 1.5 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerMax 1.6 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerMax 1.7 v6 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerFlex 1.3 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerFlex 1.4 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerFlex 1.5 v5 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerScale 1.4 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerScale 1.5 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerScale 1.6 v6 1.19, 1.20, 1.21 4.6, 4.7   CSI Unity 1.4 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI Unity 1.5 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI Unity 1.6 v5 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerStore 1.2 v2 1.17, 1.18, 1.19 4.5, 4.6   CSI PowerStore 1.3 v3 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerStore 1.4 v4 1.19, 1.20, 1.21 4.6, 4.7     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  NOTE: The recommended version of OLM for upstream Kubernetes is v0.17.0.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone github.com/dell/dell-csi-operator $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\n  Clone the Dell CSI Operator repository. Run bash scripts/install.sh to install the operator.   NOTE: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default. Any existing installations of Dell CSI Operator (v1.2.0 or later) installed using install.sh to the ‘default’ or ‘dell-csi-operator’ namespace can be upgraded to the new version by running install.sh --upgrade.\n  Run the command oc get pods -n dell-csi-operator to validate the install. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace.   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.20 \u0026 v1.21, make sure to install v1 VolumeSnapshot CRDs On clusters running v1.19, make sure to install v1beta1 VolumeSnapshot CRDs   External Volume Snapshot Controller with the correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nReplacing CSI Operator with Dell CSI Operator Dell CSI Operator was previously available, with the name CSI Operator, for both manual and OLM installation.\nCSI Operator has been discontinued and has been renamed to Dell CSI Operator. This is just a name change and as a result, the Kubernetes resources created as part of the Operator deployment will use the name dell-csi-operator instead of csi-operator.\nBefore proceeding with the installation of the new Dell CSI Operator, any existing CSI Operator installation has to be completely removed from the cluster.\nNote - This doesn’t impact any of the CSI Drivers which have been installed in the cluster\nIf the old CSI Operator was installed manually, then run the following command from the root of the repository which was used originally for installation\nbash scripts/undeploy.sh  If you don’t have the original repository available, then run the following commands\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator git checkout csi-operator-v1.0.0 bash scripts/undeploy.sh  Note - Once you have removed the old CSI Operator, then for installing the new Dell CSI Operator, you will need to pull/checkout the latest code\nIf you had installed the old CSI Operator using OLM, then please follow the uninstallation instructions provided by OperatorHub. This will mostly involve:\n* Deleting the CSI Operator Subscription * Deleting the CSI Operator CSV  Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  NOTE: If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass needs to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ncommon\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. Any modifications to this should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects  In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again.  NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/storage-plugin-docs/docs/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nInstallation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 1.5 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerMax 1.6 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerFlex 1.3 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerFlex 1.4 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerScale 1.4 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerScale 1.5 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI Unity 1.4 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI Unity 1.5 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerStore 1.2 v2 1.17, 1.18, 1.19 4.5, 4.6   CSI PowerStore 1.3 v3 1.18, 1.19, 1.20 4.6, 4.7     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  Pre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone github.com/dell/dell-csi-operator $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Clone the Dell CSI Operator repository. Skip this step for Offline Install. And continue using workspace created by untar of dell-csi-operator-bundle.tar.gz. Run bash scripts/install.sh to install the operator  Run the command oc get pods to validate the install completed, should be able to see the operator related pod on default namespace   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.20, make sure to install v1 VolumeSnapshot CRDs On clusters running v1.18 \u0026 v1.19, make sure to install v1beta1 VolumeSnapshot CRDs   External Volume Snapshot Controller with correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be in active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running on Red Hat CoreOS , you can refer the URL https://coreos.com/os/docs/latest/iscsi.html#enable-automatic-iscsi-login-at-boot for additional information.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively you can check the status of multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally , you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer official documentation of multipath configuration.\nReplacing CSI Operator with Dell CSI Operator Dell CSI Operator was previously available, with the name CSI Operator, for both manual and OLM installation.\nCSI Operator has been discontinued and has been renamed to Dell CSI Operator. This is just a name change and as a result, the Kubernetes resources created as part of the Operator deployment will use the name dell-csi-operator instead of csi-operator.\nBefore proceeding with the installation of the new Dell CSI Operator, any existing CSI Operator installation has to be completely removed from the cluster.\nNote - This doesn’t impact any of the CSI Drivers which have been installed in the cluster\nIf the old CSI Operator was installed manually, then run the following command from the root of the repository which was used originally for installation\nbash scripts/undeploy.sh  If you don’t have the original repository available, then run the following commands\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator git checkout csi-operator-v1.0.0 bash scripts/undeploy.sh  Note - Once you have removed the old CSI Operator, then for installing the new Dell CSI Operator, you will need to pull/checkout the latest code\nIf you had installed old CSI Operator using OLM, then please follow un-installation instructions provided by OperatorHub. This will mostly involve:\n* Deleting the CSI Operator Subscription * Deleting the CSI Operator CSV  Install CSI Driver To install CSI drivers using Dell CSI Operator, please refer here\nNOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/storage-plugin-docs/v1/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\n For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page. For installing via OpenShift with the certified Operator, go to the OpenShift page. For installing manually, follow the instructions below.  Manual Installation Pre-requisites Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes cluster v1.17, v1.18, v1.19 OpenShift Clusters 4.5, 4.6 with RHEL 7.x \u0026 RHCOS worker nodes For upstream k8s clusters, make sure to install  Beta VolumeSnapshot CRDs (can be installed using the Operator installation script) External Volume Snapshot Controller     Note- For more insights or detailed pre-requisites refer https://github.com/dell/dell-csi-operator\n Steps  Clone the Dell CSI Operator repository Run ‘bash scripts/install.sh’ to install the operator  Run the command ‘oc get pods’ to validate the install completed  Should be able to see the operator related pod on default namespace     Driver Install via Dell CSI Operator For information on how to install the CSI drivers via the Dell CSI Operator, please refer to the sub-pages below for each driver.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/storage-plugin-docs/v2/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v1/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v2/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v1/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v2/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v1/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v2/grasp/","title":"Learn"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes 1.17 and generally available (v1) in Kubernetes version \u003e=1.20.\nIn order to use Volume Snapshots, ensure the following components with appropriate versions have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  ","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in …","ref":"/storage-plugin-docs/docs/concepts/","title":"Concepts"},{"body":"Volume Snapshot Feature The Volume Snapshot feature started in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and generally available (v1) in Kubernetes version 1.20.\nIn order to use Volume Snapshots, ensure the following components with appropriate versions have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  ","excerpt":"Volume Snapshot Feature The Volume Snapshot feature started in alpha …","ref":"/storage-plugin-docs/v1/concepts/","title":"Concepts"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v1/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v2/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/blog/news/","title":"News About Docsy"},{"body":"The partnership between Dell Technologies and Google to support Anthos as an on-prem/hybrid Kubernetes platform tightens and expands.\n Anthos 1.5  PowerMax v1.4 PowerStore v1.1   Anthos bare metal  PowerMax PowerStore asdf-bmctl   Go further  Anthos 1.5 First let us talk about Anthos 1.5 that runs on top of VMware hypervisor. Dell is a storage and platform partner since the version 1.1 and it continues !\nBoth drivers (csi-powermax and csi-powerstore are qualified for iSCSI.\nTo ensure the iSCSI daemon is started, you can use the following DaemonSet to take care of it :\nkubectl create -f https://raw.githubusercontent.com/coulof/ds-iscsi/master/ds-iscsi.yaml PowerMax v1.4 As discussed in that post we provide a new installer script for every driver.\nIt never have been easier to install the CSI driver on Anthos. To do so, simply follow the steps of the Product Guide and add --skip-verify for the install command line :\n./csi-install.sh --namespace powermax --values my-powermax-settings.yaml --skip-verify If you come from an existing installation, there is nothing else to do.\nPowerStore v1.1 For the first time, it is my pleasure to announce csi-powerstore qualifies for Anthos v1.5 for iSCSI protocol (NFS will come later).\nPowerStore storage fits particularly well workloads that are on the Edge. Same here the installation on Anthos is the same as what is documented in the product guide with the addition --skip-verify option:\n./csi-install.sh --namespace csi-powerstore --values ./my-powerstoresettings.yaml --skip-verify Anthos bare metal Google recently announced the Anthos for bare metal, which, as its name indicates, brings support for Anthos Kubernetes engine on bare-metal server. This is a great opportunity to leverage specialized hardware or get rid of any kind of constraint on the VM hypervisor.\nPowerMax Thanks to the CSI driver can take full advantage of your Fiber Channel infrastructure and PowerMax end-to-end NVMe capability on Anthos bare metal. That type of architecture fits well with IO intensive workload and business critical application, often tight to transactional data.\nCheckout the installation process in video:\n PowerStore The save level of service is given to PowerStore with a full support for Anthos bare metal.\nCheckout the installation process in video:\n asdf-bmctl During the qualification process I had to juggle with at least 3 different versions of the Anthos bare metal installer.\nBeing sick of doing symlinks anytime I needed to change version, I wrote an asdf plugin to list the available versions, install them, and attach an Anthos bare metal configuration to a specific version.\nYou can :\n install it with asdf plugin-add bmctl git@eos2git.cec.lab.emc.com:coulof/asdf-bmctl.git list the versions with asdf list-all bmctl install them with asdf install bmctl 1.6.0 and then you can set your version locally asdf local bmctl 1.6.0 or globally asdf global bmctl 1.6.0.  Go further If you need a demo or have any question on Dell CSI drivers with Anthos reach out the Dell container community website\n","excerpt":"The partnership between Dell Technologies and Google to support Anthos …","ref":"/storage-plugin-docs/blog/2020/10/30/google-anthos-announcements/","title":"Google Anthos announcements"},{"body":"The quaterly update for Dell CSI Driver is there !\n New features  Across portfolio Volume Cloning Volume Expansion online and offline Raw Block Support RedHat CoreOS Docker EE 3.1 Dell CSI Operator CSI Driver for PowerMax CSI Driver for PowerStore CSI Driver for PowerFlex   One more thing ; Ansible for PowerStore v1.1 Useful links  New features Across portfolio This release gives for every driver the :\n Support of OpenShift 4.4 as well as Kubernetes 1.17, 1.18, 1.19 Support for Kubernetes Volume Snapshot Beta API New installer !  With Volume Snapshot’s promotion to beta, one significant change is the CSI external-snapshotter sidecar has been split into two controllers, a common snapshot controller and a CSI external-snapshotter sidecar.\nThe new install script available under dell-csi-helm-installer/csi-install.sh will :\n By default, install of the external-snaphotter for CSI driver. Optionally, install the beta snapshot CRD when the option --snapshot-crd is set during the initial installation.  Most recent Kubernetes distributions like OpenShift or GKE come with the common snapshotter controller installed.\nFor Kubernetes vanilla, you have to deploy the common snapshotter manually. The instructions are available here.\n /!\\ The drivers have validated the external-snapshotter version 1.2 and not the bleeding-edge version\n Volume Cloning Volume cloning is now available for every driver, but PowerFlex (that feature is on the roadmap).\nIt never has been easier [to spin a new environement from the production](({% post_url {% post_url note/dell/2020-05-29-gitlab-powermax %})).\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clone-pvc-0spec:accessModes:- ReadWriteOnceresources:requests:storage:5GistorageClassName:powermaxdataSource:kind:PersistentVolumeClaimname:pvc-0In the PVC definition you must make sure the source of the clone has the same storageClassName, request.storage size, namespace and accessModes.\nVolume Expansion online and offline That feature was already present in csi-powerscale ; it is now available for every Dell CSI driver.\nTo expand a volume, you just have to edit the PV size ; blazing fast example below: {: .size-small}\nRaw Block Support The Raw Block Support was already available with csi-powermax ; it is now available in csi-vxflexos and csi-powerstore.\nThat feature can be used if your application needs a filesystem different from xfs or ext4 or applications that can take advantage of a block device (like HDFS, Oracle ASM, etc.).\nRedHat CoreOS But for PowerFlex, every driver has been qualified with OpenShift 4.3 and 4.4 on CoreOS type of nodes !\nDocker EE 3.1 Docker Enterprise Edition (now part of Mirantis) makes his appearance to the list of officially supported by support.dell.com Kubernetes distributions.\nThe first drivers to qualify Docker EE are : csi-powerscale, csi-unity and csi-vxflexos.\nDell CSI Operator The dell-csi-operator adds support for the installation of the csi-powerstore and the multi-array support for csi-unity.\n At the moment of the publication, the new operator is under the RedHat certification process to get official support. The version 1.1 is not available yet in OperatorHub.io or OpenShift UI. Stay tuned for the update.\n CSI Driver for PowerMax Upon installation, we can enable the CSI PowerMax Reverse Proxy service. The CSI PowerMax Reverse Proxy is a reverse proxy that forwards CSI driver requests to Unisphere servers.\nIt can be used to improve reliability by having redundant Unisphere, or scale-up the number of requests to be sent to Unisphere and the managed PowerMax arrays.\nCSI Driver for PowerStore The csi-powerstore adds NFS to the list of supported protocols. It has all the features that iSCSI and Fiber Channel storage classes have.\nIf you need concurrent filesystem access (i.e. ReadWriteMany access mode) you can use the NFS protocol.\nCSI Driver for PowerFlex The csi-vxflexos is the first driver to bring topology support. It avoids the driver tried to mount a volume when the SDC is not installed (I see you non-CoreOS support ;-))\nOne more thing ; Ansible for PowerStore v1.1 The biggest “hot new feature” is the support for file operation in ansible-powerstore; this means we have access to new modules for:\n File system Snapshot File system NAS server NFS export SMB Share Quota  And of course all the modules conform to Ansible Idempotency requirement.\nUseful links For more details you can check :\n The product guides and release notes in the repositories for csi-powermax, csi-powerscale, csi-powerstore, csi-unity, csi-vxflexos and ansible-powerstore. The FAQs on on Dell container community website:  FAQ for CSI Driver for PowerMax FAQ for CSI Driver for PowerScale FAQ for CSI Driver for PowerStore FAQ for CSI Driver for PowerFlex FAQ for CSI Driver for Unity    ","excerpt":"The quaterly update for Dell CSI Driver is there !\n New features …","ref":"/storage-plugin-docs/blog/2020/09/28/csi-drivers-volume-expansion-and-beta-snapshot-support-update/","title":"CSI drivers Volume expansion and beta Snapshot support update !"},{"body":"Every quarter Dell Technologies ships new versions of his CSI Drivers and Ansible modules.\n Dell EMC has anounced new set of CSI Drivers for their storage arrays. Some highliths for these June 2020 releases:\n Qualifications for OpenShift 4.3 and Kubernetes 1.16 for all the drivers Easy upgrade with the CSI Operator for all the drivers Helm 3 support for all the drivers Multi-array support for PowerMax and Unity NFS support for Unity Volume expansion for Isilon Volume cloning for PowerMax CHAP for PowerMax   For the Ansible modules you will have:\n a brand new Ansible module for Unity ! Ansible for Isilon v1.1 brings support for SmartQuotas and is compatible with next OneFS major version.   For more details you can check :\n The product guides and release notes in the repositories for csi-powermax v1.3, csi-isilon v1.2, csi-unity v1.2, csi-vxflexos v1.1.5, ansible-unity v1.0 and ansible-isilon v1.1 The FAQs on on Dell container community website:  FAQ for CSI Driver for PowerMax FAQ for CSI Driver for Unity FAQ for CSI Driver for VxFlexOS FAQ for CSI Driver for Isilon FAQ for Ansible Isilon    ","excerpt":"Every quarter Dell Technologies ships new versions of his CSI Drivers …","ref":"/storage-plugin-docs/blog/2020/06/15/june-2020-dell-storage-enablers-big-update/","title":"June 2020 Dell storage enablers big update !"},{"body":"PowerMax is now supported by Google Anthos 1.3\nKeep in mind the snapshot alpha version is not supported in GKE.\nMore details on the support matrix and installation steps, check the official announcement on Dell container community website.\n","excerpt":"PowerMax is now supported by Google Anthos 1.3\nKeep in mind the …","ref":"/storage-plugin-docs/blog/2020/05/27/anthos-1.3-qualification-for-powermax/","title":"Anthos 1.3 Qualification for PowerMax"},{"body":"Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  NOTE: If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Unsupported modifications Kubernetes doesn’t allow to update a storage class once it has been created. Any attempt to update a storage class will result in a failure.\n Note – Any attempt to rename a storage class or snapshot class will result in the deletion of older class and creation of a new class.\n Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator can’t update storage classes as it is prohibited by Kubernetes. Any attempt to do so will cause an error and the driver Custom Resource will be left in a Failed state. Refer the Troubleshooting section to fix the driver CR. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ncommon\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nstorageclass\nList of Storage Class fields\n name - name of the Storage Class default - Used to specify if the storage class will be marked as default (only set one storage class as default in a cluster) reclaimPolicy - Sets the PersistentVolumeReclaim Policy for the PVCs. Defaults to Delete if not specified parameters - driver specific parameters. Refer individual driver section for more details allowVolumeExpansion - Set to true for allowing volume expansion for PVC volumeBindingMode - Sets the VolumeBindingMode in the Storage Class. If left blank, it will be set to the default value for the driver version you are installing allowedTopologies - Sets the topology keys and values which allows the pods/and volumes to be scheduled on nodes that have access to the storage.  snapshotclass\nList of Snapshot Class specifications\n name - name of the snapshot class parameters - driver specific parameters. Refer individual driver section for more details  forceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"storageClass:- name:bronzedefault:truereclaimPolicy:Deleteparameters:SYMID:\"000000000001\"SRP:DEFAULT_SRPServiceLevel:BronzeYou can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nNote - The name of the Storage Class or the Volume Snapshot Class (which are created in the Kubernetes/OpenShift cluster) is created using the name of the driver and the name provided for these classes in the manifest. This is done in order to ensure that these names are unique if there are multiple drivers installed in the same cluster.\nFor e.g. - With the above sample manifest, the name of the storage class which is created in the cluster will be test-powermax-bronze.\nYou can get the name of the StorageClass and SnapshotClass created by the operator by running the commands - kubectl get storageclass and kubectl get volumesnapshotclass\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. Any modifications to this should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  ","excerpt":"Installing CSI Driver via Operator CSI Drivers can be installed by …","ref":"/storage-plugin-docs/docs/installation/operator/installdriver/","title":""},{"body":"Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  NOTE: If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Unsupported modifications Kubernetes doesn’t allow to update a storage class once it has been created. Any attempt to update a storage class will result in a failure.\n Note – Any attempt to rename a storage class or snapshot class will result in the deletion of older class and creation of a new class.\n Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator can’t update storage classes as it is prohibited by Kubernetes. Any attempt to do so will cause an error and the driver Custom Resource will be left in a Failed state. Refer the Troubleshooting section to fix the driver CR. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ncommon\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nstorageclass\nList of Storage Class fields\n name - name of the Storage Class default - Used to specify if the storage class will be marked as default (only set one storage class as default in a cluster) reclaimPolicy - Sets the PersistentVolumeReclaim Policy for the PVCs. Defaults to Delete if not specified parameters - driver specific parameters. Refer individual driver section for more details allowVolumeExpansion - Set to true for allowing volume expansion for PVC volumeBindingMode - Sets the VolumeBindingMode in the Storage Class. If left blank, it will be set to the default value for the driver version you are installing allowedTopologies - Sets the topology keys and values which allows the pods/and volumes to be scheduled on nodes that have access to the storage.  snapshotclass\nList of Snapshot Class specifications\n name - name of the snapshot class parameters - driver specific parameters. Refer individual driver section for more details  forceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"storageClass:- name:bronzedefault:truereclaimPolicy:Deleteparameters:SYMID:\"000000000001\"SRP:DEFAULT_SRPServiceLevel:BronzeYou can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nNote - The name of the Storage Class or the Volume Snapshot Class (which are created in the Kubernetes/OpenShift cluster) is created using the name of the driver and the name provided for these classes in the manifest. This is done in order to ensure that these names are unique if there are multiple drivers installed in the same cluster.\nFor e.g. - With the above sample manifest, the name of the storage class which is created in the cluster will be test-powermax-bronze.\nYou can get the name of the StorageClass and SnapshotClass created by the operator by running the commands - kubectl get storageclass and kubectl get volumesnapshotclass\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. Any modifications to this should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  ","excerpt":"Installing CSI Driver via Operator CSI Drivers can be installed by …","ref":"/storage-plugin-docs/v1/installation/operator/installdriver/","title":""},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/storage-plugin-docs/docs/archives/","title":"Archives"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/storage-plugin-docs/v1/archives/","title":"Archives"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/storage-plugin-docs/v2/archives/","title":"Archives"},{"body":"This is the blog section. It has two categories: News and Releases.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\n","ref":"/storage-plugin-docs/blog/","title":"Blog"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/storage-plugin-docs/docs/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor e.g. - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this -\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification\nThe above happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations At times because of inconsistencies in fetching data from the Kubernetes cache, state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/storage-plugin-docs/v1/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":" Welcome to Dell Technologies CSI driver for Kubernetes documentation! Learn More          ","excerpt":" Welcome to Dell Technologies CSI driver for Kubernetes documentation! …","ref":"/storage-plugin-docs/","title":"Dell Technologies"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Docker Universal Control Plane (UCP).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on UCP backed clusters may run any of the OSs which we support with upstream clusters.\nDocker EE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/storage-plugin-docs/v1/partners/docker/","title":"Docker EE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Docker Universal Control Plane (UCP).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on UCP backed clusters may run any of the OSs which we support with upstream clusters.\nDocker EE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/storage-plugin-docs/v2/partners/docker/","title":"Docker EE"},{"body":"NOTE: This doc version is no longer supported by us. You can check our latest version\n","excerpt":"NOTE: This doc version is no longer supported by us. You can check our …","ref":"/storage-plugin-docs/v1/","title":"Documentation"},{"body":"NOTE: This doc version is no longer supported by us. You can check our latest version\n","excerpt":"NOTE: This doc version is no longer supported by us. You can check our …","ref":"/storage-plugin-docs/v2/","title":"Documentation"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/storage-plugin-docs/docs/partners/docker/","title":"MKE"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repositories in order to create an offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  Building an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.4.0.000R dellemc/csi-isilon:v1.5.0 dellemc/csi-isilon:v1.6.0 dellemc/csipowermax-reverseproxy:v1.3.0 dellemc/csi-powermax:v1.5.0.000R dellemc/csi-powermax:v1.6.0 dellemc/csi-powermax:v1.7.0 dellemc/csi-powerstore:v1.2.0.000R dellemc/csi-powerstore:v1.3.0 dellemc/csi-powerstore:v1.4.0 dellemc/csi-unity:v1.4.0.000R dellemc/csi-unity:v1.5.0 dellemc/csi-unity:v1.6.0 dellemc/csi-vxflexos:v1.3.0.000R dellemc/csi-vxflexos:v1.4.0 dellemc/csi-vxflexos:v1.5.0 dellemc/dell-csi-operator:v1.4.0 dellemc/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 docker.io/busybox:1.32.0 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 quay.io/k8scsi/csi-resizer:v1.0.0 quay.io/k8scsi/csi-resizer:v1.1.0 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing an offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.4.0.000R dellemc/csi-isilon:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.5.0 dellemc/csi-isilon:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.6.0 dellemc/csipowermax-reverseproxy:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csipowermax-reverseproxy:v1.3.0 dellemc/csi-powermax:v1.5.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.5.0.000R dellemc/csi-powermax:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.6.0 dellemc/csi-powermax:v1.7.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.7.0 dellemc/csi-powerstore:v1.2.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.2.0.000R dellemc/csi-powerstore:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.3.0 dellemc/csi-powerstore:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.4.0 dellemc/csi-unity:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.4.0.000R dellemc/csi-unity:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.5.0 dellemc/csi-unity:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.6.0 dellemc/csi-vxflexos:v1.3.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.3.0.000R dellemc/csi-vxflexos:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.4.0 dellemc/csi-vxflexos:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.5.0 dellemc/dell-csi-operator:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/dell-csi-operator:v1.4.0 dellemc/sdc:3.5.1.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1-1 docker.io/busybox:1.32.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/busybox:1.32.0 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.1.0 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.1.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.0.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.1.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.2.1 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.3 k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.1.0 quay.io/k8scsi/csi-resizer:v1.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.0.0 quay.io/k8scsi/csi-resizer:v1.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.1.0 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.4.0.000R changing: dellemc/csi-isilon:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.5.0 changing: dellemc/csi-isilon:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.6.0 changing: dellemc/csipowermax-reverseproxy:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csipowermax-reverseproxy:v1.3.0 changing: dellemc/csi-powermax:v1.5.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.5.0.000R changing: dellemc/csi-powermax:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.6.0 changing: dellemc/csi-powermax:v1.7.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.7.0 changing: dellemc/csi-powerstore:v1.2.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.2.0.000R changing: dellemc/csi-powerstore:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.3.0 changing: dellemc/csi-powerstore:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.4.0 changing: dellemc/csi-unity:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.4.0.000R changing: dellemc/csi-unity:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.5.0 changing: dellemc/csi-unity:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.6.0 changing: dellemc/csi-vxflexos:v1.3.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.4.0 changing: dellemc/csi-vxflexos:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.5.0 changing: dellemc/dell-csi-operator:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/dell-csi-operator:v1.4.0 changing: dellemc/sdc:3.5.1.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1 changing: dellemc/sdc:3.5.1.1-1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1-1 changing: docker.io/busybox:1.32.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/busybox:1.32.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.0.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.1.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.2.1 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.0.1 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.1.0 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.2.0 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.0.2 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.1.0 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.2.1 changing: k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.2.0 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.2 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.3 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.0.0 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.1.0 changing: quay.io/k8scsi/csi-resizer:v1.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.0.0 changing: quay.io/k8scsi/csi-resizer:v1.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.1.0 * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\n Offline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default.  ","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/storage-plugin-docs/docs/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple linux based systems may be required to create and process an offline bundle for use.\n One linux based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One linux based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repos in order to create and offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  Building an offline bundle This needs to be performed on a linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to a image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing a offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 changing: dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R changing: dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R changing: dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R changing: dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R changing: dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R changing: dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R changing: dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R changing: dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R changing: quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 changing: quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 changing: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 changing: quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 changing: quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 changing: quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 changing: quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTE: Installation should be done using the files that was obtained after unpacking the bundle as the image tags in the manifests are modifed to point to the internal registry.\n","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/storage-plugin-docs/v1/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple linux based systems may be required to create and process an offline bundle for use.\n One linux based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One linux based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repos in order to create and offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking an offline bundle and preparing for installation Perform either a Helm installation or Operator installation  Building an offline bundle This needs to be performed on a linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nThe build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking an offline bundle and preparing for installation This needs to be performed on a linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to a image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing a offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 changing: dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R changing: dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R changing: dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R changing: dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R changing: dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R changing: dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R changing: dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R changing: dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R changing: quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 changing: quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 changing: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 changing: quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 changing: quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 changing: quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 changing: quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Complete Perform either a Helm installation or Operator installation Now that the required images have been made available and the Helm Charts/Operator configuration updated, installation can proceed by following the usual installation procedure as documented.\n","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/storage-plugin-docs/v2/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"Release Notes - Dell CSI Operator 1.4.0  Note: There will be a delay in certification of Dell CSI Operator 1.4.0 and it will not be available for download from the Red Hat OpenShift certified catalog. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.4.0 release.\n New Features/Changes  Added support for Kubernetes v1.21 Deprecated support for Kubernetes v1.18 Migrated to Operator SDK v1.5.0 Deprecated Storage Class Creation and Support Deprecated Volume Snapshot Class Creation and Support  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     A warning message will be listed in the events for cluster scoped objects if the driver is not upgraded after an operator upgrade. This happens because of the fix provided by Kubernetes in 1.20 for one of the known issue. After an operator upgrade, the objects will get updated automatically after 45 mins in case of no driver upgrade.    Support The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.4.0  Note: There will be a delay …","ref":"/storage-plugin-docs/docs/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.3.0  Note: There will be a delay in certification of Dell CSI Operator 1.3.0 and it will not be available for download from the Red Hat OpenShift certified catalog. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.3.0 release.\n New Features/Changes  Added support for OpenShift 4.6, 4.7 with RHEL and CoreOS worker nodes Added support for Upstream Kubernetes cluster v1.18, v1.19, v1.20 Migrated to Operator SDK 1.0 Added support for CSI Ephemeral Inline Volumes Changed driver controller installation from StatefulSet to Deployment Added Support for multiple replicas for driver controller Deployment Added Support for setting volumeBindingMode for Storage Classes Added Support for setting topology keys for Storage Classes  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     A warning message will be listed in the events for StorageClasses if the driver is not upgraded after an operator upgrade. This happens because of the fix provided by Kubernetes in 1.20 for one of the known issue. StorageClasses will get updated automatically after 45 mins if there is no driver upgrade, after an operator upgrade.    Support The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.3.0  Note: There will be a delay …","ref":"/storage-plugin-docs/v1/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.2.0  Note: There is a delay in Operator 1.2.0 certification hence it will not be visible in Red Hat OpenShift certified catalogue immediately after release on GitHub.\n New Features/Changes  Added support for OpenShift 4.5, 4.6 with RHEL and CoreOS worker nodes Migrated to Operator SDK 1.0 Added support for CSI Ephemeral Inline Volumes Changed driver controller installation from StatefulSet to Deployment Added Support for multiple replicas for driver controller Deployment Added Support for setting volumeBindingMode for Storage Classes Added Support for setting topology keys for Storage Classes  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no Known issues in this release.\nSupport The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.2.0  Note: There is a delay in …","ref":"/storage-plugin-docs/v2/release/operator/","title":"Operator"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes 1.17 and is generally available (v1) in Kubernetes version \u003e=1.20.\nThe CSI PowerFlex driver version 1.5 supports v1beta1 snapshots on Kubernetes 1.19 and v1 snapshots on Kubernetes 1.20 and 1.21.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class Before PowerFlex driver v1.5, the installation of the driver created default instance of VolumeSnapshotClass. API version of this VolumeSnapshotClass instance was defined based on Kubernetes version, as below:\nFollowing is the manifest for the Volume Snapshot Class created during installation, prior to PowerFlex driver v1.5:\n{{- if eq .Values.kubeversion \"v1.20\" }} apiVersion: snapshot.storage.k8s.io/v1 {{- else }} apiVersion: snapshot.storage.k8s.io/v1beta1 {{- end}} kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Installation of PowerFlex driver v1.5 does not create VolumeSnapshotClass. You can find samples of default v1beta1 and v1 VolumeSnapshotClass instances in helm/samples/volumesnapshotclass directory. There are two samples, one for v1beta1 version and the other for v1 snapshot version. If needed, install appropriate default sample, based on the version of snapshot CRDs in your cluster.\nCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. PowerFlex driver implements this extension in v1.5. This feature is currently in Technical Preview. To use this feature users have to deploy csi-volumegroupsnapshotter side-car as part of the PowerFlex driver.\nMore details can be found here: dell-csi-volumegroup-snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound  NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\n Volume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 supports additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com  WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.  Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\n NOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\n Controller HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install the controller on worker nodes tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install the controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node.  Multiarray Support The CSI PowerFlex driver version 1.5 adds support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file under the top directory named config.yaml with the following content:\n- username:\"admin\"# username for connecting to APIpassword:\"password\"# password for connecting to APIsystemID:\"ID1\"# system ID for systemendpoint:\"https://127.0.0.1\"# full URL path to the PowerFlex APIskipCertificateValidation:true# skip array certificate validation or notisDefault:true# treat current array as default (would be used by storage class without arrayIP parameter)mdm:\"10.0.0.1,10.0.0.2\"# MDM IPs for the system- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"Here we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nCreating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nFind the sample yaml files under helm/samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID you have.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nDynamic Logging Configuration This feature is introduced in version 1.5, CSI Driver for PowerFlex now supports dynamic logging configuration.\nTo accomplish this, we utilize two fields in logConfig.yaml: LOG_LEVEL and LOG_FORMAT.\nLOG_LEVEL: minimum level that the driver will log LOG_FORMAT: format the driver should log in, either text or JSON  NOTE: To see the available options for LOG_LEVEL, consult: https://github.com/sirupsen/logrus#level-logging\n If the configmap does not exist yet, simply edit logConfig.yaml, changing the values for LOG_LEVEL and LOG_FORMAT as you see fit.\nIf the configmap already exists, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos driver-config\nor you could edit logConfig.yaml, and use this command:\nkubectl apply -f logConfig.yaml\nand then make the necessary adjustments for LOG_LEVEL and LOG_FORMAT.\nIf LOG_LEVEL or LOG_FORMAT are set to options outside of what is supported, the driver will use the default values of “info” and “text” .\n","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in …","ref":"/storage-plugin-docs/docs/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell EMC PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nSDC Deployment The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS).\nOn Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article (https://www.dell.com/support/kbdoc/en-us/000184206/how-to-use-a-private-repository-for) has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes 1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to a new image.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the config.yaml file in the top-level directory.\n  Prepare the config.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system. true -   password Password for accessing PowerFlex system. true -   systemID System name/ID of PowerFlex system. true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system. true -   skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true -    Example: config.yaml\n- username:\"admin\"password:\"password\"systemID:\"ID1\"endpoint:\"https://127.0.0.1\"skipCertificateValidation:trueisDefault:truemdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you update the secret, you will have to reinstall the driver. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yaml format and replace/update the secret.    Create the config map for use by the driver. The config map controls the level and format of the driver’s logging. To create it:\nkubectl create -f logConfig.yaml\nTo see possible configuration options, see the ‘Dynamic Logging Configuration” section in Features.\n  If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:\n     Parameter Description Required Default     volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No “k8s”   controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No FALSE   controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \" \"   monitor This section allows the configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled  No FALSE   vgsnapshotter Volume Group Snapshotter(vgsnapshotter) is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled  No FALSE    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml  NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script runs the verify-csi-vxflexos.sh script that is present in the same directory. It will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container This script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. It is mandatory to run the first installation and installation after changes to MDM configuration in vxflexos-config secret without skipping the verification. After that, you can use --skip-verify-node or --skip-verify . (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerFlex v1.4 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n NOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the helm/samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.json. An example of config.json is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflex_v140_ops_46.yaml sideCars:# Uncomment the following section if you want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"initContainers:- image:dellemc/sdc:3.5.1.1imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"\"Manual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system true -   password Password for accessing PowerFlex system true -   systemID System name/ID of PowerFlex system true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system true -   insecure Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma true -    Example: config.json\n[ { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID1\", \"endpoint\": \"https://127.0.0.1\", \"insecure\": true, \"isDefault\": true, \"mdm\": \"10.0.0.1,10.0.0.2\" }, { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID2\", \"endpoint\": \"https://127.0.0.2\", \"insecure\": true, \"mdm\": \"10.0.0.3,10.0.0.4\" } ] After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.json\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.json -o yaml --dry-run=client | kubectl replace -f -\nNote:\n The user needs to validate the JSON syntax and array-related key/values while replacing the vxflexos-creds secret. If you update the secret, you must reinstall the driver. System ID, MDM configuration, etc. now are taken directly from config.json. MDM provided in the input_sample_file.yaml will be overidden with MDM values in config.json.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/docs/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in helm/samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update betasnap1.yaml and betasnap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in betasnap1.yaml and betasnap2.yaml. The following are the contents of betasnap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snap1namespace:helmtest-vxflexosspec:volumeSnapshotClassName:vxflexos-snapclasssource:persistentVolumeClaimName:pvol0 NOTE: apiVersion in both betasnap yamls must match apiVersion in your VolumeSnapshotClass, if version in VolumeSnapshotClass is v1, adjust apiVersion like so: apiVersion: snapshot.storage.k8s.io/v1 in betasnap1.yaml and betasnap2.yaml\n Results\nThe snaptest.sh script will create a snapshot using the definitions in the betasnap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `helm/samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n NOTE: apiVersion in both betasnap yamls must match apiVersion in your VolumeSnapshotClass, if version in vxflexos-snapclass is v1, adjust apiVersion like so: apiVersion: snapshot.storage.k8s.io/v1\n  Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update betasnap1.yaml and betasnap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using betasnap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snap1kind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in betasnap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/docs/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.5.0 New Features/Changes  Added support for Kubernetes v1.21 Added support for OpenShift 4.6 EUS with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.4 Added support for PowerFlex 3.6 Added support for custom file system format options Added support for dynamic log configuration Removed volume snapshot classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","excerpt":"Release Notes - CSI PowerFlex v1.5.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/docs/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.20/v1.21 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/v1.21 requires v1 version of snapshot CRDs. If on Kubernetes 1.20/1.21 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements   Driver pods are not ready, with the error message: MountVolume.SetUp failed for volume \"log-config\" : configmap \"driver-config\" not found Create the configmap:  kubectl create -f logConfig.yaml    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/storage-plugin-docs/docs/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes 1.17 and is generally available (v1) in Kubernetes version 1.20.\nThe CSI PowerFlex driver version 1.4 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI PowerFlex 1.4 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\n{{- if eq .Values.kubeversion \"v1.20\" }} apiVersion: snapshot.storage.k8s.io/v1 {{- else }} apiVersion: snapshot.storage.k8s.io/v1beta1 {{- end}} kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, that is, when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIn case you are creating more storage classes, ensure that this attribute is set to true if you wish to expand any Persistent Volumes created using these new storage classes.\nFollowing is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce , ReadWriteMany , and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nTopology Support The CSI PowerFlex driver version 1.2 and later support Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that pod scheduling takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If controller count is greater than the number of available nodes, excess controller pods will be stuck in pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run node portion of CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run node portion of CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment . If there is a SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes which do not support automatic SDC deployment by SDC init container, manuall installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstall of SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from node.  Multiarray Support The CSI PowerFlex driver version 1.4 adds support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample json file under the top directory named config.json with the following content:\n[ { \"username\": \"admin\", # username for connecting to API \"password\": \"password\", # password for connecting to API \"systemID\": \"ID1\",\t# system ID for system \"endpoint\": \"http://127.0.0.1\", # full URL path to the PowerFlex API \"insecure\": true, # use insecure connection or not \"isDefault\": true, # treat current array as default (would be used by storage class without arrayIP parameter) \"mdm\": \"10.0.0.1,10.0.0.2\" # MDM IP for the system }, { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID2\", \"endpoint\": \"https://127.0.0.2\", \"insecure\": true, \"mdm\": \"10.0.0.3,10.0.0.4\" } ] Here we specify that we want CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json\nCreating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nFind the sample yaml files under helm/samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID you have.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume The CSI PowerFlex driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest will create a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test will deploy the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\n","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in …","ref":"/storage-plugin-docs/v1/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Configure Mount propagation on container runtime (example: Docker) Install PowerFlex Storage Data Client Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. See Dell EMC PowerFlex documentation for more information to configure this setting.\nConfigure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerFlex. The following is instruction on how to do this with Docker. If you use another container runtime, follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n The service section of /etc/systemd/system/multi-user.target.wants/docker.service needs to be edited in a few places. First, the Requires entry under the [Unit] header needs have docker.service added to it, as shown. Second, MountFlags=shared needs to be added under the [Service] header. [Unit] ... Requires=docker.socket containerd.service docker.service [Service] ... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run node portion of CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nSDC Deployment The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run node portion of CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS).\nOn Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC ftp site, which is setup by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article (https://www.dell.com/support/kbdoc/en-us/000184206/how-to-use-a-private-repository-for) has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Volume Snapshot Requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to new image.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the config.json file in the top-level directory.\n  Prepare the config.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system true -   password Password for accessing PowerFlex system true -   systemID System name/ID of PowerFlex system true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system true -   insecure Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be an list of MDM IP addresses or hostnames separated by comma true -    Example: config.json\n[ { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID1\", \"endpoint\": \"http://127.0.0.1\", \"insecure\": true, \"isDefault\": true, \"mdm\": \"10.0.0.1,10.0.0.2\" }, { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID2\", \"endpoint\": \"https://127.0.0.2\", \"insecure\": true, \"mdm\": \"10.0.0.3,10.0.0.4\" } ] After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the JSON syntax and array related key/values while replacing the vxflexos-creds secret. If you update the secret, you will have to reinstall the driver. System ID, MDM configuration etc. now are taken directly from config.json, and no longer the values file.    If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:\n     Parameter Description Required Default     volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No “k8s”   controllerCount Set to deploy multiple controller instances. If controller count is greater than the number of available nodes, excess pods will be left in pending state. You can increase number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No FALSE   controller This section allows configuration of controller specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \" \"   monitor This section allows configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information - -   enabled  No FALSE    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml  NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script runs verify-csi-vxflexos.sh script that is present in the same directory. It will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by init container and sdc-monitor container This script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. It is mandatory to run the first installation and installation after changes to MDM configuration in vxflexos-config secret without skipping the verification. After that you can use --skip-verify-node or --skip-verify . (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes Starting in CSI PowerFlex v1.4, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerFlex v1.4 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/storage-plugin-docs/v1/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex v1.4.0 can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non supported verisons of the OS also do the manual SDC deployment steps given below. Refer https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When driver is created, MDM value for initContainers in driver CR is set by operator from mdm attributes in driver configuration file, config.json. Example of config.json is below in this document. Do not set MDM value for initContainers in driver CR file manually. Note: To use a sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflex_v140_ops_46.yaml sideCars:# Uncomment the following section if you want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"initContainers:- image:dellemc/sdc:3.5.1.1imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"\"Manual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system true -   password Password for accessing PowerFlex system true -   systemID System name/ID of PowerFlex system true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system true -   insecure Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be an list of MDM IP addresses or hostnames separated by comma true -    Example: config.json\n[ { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID1\", \"endpoint\": \"http://127.0.0.1\", \"insecure\": true, \"isDefault\": true, \"mdm\": \"10.0.0.1,10.0.0.2\" }, { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID2\", \"endpoint\": \"https://127.0.0.2\", \"insecure\": true, \"mdm\": \"10.0.0.3,10.0.0.4\" } ] After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.json\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.json -o yaml --dry-run=client | kubectl replace -f -\nNote:\n The user needs to validate the JSON syntax and array related key/values while replacing the vxflexos-creds secret. If you update the secret, you must reinstall the driver. System ID, MDM configuration etc. now are taken directly from config.json. MDM provided in the input_sample_file.yaml will be overided with MDM values in config.json.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here .\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If the number of controller pods are greater than number of available nodes, excess pods will become stay in a pending state. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false   StorageClass parameters      storagePool Defines the PowerFlex storage pool from which this driver will provision volumes. You must set this for the primary storage pool to be used Yes pool1   allowVolumeExpansion Once the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No true   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the X_CSI_VXFLEXOS_SYSTEMNAME in the key with the actual systemname value No X_CSI_VXFLEXOS_SYSTEMNAME   initContainers:value Set the MDM IP’s here if installing on CoreOS to enable automatic SDC installation Yes (OpenShift) “10.xx.xx.xx,10.xx.xx.xx”      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/v1/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use one of the pre-defined storage classes created by the CSI Driver for Dell EMC PowerFlex installation process. This step yields a mounted ext4 file system. You can see the storage class definitions in the PowerFlex installation helm chart files storageclass.yaml and storageclass-xfs.yaml. If you compare pvol0.yaml and pvol1.yaml , you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1alpha1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:helmtest-vxflexosspec:snapshotClassName:vxflexos-snapclasssource:name:pvolkind:PersistentVolumeClaimResults\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0 , then the created snapshot is named pvol0-snap.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n test.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation creates this class as its default snapshot class. You can see its definition in the installation directory file volumesnapshotclass.yaml.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updateddirectory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim , which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/v1/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.4.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Added support for Fedora CoreOS Added SDC deployment on Fedora CoreOS nodes Added support for Ephemeral Inline Volume Added support multi-mount volumes Added support for managing multiple PowerFlex arrays from one driver Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.18.5+   Installation warning: “OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6” Ignore this warning and continue with the installation. v1.4.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI PowerFlex v1.4.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/v1/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that needs to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs shows that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs shows that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/storage-plugin-docs/v1/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver version 1.2 and later support beta snapshots. Earlier versions of the driver supported alpha snapshots.\nVolume Snapshots feature in Kubernetes has moved to beta in Kubernetes version 1.17. It was an alpha feature in earlier releases (1.13 onwards). The snapshot API version has changed from v1alpha1 to v1beta1 with this migration.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI PowerFlex 1.3 driver, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, that is, when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIn case you are creating more storage classes, make sure that this attribute is set to true if you wish to expand any Persistent Volumes created using these new storage classes.\nFollowing is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the closest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce , ReadWriteMany , and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nTopology Support The CSI PowerFlex driver version 1.2 and later support Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer defined topology, users cannot create their own labels for nodes and storage classed and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that pod scheduling takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If controller count is greater than the number of available nodes, excess controller pods will be stuck in pending state.\n If you’re using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nAutomated SDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Red Hat CoreOS (RHCOS) nodes in an OpenShift cluster. Only RHCOS is supported at this time. The deployment of the SDC kernel module on RHCOS nodes is done via an init container. Automated installation is supported in both via Helm and Dell CSI Operator based installs. The following describes further details of this feature:\n On RHCOS nodes, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the node. If there is a SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On non-RHCOS nodes, the SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC There is no automated uninstall of SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from node.  ","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver version 1.2 and later …","ref":"/storage-plugin-docs/v2/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 Enable Zero Padding on PowerFlex Configure Mount propagation on container runtime (i.e. Docker) Install PowerFlex Storage Data Client Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. See Dell EMC PowerFlex documentation for more information to configure this setting.\nConfigure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerFlex. The following is instruction on how to do this with Docker. If you use another container runtime, follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Note: Some distribution, like Ubuntu, already has MountFlags set by default\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all worker nodes. If installing on Red Hat CoreOS (RHCOS) nodes on OpenShift you can install using the automated SDC deployment feature. If installing on non-RHCOS nodes, you must install SDC manually.\nAutomatic SDC Deployment The automated deployment of the SDC runs by default when installing the driver. It installs an SDC container to faciliate the installation. While the install is automated there are a few configuration options for this feature. Those are referenced in the Install the Driver section. More details on how the automatic SDC deployment works can be found in the Feature section of this site on the PowerFlex page.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC ftp site, which is setup by default. Some users might want to mirror this repository to a local location. The PowerFlex documentation has instructions on how to do this. If a mirror is used, you need to create an SDC repo secret for managing the credentials to the mirror. Details on how to create the secret are in the Install the Driver section.\nManually SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.    Volume Snapshot requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nYou can also install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script. If you are installing the driver using the Dell CSI Operator, there is a helper script provided to install the snapshot CRDs - scripts/install_snap_crds.sh.\nVolume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository. Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one. Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to new image. Edit the helm/secret.yaml, point to correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername \u0026 mypassword are credentials for a user with PowerFlex priviledges.\n Create the secret by running kubectl create -f secret.yaml If not using automated SDC deployment, create a dummy SDC repo secret file: kubectl create -f sdc-repo-secret.yaml If using automated SDC deployment:  Check the SDC container image is the correct version for your version of PowerFlex. Create a secret for the SDC repo credentials and provide the URL for the repo.  To create the secret, you must update the details in helm/sdc-repo-secret.yaml file and running kubectl create -f sdc-repo-secret.yaml. To set the repo URL, you must set the repoUrl parameter in the myvalues.yaml file.     Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the myvalues.yaml file.  NOTE: Your SDC might have multiple VxFlex OS systems registered. Ensure that you choose the correct values.   Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:     Parameter Description Required Default     systemName Set to the PowerFlex/VxFlex OS system name or system ID to be used with the driver. Yes “systemname”   restGateway Set to the URL of your system’s REST API Gateway. You can obtain this value from the PowerFlex administrator. Yes “https://123.0.0.1”   storagePool Set to a default (existing) storage pool name in your PowerFlex/VxFlex OS system. Yes “sp”   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No “k8s”   controllerCount Set to deploy multiple controller instances. If controller count is greater than the number of available nodes, excess pods will be left in pending state. You can increase number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   StorageClass Helm charts create a Kubernetes StorageClass while deploying CSI Driver for Dell EMC PowerFlex. This section includes relevant variables. - -   name Defines the name of the Kubernetes storage class that the Helm charts will create. For example, the vxflexos base name will be used to generate names such as vxflexos and vxflexos-xfs. No “vxflexos”   isDefault Sets the newly created storage class as default for Kubernetes. Set this value to true only if you expect PowerFlex to be your principle storage provider, as it will be used in PersitentVolumeClaims where no storageclass is provided. After installation, you can add custom storage classes, if desired. No TRUE   reclaimPolicy Defines whether the volumes will be retained or deleted when the assigned pod is destroyed. The valid values for this variable are Retain or Delete. No “Delete”   controller This section allows configuration of controller specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \" \"   monitor This section allows configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   sdcKernelMirror [RHCOS only] The PowerFlex SDC may need to pull a new module that is known to work with newer Linux kernels. The default location of this mirror os at ftp.emc.com. The PowerFlex documentation has instructions for methods to mirror this repository to a local location if necessary. - -   repoUrl Set the URL of the ftp mirror containing SDC kernel modules. Only ftp locations are allowed. A blank string signifies the default mirror, which is “ftp://ftp.emc.com”. No \" \"   11. Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml       NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script also runs the verify.sh script that is present in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. You can also skip the verification step by specifiying the --skip-verify-node option. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/storage-plugin-docs/v2/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: Automated SDC Deployment for Operator  This applies to OpenShift with RHCOS Nodes Only. This feature deploys the sdc kernel modules on CoreOS nodes with the help of an init container. Required: MDM value need to be provided in CR file for the sdc init container to work. Expect error if not in proper format. To use a specific image from ftp site, pass in repo url, repo password and repo username.  Repo username and repo password are to be encrypted by a secret and passed in. Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml.   Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflexos_v130_ops_46.yaml #sideCars:# Uncomment the following section if you want to run the monitoring sidecar# - name: sdc-monitor# envs:# - name: HOST_PID# value: \"1\"initContainers:- image:dellemc/sdc:3.6.0.176-3.5.1000.176imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.xx.xx.xx,10.xx.xx.xx\"Install Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace. Create PowerFlex credentials: Create a file called vxflexos-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:vxflexos-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f vxflexos-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerFlex using the sample files provided here . Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If controller pods is greater than number of available nodes, excess pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_SYSTEMNAME Defines the name of the PowerFlex system from which volumes will be provisioned. This must either be set to the PowerFlex system name or system ID Yes systemname   X_CSI_VXFLEXOS_ENDPOINT Defines the PowerFlex REST API endpoint, with full URL, typically leveraging HTTPS. You must set this for your PowerFlex installations REST gateway Yes https://127.0.0.1   CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   StorageClass parameters      storagePool Defines the PowerFlex storage pool from which this driver will provision volumes. You must set this for the primary storage pool to be used Yes pool1   allowVolumeExpansion Once the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No true   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the X_CSI_VXFLEXOS_SYSTEMNAME in the key with the actual systemname value No X_CSI_VXFLEXOS_SYSTEMNAME   initContainers:value Set the MDM IP’s here if installing on CoreOS to enable automatic SDC installation Yes (OpenShift) “10.xx.xx.xx,10.xx.xx.xx”     Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/v2/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use one of the pre-defined storage classes created by the CSI Driver for Dell EMC PowerFlex installation process. This step yields a mounted ext4 file system. You can see the storage class definitions in the PowerFlex installation helm chart files storageclass.yaml and storageclass-xfs.yaml. If you compare pvol0.yaml and pvol1.yaml , you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion: snapshot.storage.k8s.io/v1alpha1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: helmtest-vxflexos spec: snapshotClassName: vxflexos-snapclass source: name: pvol kind: PersistentVolumeClaim Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0 , then the created snapshot is named pvol0-snap.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n test.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation creates this class as its default snapshot class. You can see its definition in the installation directory file volumesnapshotclass.yaml.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updateddirectory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim , which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/v2/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.3.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added automatic SDC deployment on OpenShift CoreOS nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for volume cloning Added support for Controller high availability (multiple-controllers)  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+    ","excerpt":"Release Notes - CSI PowerFlex v1.3.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/v2/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that needs to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs shows that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs shows that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on worker node, and ensure your container run time manager is properly configured to be   utilized with SElinux.    Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/storage-plugin-docs/v2/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Multi Unisphere Support Starting v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in the StandAlone mode along with the driver. For more details on how to configure the driver along with the ReverseProxy, please refer the section here\nVolume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and was generally available (v1) in Kubernetes version \u003e= 1.20.\nThe CSI PowerMax driver version 1.7 supports v1beta1 snapshots on Kubernetes 1.19 and v1 snapshots on Kubernetes 1.20/1.21.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the helm/samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\n It can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod.  It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller Pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n A set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\n","excerpt":"Multi Unisphere Support Starting v1.7, the CSI PowerMax driver can …","ref":"/storage-plugin-docs/docs/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume CSI PowerMax ReverseProxy (optional)  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port group names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For more information about configuring iSCSI, you can refer Dell EMC Host Connectivity guide.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in the port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support beta Volume snapshots in Kubernetes 1.17 or later:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes v1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml file, point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges.\n Create the secret by running kubectl create -f helm/secret.yaml. If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty   global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - -   defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds   storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - -   storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001   endpoint This refers to the URL of the Unisphere server managing storageArrayId Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443   backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. No https://backup-1.unisphe.re:8443   managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - -   endpoint This refers to the URL of the Unisphere server Yes https://primary-1.unisphe.re:8443   credentialsSecret This refers to the user credentials for endpoint No primary-1-secret   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   limits This refers to various limits for Reverse Proxy No -   maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5   maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4   maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50   maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50   csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   image This refers to the image of the CSI Powermax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v1.3.0   tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret   deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222   mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone”    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes Starting CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the helm/samples/storageclass folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerMax v1.5 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nVolume Snapshot Class Starting CSI PowerMax v1.7, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the helm/samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver without Proxy In this mode, the CSI PowerMax driver can only connect to a single Unisphere server. So, you just specify a list of storage arrays and the address of the Unisphere server\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://unisphere-address:8443 Note: If you provide multiple endpoints in the list of management servers, the installer will only use the first server in the list\n CSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy just acts as a passthrough for the RESTAPI calls and only provides limited functionality like rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://primary-unisphere:8443skipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-unisphere:8443#Optional# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.3.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\n CSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you the capability to connect to Multiple Unisphere servers. You can specify primary \u0026 backup Unisphere servers for each storage array. In case you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"endpoint:https://primary-1.unisphe.re:8443backupEndpoint:https://backup-1.unisphe.re:8443- storageArrayId:\"000000000002\"endpoint:https://primary-2.unisphe.re:8443backupEndpoint:https://backup-2.unisphe.re:8443managementServers:- endpoint:https://primary-1.unisphe.re:8443credentialsSecret:primary-1-secretskipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-1.unisphe.re:8443credentialsSecret:backup-1-secretskipCertificateValidation:true- endpoint:https://primary-2.unisphe.re:8443credentialsSecret:primary-2-secretskipCertificateValidation:true- endpoint:https://backup-2.unisphe.re:8443credentialsSecret:backup-2-secretskipCertificateValidation:true# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.3.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n ","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_MANAGED_ARRAYS List of comma-separated array id(s) which will be managed by the driver Yes -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: “a-b-c-%foo%-xyz” where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if the node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false     Execute the following command to create PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.   Note: There is a new mandatory env - X_CSI_MANAGED_ARRAYS which has to be set while installing or upgrading the driver.\n CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component that can be installed along with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with the CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec  tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config   Note: Only one of the Linked or StandAlone config needs to be supplied. The appropriate mode needs to set in the spec as well.\n Here is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.3.0# \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:truestandAloneConfig:# Set mode to \"StandAlone\" in order to use this configstorageArrays:- storageArrayId:\"000000000001\"# Unisphere server managing the PowerMax arrayprimaryURL:https://unisphere-1-addr:8443# proxyCredentialSecrets are used by the clients of the proxy to connect to it# If using proxy in the stand alone mode, then the driver must be provided the# same secret.# The format of the proxy credential secret are exactly the same as the unisphere credential secret# For using the proxy with the driver, use the same proxy credential secrets for# all the managed storage arraysproxyCredentialSecrets:- proxy-creds- storageArrayId:\"000000000002\"primaryURL:https://unisphere-2-addr:8443# An optional backup Unisphere server managing the same array# This can be used by the proxy to fall back to in case the primary# Unisphere is inaccessible temporarilybackupURL:unisphere-3-addr:8443proxyCredentialSecrets:- proxy-credsmanagementServers:- url:https://unisphere-1-addr:8443# Secret containing the credentials of the Unisphere serverarrayCredentialSecret:unsiphere-1-credsskipCertificateValidation:true- url:https://unisphere-2-addr:8443arrayCredentialSecret:unsiphere-2-credsskipCertificateValidation:true- url:https://unisphere-3-addr:8443arrayCredentialSecret:unsiphere-3-credsskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  There is a new sample file - powermax_revproxy_standalone_with_driver.yaml present in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\n Note: dell-csi-operator doesn’t support the installation of CSI PowerMax ReverseProxy as a sidecar to the controller pod. This facility is only present with the dell-csi-helm-installer\n ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/docs/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are created storageclass names like  and . You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file betaSnap1.yaml/snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/docs/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.7.0 New Features/Changes  Removed Volume Snapshotclass from helm template Added support for Multi Unisphere Added support for Kubernetes v1.21 Added support for Docker MKE 3.4.0 Added support for RHEL 8.4  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete Volume fails with the error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Getting initiators list fails with context deadline error The following error can occur during the driver installation if a large number of initiators are present on the array. There is no workaround for this but can be avoided by deleting stale initiators on the array    ","excerpt":"Release Notes - CSI PowerMax v1.7.0 New Features/Changes  Removed …","ref":"/storage-plugin-docs/docs/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/storage-plugin-docs/docs/troubleshooting/powermax/","title":"PowerMax"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and was generally available (v1) in Kubernetes version 1.20.\nThe CSI PowerMax driver version 1.6 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI PowerMax 1.6 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class you will need and there is no need to create any other Volume Snapshot Class.\nThe following is the manifest for the Volume Snapshot Class created during installation (using the default driver name):\napiVersion: snapshot.storage.k8s.io/v1 deletionPolicy: Delete kind: VolumeSnapshotClass metadata: name: powermax-snapclass driver: csi-powermax.dellemc.com Note: The apiVersion for VolumeSnapshotClass object created on clusters running Kubernetes versions \u003c 1.20 will be v1beta1\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax , then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions which should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports expansion of Persistent Volumes (PVs). This expansion is done online, that is, when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThis is a sample manifest for a storage class which allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere forPowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server which acts as a reverse proxy for the Unisphere forPowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and performance of the CSI PowerMax driver.\nOptionally you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation CSI PowerMax Reverse Proxy is installed as a Kubernetes deployment in the same namespace as the driver.\nIt is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is a HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be a X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1 , then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps Kubernetes scheduler place PVCs on worker nodes which have access to backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes which have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n NOTE: The storage classes created during the driver installation (via Helm) do not contain any topology keys and have the volumeBindingMode set to Immediate. A set of sample storage class definitions to enable topology -aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nIf you are using dell-csi-operator to create storage classes while installing the CSI PowerMax 1.5 driver, you can set the allowedTopologies value appropriately. volumeBindingMode is set to WaitForFirstConsumer if not specified explicitly.\n","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in …","ref":"/storage-plugin-docs/v1/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the powermax namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace powermax:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Configure Mount propagation on container runtime (that is, Docker) Linux multipathing requirements Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port groups names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For information about configuring iSCSI, see Dell EMC PowerMax documentation on Dell EMC Support.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nConfigure Mount Propagation on Container Runtime You must configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax. The following steps explain how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes. Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nLinux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  Volume Snapshot Requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml file, point to the correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax priviledges.\n Create the secret by running kubectl create -f helm/secret.yaml If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file `cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     unisphere Specifies the URL of the Unisphere for PowerMax server. If using the CSI PowerMax Reverse Proxy, leave this value unchanged at https://127.0.0.1:8443. Yes “https://127.0.0.1:8443”   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   arrayWhitelist List of comma-separated array IDs. If this parameter remains empty, the driver manages all the arrays that are managed by the Unisphere instance that is configured for the driver. Specify the IDs of the arrays that you want to manage, using the driver. No Empty   symmetrixID Specify a Dell EMC PowerMax array that the driver manages. This value is used to create a default storage class. Yes “000000000000”   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template which will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, then leave this value empty. No Empty   csireverseproxy This section refers to configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation No 2222   primary Mandatory section for Reverse Proxy - -   unisphere This must specify the URL of the Unisphere for PowerMax server Yes, if using Reverse Proxy “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   backup Optional section for Reverse Proxy. Specify Unisphere server address which the Reverse Proxy can fall back to if the primary Unisphere is unreachable or unresponsive.\nNOTE: If you do not want to specify a backup Unisphere server, then remove the backup section from the file - -   unisphere Specify the IP address of the Unisphere for PowerMax server which manages the arrays being used by the CSI driver No “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server No Empty    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes Starting in CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerMax v1.5 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the …","ref":"/storage-plugin-docs/v1/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire the lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_POWERMAX_ARRAYS List of comma-separated array id(s) which will be managed by the driver No -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: “a-b-c-%foo%-xyz” where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if the node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false   StorageClass parameters      SYMID Symmetrix ID Yes 000000000001   SRP Storage Resource Pool Name Yes DEFAULT_SRP   ServiceLevel Service Level No Bronze   FsType File System type (xfs/ext4) xfs    allowVolumeExpansion After the allowed topology is modified in storage class, Pods/and volumes will always be scheduled on nodes that have access to the storage No false   allowedTopologies:key This is to enable topology to allow Pods/and volumes to always be scheduled on nodes that have access to the storage. You need to specify the PowerMax array ID and append .fc or .iscsi at the end of it to specify a protocol. For more details on this feature see the related documentation No “000000000001”     Execute the following command to create PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component which can be installed along with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret which holds a SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs\nconfig : This section contains the details of the Reverse Proxy configuration\nmode : This value is set to Linked by default. Do not change this value\nlinkConfig : This section contains the configuration of the Linked mode\nprimary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable\nurl : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.0.0.000R# \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:# Mode for the proxy - only supported mode for now is \"Linked\"mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/v1/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. These examples automate the creation of Pods using the default storage classes that were created during installation. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with different number of volumes. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n test\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the ./stoptest.sh -t 2vols -n test script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are using the default storageclass names (powermax and powermax-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates/ directory). You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot on that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass which is created by the Helm-based installer. If you have an operator-based deployment, the name of the snapshot class will differ. You must update the snapshot class name in the file betaSnap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/v1/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.6.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.18.5+   Delete Volume fails with error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Driver installation warning: “OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6” Ignore this warning and continue with the installation. v1.6.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI PowerMax v1.6.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/v1/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/storage-plugin-docs/v1/troubleshooting/powermax/","title":"PowerMax"},{"body":"Volume Snapshot Feature The CSI PowerMax driver supports beta snapshots. Driver versions prior to version 1.4 supported alpha snapshots.\nThe Volume Snapshots feature in Kubernetes has moved to beta in Kubernetes version 1.17. It was an alpha feature in earlier releases (1.13 onwards). The snapshot API version has changed from v1alpha1 to v1beta1 with this migration.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class Starting CSI PowerMax 1.4 driver, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class you will need and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation (using the default driver name):\napiVersion: snapshot.storage.k8s.io/v1beta1 deletionPolicy: Delete kind: VolumeSnapshotClass metadata: name: powermax-snapclass driver: csi-powermax.dellemc.com Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pmax-snapshot-demo namespace: test spec: volumeSnapshotClassName: powermax-snapclass source: persistentVolumeClaimName: pmax-pvc-demo Once the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-restore-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-snapshot-demo kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Creating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-clone-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-pvc-demo kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP With version 1.3.0, support has been added for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (Experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax , then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions which should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting with v1.4, the CSI PowerMax driver supports expansion of Persistent Volumes (PVs). This expansion is done online, that is, when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThis is a sample manifest for a storage class which allows for Volume Expansion.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-expand-sc annotations: storageclass.beta.kubernetes.io/is-default-class: false provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true #Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: SYMID: \"000000000001\" SRP: \"DEFAULT_SRP\" ServiceLevel: \"Bronze\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5Gi, then you can resize it to 10 Gi by updating the PVC.\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc NOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of volume, it cannot be used to shrink a volume.\nRaw block support Starting v1.4, CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powermax resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere forPowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server which acts as a reverse proxy for the Unisphere forPowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and performance of the CSI PowerMax driver.\nOptionally you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation CSI PowerMax Reverse Proxy is installed as a Kubernetes deployment in the same namespace as the driver.\nIt is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is a HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be a X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1 , then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state\n If you’re using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment \u0026 driver node Daemonset. There are two new sections in the values file - controller \u0026 node - where you can specify these values separately for the controller and node pods.\ncontroller If you want to apply nodeSelectors \u0026 tolerations for the controller pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller pods to worker nodes only (Default):  controller: nodeSelector: tolerations:  Set the following values for controller pods to tolerate the taint NoSchedule on master nodes:  controller: nodeSelector: tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\"  Set the following values for controller pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller: nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node If you want to apply nodeSelectors \u0026 tolerations for the node pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add/remove tolerations to this list\n# \"node\" allows to configure node specific parameters node: # \"node.nodeSelector\" defines what nodes would be selected for pods of node daemonset # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"node.tolerations\" defines tolerations that would be applied to node daemonset # Add/Remove tolerations as per requirement # Leave as blank if you wish to not apply any tolerations tolerations: - key: \"node.kubernetes.io/memory-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/disk-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/network-unavailable\" operator: \"Exists\" effect: \"NoExecute\" Topology Support Starting from version 1.5, the CSI PowerMax driver supports Topology aware Volume Provisioning which helps Kubernetes scheduler place PVCs on worker nodes which have access to backend storage. When used in conjunction with nodeSelectors which can be specified for the driver node pods, it provides an effective way to provision applications on nodes which have access to the PowerMax array.\nAfter a successful installation of the driver, if a node pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer defined topology i.e. users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage In order to utilize the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For e.g. - A PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-fc parameters: SRP: \"SRP_1\" SYMID: \"000000000001\" ServiceLevel: \u003cService Level\u003e #Insert Service Level Name provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true allowedTopologies: - matchLabelExpressions: - key: csi-powermax.dellemc.com/000000000001 values: - csi-powermax.dellemc.com - key: csi-powermax.dellemc.com/000000000001.fc values: - csi-powermax.dellemc.com In the above example if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n NOTE: The storage classes created during the driver installation (via Helm) do not contain any topology keys and have the volumeBindingMode set to Immediate. A set of sample storage class definitions to enable topology aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nIf you are using dell-csi-operator to create storage classes while installing the CSI PowerMax 1.5 driver, you can set the allowedTopologies value appropriately. volumeBindingMode is set to WaitForFirstConsumer if not specified explicitly.\n","excerpt":"Volume Snapshot Feature The CSI PowerMax driver supports beta …","ref":"/storage-plugin-docs/v2/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the powermax namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace powermax:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Configure Mount propagation on container runtime (that is, Docker) Linux multipathing requirements Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port groups names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For information about configuring iSCSI, see Dell EMC PowerMax documentation on Dell EMC Support.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nConfigure Mount Propagation on Container Runtime You must configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax. The following steps explain how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes. Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Linux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  Volume Snapshot requirements Volume Snapshot CRDs The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nAlternately, you can install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script.\nVolume Snapshot Controller Starting with the beta Volume Snapshots, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on the GitHub repository for snapshot controller will install v3.0.2 of the snapshotter controller - (k8s.gcr.io/sig-storage/snapshot-controller:v3.0.2) Dell EMC recommends using the v3.0.2 image of the CSI external snapshotter - (k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2) The CSI external-snapshotter sidecar is still installed with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml, point to the correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax priviledges.\n Create the secret by running kubectl create -f helm/secret.yaml If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds a SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file `cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     unisphere Specifies the URL of the Unisphere for PowerMax server. If using the CSI PowerMax Reverse Proxy, leave this value unchanged at https://127.0.0.1:8443. Yes “https://127.0.0.1:8443”   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   arrayWhitelist List of comma-separated array IDs. If this parameter remains empty, the driver manages all the arrays that are managed by the Unisphere instance that is configured for the driver. Specify the IDs of the arrays that you want to manage, using the driver. No Empty   symmetrixID Specify a Dell EMC PowerMax array that the driver manages. This value is used to create a default storage class. Yes “000000000000”   storageResourcePool Must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template which will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, then leave this value empty. No Empty   csireverseproxy This section refers to configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation No 2222   primary Mandatory section for Reverse Proxy - -   unisphere This must specify the URL of the Unisphere for PowerMax server Yes, if using Reverse Proxy “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   backup Optional section for Reverse Proxy. Specify Unisphere server address which the Reverse Proxy can fall back to if the primary Unisphere is unreachable or unresponsive.\nNOTE: If you do not want to specify a backup Unisphere server, then remove the backup section from the file - -   unisphere Specify the IP address of the Unisphere for PowerMax server which manages the arrays being used by the CSI driver No “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server No Empty    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the …","ref":"/storage-plugin-docs/v2/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. Please refer detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller Pods you deploy. If controller Pods are greater than number of available nodes, excess Pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended onto all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_POWERMAX_ARRAYS List of comma-separated array id(s) which will be managed by the driver No -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature review related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: “a-b-c-%foo%-xyz” where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature review the related documentation No false   StorageClass parameters      SYMID Symmetrix ID Yes 000000000001   SRP Storage Resource Pool Name Yes DEFAULT_SRP   ServiceLevel Service Level No Bronze   FsType File System type (xfs/ext4) xfs    allowVolumeExpansion After the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No false   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to specify the PowerMax array ID and append .fc or .iscsi at the end of it to specify a protocol. For more details on this feature review the related documentation No “000000000001”     Execute the following command to create PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component which can be installed along with the CSI PowerMax driver. For more details on this feature review the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator is going to create a Deployment and ClusterIP service as part of the installation\nNote - If you wish to use the ReverseProxy with CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret which holds a SSL certificate and a private key which is required by the reverse proxy server. Use a tool like openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs\nconfig : This section contains the details of the Reverse Proxy configuration\nmode : This value is set to Linked by default. Do not change this value\nlinkConfig : This section contains the configuration of the Linked mode\nprimary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if Primary Unisphere is unreachable\nurl : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion: storage.dell.com/v1 kind: CSIPowerMaxRevProxy metadata: name: powermax-reverseproxy # \u003c- Name of the CSIPowerMaxRevProxy object namespace: test-powermax # \u003c- Set the namespace to where you will install the CSI PowerMax driver spec: # Image for CSI PowerMax ReverseProxy image: dellemc/csipowermax-reverseproxy:v1.0.0.000R # \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy: Always # TLS secret which contains SSL certificate and private key for the Reverse Proxy server tlsSecret: csirevproxy-tls-secret config: # Mode for the proxy - only supported mode for now is \"Linked\" mode: Linked linkConfig: primary: url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true # This setting determines if client side Unisphere certificate validation is to be skipped certSecret: \"\" # Provide this value if skipCertificateValidation is set to false backup: # This is an optional field and lets you configure a backup unisphere which can be used by proxy server url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true Installation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/v2/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. These examples automate the creation of Pods using the default storage classes that were created during installation. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with different number of volumes. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n test\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the ./stoptest.sh -t 2vols -n test script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are using the default storageclass names (powermax and powermax-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates/ directory). You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot on that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass which is created by the Helm-based installer. If you have an operator-based deployment, the name of the snapshot class will differ. You must update the snapshot class name in the file betaSnap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/v2/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.5.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Docker EE 3.1 Added support for Controller high availability (multiple-controllers) Added support for Topology Added support for mount options Changed driver base image to UBI 8.x  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+   Delete Volume fails with error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release    ","excerpt":"Release Notes - CSI PowerMax v1.5.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/v2/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/storage-plugin-docs/v2/troubleshooting/powermax/","title":"PowerMax"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\n Creation of secret.json or secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  PVC Creation Feature Following yaml content can be used to create a PVC without referring any PV.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:testvolumenamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonVolume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.6, no default Volume Snapshot Class will get created.\nFollowing are the manifests for the Volume Snapshot Class:\n VolumeSnapshotClass - v1  # For kubernetes version 20 and above (v1 snaps)apiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"VolumeSnapshotClass - beta  # For kubernetes version 18 and 19 (beta snaps)apiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"### Create Volume SnapshotThe following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:testvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:pvcsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.json\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-expansion-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.json for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which match all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.commountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nVolume Limit The CSI Driver for Dell EMC PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\n NOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\n Node selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world.)For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nDynamic log level change Log levels (debug, info, error, warning) were controlled only in my-isilon-settings.yaml which required restarting of csi-driver. Now the control has been transferred to secret definition (secret.json or secret.yaml). Changing the Log level in secret dynamically changes the log levels in controller and node logs.\n","excerpt":"Multicluster support You can connect a single CSI-PowerScale driver …","ref":"/storage-plugin-docs/docs/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerScale:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerScale.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\n(Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes 1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerscale.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\n  Collect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.json.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. true 1   isiPort “isiPort” defines the HTTPs port number of the PowerScale OneFS API server. false 8080   allowedNetworks “allowedNetworks” defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. false [ ]   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value will affect the default storage class implementation. false true   isiAccessZone The name of the access zone a volume can be created in. false System   volumeNamePrefix “volumeNamePrefix” defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount “controllerCount” defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization. false false   isiPath The default base path for the volumes to be created, will be used if a storage class does not have the IsiPath parameter specified. false /ifs/data/csi   autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used (that is, the mount command will not explicitly specify “-o vers=3” option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify ‘vers=3’ as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. false false   maxIsilonVolumesPerNode Specify the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. true 0   Controller parameters Set nodeSelector and tolerations for controller.     nodeSelector Define nodeSelector for the controllers, if required. false    tolerations Define tolerations for the controllers, if required. false    Node parameters Set nodeSelector and tolerations for node pods.     nodeSelector Define nodeSelector for the node pods, if required. false    tolerations Define tolerations for the node pods, if required. false     NOTE:\n User should provide all boolean values with double-quotes. This applies only for my-isilon-settings.yaml. Example: “true”/“false” ControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver.    Create a secret file for the OneFS credentials by editing the secret.json or secret.yaml file present under helm directory. Either secret.json or secret.yaml can be used for adding the credentials of one or more OneFS storage arrays. The following table lists driver configuration parameters for a single storage array.\n   Parameter Description Required Default     isiIP “isiIP” defines the HTTPs endpoint of the PowerScale OneFS API server. true -   endpoint This is a new way of defining existing isiIP. User can use either isiIP or endpoint but not both. true -   clusterName PoweScale cluster against which volume CRUD operations are performed through this secret. This is a logical name. true -   username Username for accessing PowerScale OneFS system. true -   password Password for accessing PowerScale OneFS system. true -   isDefaultCluster Defines whether this storage array should be the default. This entry should be present only for one OneFS array and that array will be marked default for existing volumes. false false   isDefault This is a new way of defining the existing isDefaultCluster key. User can use either isDefaultCluster or isDefault key but not both. false false   Optional parameters Following parameters are Optional if provided will override default values of values.yaml .     isiPort isiPort defines the HTTPs port number of the PowerScale OneFS API server. false 8080   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and hostname should be verified. false false   skipCertificateValidation This is a new way of defining the existing isiInsecure key. User can use either skipCertificateValidation or isiInsecure key but not both. false false   isiPath The base path for the volumes to be created. Note: isiPath value provided in the storage class will take the highest precedence while creating PVC. true -   LogLevel Log level of Drivers false “debug”    The username specified in secret.json / secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\n   Privilege Type     ISI_PRIV_LOGIN_PAPI Read Only   ISI_PRIV_NFS Read Write   ISI_PRIV_QUOTA Read Write   ISI_PRIV_SNAPSHOT Read Write   ISI_PRIV_IFS_RESTORE Read Only   ISI_PRIV_NS_IFS_ACCESS Read Only      If user creates secret.json, then after editing the file, run the following command to create a secret called ‘isilon-creds’  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json\nAlternatively, if user creates secret.yaml, then the secret can be created by running the following command: kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nNOTE:\n If any key/value is present in secret file and my-isilon-settings.yaml, then the values provided secret takes precedence. If any key/value is present in all my-isilon-settings.yaml, secret and storageClass, then the values provided in storageClass parameters takes precedence. User has to validate the JSON/ yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the JSON / yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also user can prefix ‘https’ (For example, https://192.168.1.1) with the value.   Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell EMC PowerScale.\nkubectl create -f emptysecret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\n  Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘isiInsecure’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘isiInsecure’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘isiInsecure’ or ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘isiInsecure’ or ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.json CSI Driver for Dell EMC PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.json or secret.yaml. User can now update the isilon-creds secret by editing the secret.json or secret.yaml and executing the following command (replace secret.json with secret.yaml based on need)\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - Note: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell EMC PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v1.5 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.5 before upgrading to 1.6.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the helm/samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerScale v1.5 driver The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver : It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.5 before upgrading to 1.6.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace.\nExecute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\n  Create isilon-creds secret by first creating secret.json or secret.yaml file.\n2.1 Create a json file called secret.json with the following content:\n{ \"isilonClusters\": [ { \"clusterName\": \"cluster1\", \"username\": \"user\", \"password\": \"password\", \"isiIP\": \"1.2.3.4\", \"isDefaultCluster\": true }, { \"clusterName\": \"cluster2\", \"username\": \"user\", \"password\": \"password\", \"endpoint\": \"1.2.3.5\", \"isiPort\": \"8080\", \"skipCertificateValidation\": true, \"isDefault\": false, \"isiPath\": \"/ifs/data/csi\" } ] } Replace the values for the given keys as per your environment. This username/password value need not be encoded. You can refer here for more information about these isilon secret parameters. After creating the secret.json, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json\n2.2. Alternately user can create a secret.yaml file in the following format and replace the values for the given keys as per your environment. The command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml would be needed to create secret.\nisilonClusters: - clusterName: \"cluster1\" # logical name of PowerScale Cluster username: \"user\" # username for connecting to PowerScale OneFS API server password: \"password\" # password for connecting to PowerScale OneFS API server endpoint: \"1.2.3.4\" # HTTPS endpoint of the PowerScale OneFS API server isDefault: true # default cluster skipCertificateValidation: true # indicates if client side validation of server's SSL certificate can be skipped isiPath: \"/ifs/data/csi\" # base path for the volume(directory) to be created on PowerScale - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" isiPort: \"8080\" logLevel: \"debug\" # CSI log level; valid log levels- \"error\", \"warn\"/\"warning\", \"info\", \"debug\" After creating the above file, the user can use the following command to create a secret object,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Note :\n From CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. Node selector and node tolerations can be added in both controller parameters and node parameters section, based on the need.  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/docs/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at helm/samples/storageclass\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml\nResult: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\n  Note: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\n Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under /helm/samples/volumesnapshotclass. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f $PWD/volsnapclass_v1.yaml OR kubectl create -f $PWD/volsnapclass_beta.yaml\nThe above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/.\nExecute the following command to create snapshot:\nkubectl create -f $PWD/snap.yaml\nThe spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for the newly created snapshot.\n  Note:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class.   Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located under test/sample_files/\nExecute the following command to create a snapshot:\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from the snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot pvcsnap   Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create a snapshot:\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for newly created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/docs/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.6.0 New Features/Changes  Added support for Kubernetes 1.21. Added support for Red Hat Enterprise Linux (RHEL) 8.4. Added support for CSI Spec 1.3. Added support for Volume Limit. Added support for node selector functionality to helm template. Added support for secret in YAML format. Added support for Dynamic log level changes. Added support to make dnsPolicy of node component configurable via Helm  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.6.0 New Features/Changes …","ref":"/storage-plugin-docs/docs/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.   Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell EMC PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.   Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to 1.4.0 there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file   CSI Driver installation fails with the error message “error getting FQDN”. Map IP address of host with its FQDN in /etc/hosts file.   Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. 1.This parameter is configurable in the helm installer and the user can try with different “dnsPolicy” according to the environment. (values.yaml). 2. In the case of Operator installation, this parameter is not configurable at present and will be available in upcoming release. To overcome this issue, try to use appropriate “dnsPolicy” ( ClusterFirst / ClusterFirstWithHostNet ) by patching Isilon node pods. Example : kubectl patch daemonset isilon-node -n isilon -p ‘{“spec”: {“template”: {“spec”:{“dnsPolicy”: “ClusterFirst”}}}}’    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/storage-plugin-docs/docs/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Multicluster support You can connect single CSI-PowerScale driver with multiple PowerScale clusters. Pre-Requisistes:\n Creation of secret.json with credentials related to one or more Clusters. Creation of (at least) one Custom Storage classes for each non-default clusters. Creation of custom-volumesnapshot classes, if corresponding isiPaths differ in custom storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, cluster name is assumed as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle shoulb be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.3 and later, a Volume Snapshot Class is created using the new recommended snapshot APIs (depends upon Kubernetes version). This is the Volume Snapshot Class created for the default isiPath provided in my-isilon-settings.yaml (which is created based on values.yaml). For additional custom storage classes, separate custom volume snapshot class should be created (only if the isiPath is different from default storage class).\nFollowing are the manifests for the Volume Snapshot Class created during installation:\n VolumeSnapshotClass - v1  # For kubernetes version 20 (v1 snaps)# This is a sample manifest for creating snapshotclass with IsiPath other than default# pvc is created with sc which has some different IsiPath e.g. /ifs/custom# to create a snapshot for this pvc volumesnapshotclass must also be initilized with same IsiPath (i.e. /ifs/custom ) to work snapshot featureapiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass-custom\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/custom\"VolumeSnapshotClass - beta  # For kubernetes version 18 and 19 (beta snaps)# This is a sample manifest for creating snapshotclass with IsiPath other than default# pvc is created with sc which has some different IsiPath e.g. /ifs/custom# to create a snapshot for this pvc volumesnapshotclass must also be initilized with same IsiPath (i.e. /ifs/custom ) to work snapshot featureapiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass-custom\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/custom\"### Create Volume SnapshotThe following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:autotestvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:newsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.json\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in given cluster and attach newly created ephemeral inline csi volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labelled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP. Note: Only a single cluster can be configured as part of secret.json for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that Pod scheduling takes advantage of the topology and the selected node has access to provisioned volumes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which matches all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.commountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport for Docker EE The CSI Driver for Dell EMC PowerScale supports Docker EE and deployment on clusters bootstrapped with UCP (Universal Control Plane) 3.3.5. *UCP version 3.3.5 supports kubernetes 1.20 and CSI driver can be installed on UCP 3.3.5 with Helm.\nThe installation process for the driver on such clusters remains the same as the installation process on upstream clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes in UCP backed clusters may run any of the OSs which we support with upstream clusters.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backwards compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods. Also, previous workload will still be using default network and not custom networks, for previous workloads to use custom networks recreation of pods required.\n","excerpt":"Multicluster support You can connect single CSI-PowerScale driver with …","ref":"/storage-plugin-docs/v1/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts in upstream Kubernetes. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a Daemon Set:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for PowerScale, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Configure Docker service Install Helm v3 Install volume snapshot components Deploy PowerScale driver using Helm  NOTE: There is no feature gate that needs to be set explicitly for CSI drivers version 1.17 and later. All the required feature gates are either beta/GA.\nConfigure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.\nProcedure  Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows: [Service] ... MountFlags=shared  Restart the Docker service with the following commands: systemctl daemon-reload systemctl restart docker   NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nInstall volume snapshot components Install Snapshot CRDs  For Kubernetes 1.18 and 1.19, SnapShot CRDs versioned 3.0.3 (https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/client/config/crd), must be installed. For Kubernetes 1.20, SnapShot CRDs versioned 4.0.0 (https://github.com/kubernetes-csi/external-snapshotter/tree/v4.0.0/client/config/crd) must be installed.  Install Snapshot Controller  For Kubernetes 1.18 and 1.19, Snapshot controller versioned 3.0.3 (https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/deploy/kubernetes/snapshot-controller) must be installed. For Kubernetes 1.20, Snapshot controller versioned 4.0.0 (https://github.com/kubernetes-csi/external-snapshotter/tree/v4.0.0/deploy/kubernetes/snapshot-controller) must be installed.  Install CSI Driver for PowerScale Before you begin\n You must clone the source code from git repository. In the dell-csi-helm-installer directory, there will be two shell scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart.  Steps\n  Collect information from the PowerScale Systems like IP address,IsiPath, username and password. Make a note of the value for these parameters as they must be entered in the secret.json.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1 true 1   isiPort “isiPort” defines the HTTPs port number of the PowerScale OneFS API server false 8080   allowedNetworks “allowedNetworks” defines list of networks which can be used for NFS I/O traffic, CIDR format must be used false -   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and host name must be verified. This value will affect the default storage class implementation false true   isiAccessZone The name of the access zone a volume can be created in false System   volumeNamePrefix “volumeNamePrefix” defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount “controllerCount” defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableDebug Indicates whether debug level logs should be logged false true   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs false 1   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization false false   isiPath The default base path for the volumes to be created, this will be used if a storage class does not have the IsiPath parameter specified false /ifs/data/csi   autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used (that is, the mount command will not explicitly specify “-o vers=3” option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify ‘vers=3’ as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish connection to Array. This requires enableCustomTopology to be enabled. false false   Controller parameters Set nodeSelector and tolerations for controller     nodeSelector Define nodeSelector for the controllers, if required false    tolerations Define tolerations for the controllers, if required false     NOTES\n User should provide all boolean values with double quotes. This applicable only for my-isilon-settings.yaml. Example: “true”/“false” ControllerCount parameter value should not exceed number of nodes in the Kubernetes cluster. Otherwise some of the controller pods will be in “Pending” state till new nodes are available for scheduling. The installer will exit with a WARNING on the same. Whenever certSecretCount parameter changes in myvalues.yaml user needs to reinstall the driver.    Create namespace Run kubectl create namespace isilon to create the isilon namespace. Specify the same namespace name while installing the driver.\nNOTE: CSI PowerScale also supports installation of driver in custom namespace.\n  Create a secret file for the OneFS credentials by editing the secret.json present under helm directory. This secret.json can be used for adding the credentials of one or more OneFS storage arrays.The following table lists driver configuration parameters for a single storage array.\n   Parameter Description Required Default     isiIP “isiIP” defines the HTTPs endpoint of the PowerScale OneFS API server true -   clusterName PoweScale cluster against which volume CRUD operations are performed through this secret. This is a logical name. true -   username Username for accessing PowerScale OneFS system true -   password Password for accessing PowerScale OneFS system true -   isDefaultCluster defines whether this storage array should be the default.This entry should be present only for one OneFS array and that array will be marked default for existing volumes. true false   Optional parameters Following parameters are Optional, if provided , will override default values of values.yaml     isiPort isiPort defines the HTTPs port number of the PowerScale OneFS API server false -   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and host name should be verified. false false   isiPath The base path for the volumes to be created. Note: isiPath value provided in the storage class will take the highest precedence while creating PVC true -    The username specified in secret.json must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nISI_PRIV_LOGIN_PAPI ISI_PRIV_NFS ISI_PRIV_QUOTA ISI_PRIV_SNAPSHOT ISI_PRIV_IFS_RESTORE ISI_PRIV_NS_IFS_ACCESS After editing the file, run the following command to create a secret called isilon-creds  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json\nNOTES:\n If any key/value is present in both secret.json and my-isilon-settings.yaml, then the values provided secret.json will take precedence. If any key/value is present in both my-isilon-settings.yaml/secret.json and storageClass, then the values provided in storageClass parameters will take precedence. User has to validate the JSON syntax and array related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.    Install OneFS CA certificates by following the instructions from next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and empty secret should be created for the successful CSI Driver for Dell EMC Powerscale installation.\nkubectl create -f emptysecret.yaml   Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml\n  In the case of OpenShift, the driver installation will fail because of a lack of privileges over clusterRole. To resolve this issue the command oc adm policy add-scc-to-user privileged -z isilon-node -n isilon and re-install the driver. This solution will be added in next release.\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘isiInsecure’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘isiInsecure’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘isiInsecure’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘isiInsecure’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without port , depends upon the configuration of OneFS API server. Above said commands is based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as number of OneFS arrays grows. The cert secret created out of these pem files should have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1 etc.); The number should start from zero and should grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.json CSI Driver for Dell EMC PowerScale now provides supports for Multi cluster. Now user can link the single CSI Driver to multiple OneFS Clusters by updating secret.json. User can now update the isilon-creds secret by editing the secret.json and executing following command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - Storage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, please refer: https://kubernetes.io/docs/concepts/storage/storage-classes/\nStarting from v1.5 of the driver, Storage Classes would no longer be created along with the installation of the driver. A wide set of annotated storage class manifests have been provided in the helm/samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nStarting in CSI PowerScale v1.5, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v1.4 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so. Since in CSI-PowerScale 1.5 Multi array is supported. The existing storage class (of 1.4) should be treated as default storage class.\nUpgrading from an older version of the driver It is strongly recommended to upgrade older versions of CSI-PowerScale to CSI-PowerScale 1.4 before upgrading to 1.5.\nSteps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/storage-plugin-docs/v1/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nListing installed drivers with the CSI Isilon CRD User can query for csi-powerscale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace Run kubectl create namespace isilon to create the isilon namespace. Note that the namespace can be any user defined name , in this example, we assume that the namespace is ‘isilon’.\n  Create isilon-creds Create a json file called isilon-creds.json with the following content:\n{ \"isilonClusters\": [ { \"clusterName\": \"cluster1\", \"username\": \"user\", \"password\": \"password\", \"isiIP\": \"1.2.3.4\", \"isDefaultCluster\": true }, { \"clusterName\": \"cluster2\", \"username\": \"user\", \"password\": \"password\", \"isiIP\": \"1.2.3.5\", \"isiPort\": \"8080\", \"isiInsecure\": true, \"isiPath\": \"/ifs/data/csi\" } ] } Replace the values for the given keys as per your environment. This username / password value need not be encoded. You can refer here for more information about isilon secret parameters.\n  Create isilon-certs- secret Please refer this section for creating cert-secrets. Run kubectl create -f isilon-creds.yaml command to create the secret.\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   X_CSI_ISI_ENDPOINT HTTPs endpoint of the PowerScale OneFS API server Yes    X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in input yaml file.\n  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/v1/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at helm/samples/storageclass\nExecute the following command to create storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc. Note: Verify system for the new storage class.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing kubectl get pvc. Note: Verify system for the new volume. Note that the status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on storage class.\n  Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/snap.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for newly created snapshot.\nNote:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. The CSI Driver for PowerScale installation creates this class as its default snapshot class. You can see its definition using kubectl get volumesnapshotclasses isilon-snapclass -o yaml. The value of IsiPath in default VolumeSnapshotClass is taken from values.yaml. If user wants different path, she has to create custom volumesnapshot class with required IsiPath in parameters section. Sample VolumeSnapshotClass file is present under helm/samples/volumesnapshotclass    Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in spec dataSource field.\nThe sample file for volume creation from snapshot is located under test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot pvcsnap   Create new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for new created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to unattach the volume from host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/v1/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.5.0 New Features/Changes  Added support for Kubernetes 1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.x Added multi-cluster support through single instance of driver installation Added support for custom networks for NFS I/O traffic SSH permissions are no longer required. You can safely revoke the privilege ISI_PRIV_LOGIN_SSH for the CSI driver user.  Fixed Issues There are no Fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class are not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to 1.4.0 there is a possibility of “localhost” as stale entry in export We recommend you not to map hostname to loopback IP in /etc/hosts file   If the length of the nodeID exceeds 128 characters, driver fails to update CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581   Driver installation warning: “OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6” Ignore this warning and continue with the installation. v1.5.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.5.0 New Features/Changes …","ref":"/storage-plugin-docs/v1/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/storage-plugin-docs/v1/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Consuming existing volumes with static provisioning You can use existent volumes from PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=SystemclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.3 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:isilon-snapclassdriver:csi-isilon.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:autotestvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:newsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:AccessZone:SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"This manifest will create a pod and attach newly created ephemeral inline csi volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labelled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that Pod scheduling takes advantage of the topology and the selected node has access to provisioned volumes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which matches all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.commountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\n","excerpt":"Consuming existing volumes with static provisioning You can use …","ref":"/storage-plugin-docs/v2/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a Daemon Set:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for PowerScale, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes. Configure Docker service Install Helm v3 Install volume snapshot components Deploy PowerScale driver using Helm  Note: There is no feature gate that needs to be set explicitly for CSI drivers version 1.17 and later. All the required feature gates are either beta/GA.\nConfigure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.\nProcedure  Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows: [Service] ... MountFlags=shared  Restart the Docker service with systemctl daemon-reload and systemctl daemon-reload systemctl restart docker   Install volume snapshot components Install Snapshot Beta CRDs To install snapshot CRDs specify --snapshot-crd flag to driver installation script dell-csi-helm-installer/csi-install.sh during driver installation.\nInstall Common Snapshot Controller, if not already installed for the cluster.\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2  Install CSI Driver for PowerScale Before you begin\n You must clone the source code from git repository. In the dell-csi-helm-installer directory, there should be two shell scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart.  Steps\n  Collect information from the PowerScale Systems like IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and values file.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     isiIP “isiIP” defines the HTTPs endpoint of the PowerScale OneFS API server true -   isiPort “isiPort” defines the HTTPs port number of the PowerScale OneFS API server false 8080   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and host name should be verified. false true   isiAccessZone The name of the access zone a volume can be created in false System   volumeNamePrefix “volumeNamePrefix” defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount “controllerCount” defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableDebug Indicates whether debug level logs should be logged false true   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs false 1   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization false false   isiPath The default base path for the volumes to be created, this will be used if a storage class does not have the IsiPath parameter specified false /ifs/data/csi   autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used (that is, the mount command will not explicitly specify “-o vers=3” option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify ‘vers=3’ as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish connection to Array. This requires enableCustomTopology to be enabled. false false   Storage Class parameters Following parameters are related to Storage Class     name “storageClass.name” defines the name of the storage class to be defined. false isilon   isDefault “storageClass.isDefault” defines whether the primary storage class should be the default. false true   reclaimPolicy “storageClass.reclaimPolicy” defines what will happen when a volume is removed from the Kubernetes API. Valid values are “Retain” and “Delete”. false Delete   accessZone The Access Zone where the Volume would be created false System   AzServiceIP Access Zone service IP if different from isiIP, specify here and refer in storageClass false    rootClientEnabled When a PVC is being created, it takes the storage class’ value of “storageclass.rootClientEnabled” false false   Controller parameters Set nodeSelector and tolerations for controller     nodeSelector Define nodeSelector for the controllers, if required false    tolerations Define tolerations for the controllers, if required false     Note: User should provide all boolean values with double quotes. This is applicable only for my-isilon-settings.yaml. Example: “true”/“false”\nNote: controllerCount parameter value should not exceed number of nodes in the kubernetes cluster. Otherwise some of the controller pods will be in “Pending” state till new nodes are available for scheduling. The installer will exit with a WARNING on the same.\n  Create namespace Run kubectl create namespace isilon to create the isilon namespace. Specify the same namespace name while installing the driver.\nNote: CSI PowerScale also supports installation of driver in custom namespace.\n  Create a secret file for the OneFS credentials by editing the secret.yaml present under helm directory. Replace the values for the username and password parameters. Use the following command to convert username/password to base64 encoded string:\necho -n 'admin' | base64 echo -n 'password' | base64 Run kubectl create -f secret.yaml to create the secret.\nNote: The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nISI_PRIV_LOGIN_PAPI ISI_PRIV_NFS ISI_PRIV_QUOTA ISI_PRIV_SNAPSHOT ISI_PRIV_IFS_RESTORE ISI_PRIV_NS_IFS_ACCESS ISI_PRIV_LOGIN_SSH   Install OneFS CA certificates by following the instructions from next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and empty secret should be created for the successful CSI Driver for Dell EMC Powerscale installation.\nkubectl create -f emptysecret.yaml   Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/myvalues.yaml\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘isiInsecure’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘isiInsecure’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘isiInsecure’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘isiInsecure’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert.pem To create the secret, run kubectl create secret generic isilon-certs --from-file=ca_cert.pem -n isilon   Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/storage-plugin-docs/v2/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nListing installed drivers with the CSI Isilon CRD User can query for csi-powerscale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver  Create namespace Run kubectl create namespace isilon to create the isilon namespace. Create isilon-creds Create a file called isilon-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:isilon-credsnamespace:isilontype:Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f isilon-creds.yaml command to create the secret.\n Create a CR (Custom Resource) for PowerScale using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:    Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   X_CSI_ISI_ENDPOINT HTTPs endpoint of the PowerScale OneFS API server Yes    X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node     Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver.  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/v2/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing kubectl get pvc. Note: Verify system for the new volume\n  Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to Kubernetes node\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/snap.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for newly created snapshot.\nNote:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. The CSI Driver for PowerScale installation creates this class as its default snapshot class. You can see its definition using kubectl get volumesnapshotclasses isilon-snapclass -o yaml.    Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in spec dataSource field.\nThe sample file for volume creation from snapshot is located under test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot\nkubectl get volumesnapshot kubectl delete volumesnapshot testvolclaim1-snap1   Create new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for new created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to unattach the volume from host\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/storage-plugin-docs/v2/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.4.0 New Features/Changes  Added support for OpenShift 4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Controller high availability (multiple-controllers) Added Topology support Added support for CSI Ephemeral Inline Volumes Added support for mount options Enhancements to volume creation from data source Enhanced support for Docker EE 3.1  Fixed Issues    Problem summary Found in version Resolved in version     POD creation fails in OpenShift and Kubernetes environments, if hostname is not an FQDN v1.3.0 v1.4.0   When creating volume from a snapshot or volume from volume, the owner of the new files or folders that are copied from the source snapshot is the Isilon user who is specified in secret.yaml. So the original owner of a file or folder might not be the owner of the newly created file or folder.  v1.4.0    Known Issues    Issue Resolution or workaround, if known     Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class are not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\n* ISI_PRIV_LOGIN_SSH\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to 1.4.0 there is a possibility of “localhost” as stale entry in export We recommend you not to map hostname to loopback IP in /etc/hosts file   If the length of the nodeID exceeds 128 characters, driver fails to update CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.4.0 New Features/Changes …","ref":"/storage-plugin-docs/v2/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in production environment.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/storage-plugin-docs/v2/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and was generally available (v1) in Kubernetes version \u003e= 1.20.\nThe CSI PowerStore driver version 1.4 supports v1beta1 snapshots on Kubernetes 1.19 and v1 snapshots on Kubernetes 1.20/1.21.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the helm/samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powerstore-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueSnapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, specify the following in values.yaml\nsnapshot:enable:trueExternal Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# global ID to identify arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)blockProtocol:\"ISCSI\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nasName:\"nas-server\"# what NAS must be used for NFS volumes- endpoint:\"https://10.0.0.2/api/rest\"globalID:\"unique\"username:\"user\"password:\"password\"skipCertificateValidation:trueblockProtocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\n NOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\n Root squashing CSI PowerStore driver version 1.4.0 allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion:storage.k8s.io/v1kind:StorageClass...parameters:...allowRoot:\"false\"# enables or disables root squashing The 1.4 version of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\n ","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/storage-plugin-docs/docs/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerStore.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.4 supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes 1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit helm/secret.yaml, correct namespace field to point to your desired namespace.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nasName: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running sed \"s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\" helm/secret.yaml | kubectl apply -f -\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csi”   nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell EMC PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerStore v1.2 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in helm/config.yaml FsType: specifies what filesystem type driver should use, possible variants ext4, xfs, nfs, if not specified driver will use ext4 by default. allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\" or \"-nfs\" at the end to use FC or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the helm/samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# unique id of the PowerStore arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# indicates if client side validation of (management)server's certificate can be skippedisDefault:true# treat current array as a default (would be used by storage classes without arrayID parameter)blockProtocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nasName:\"nas-server\"# what NAS should be used for NFS volumesChange the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false      Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.\n After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e    ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/docs/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from helm/samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/storage-plugin-docs/docs/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.4.0 New Features/Changes  Added support for Kubernetes v1.21 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for SLES 15.2 and RHEL 8.4 as a host operating system Added support for enabling root-squashing for NFS shares Added support for disabling snapshot feature during installation Added the ability to configure nasName per Storage Class Refactored configuration files to use more generic naming  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","excerpt":"Release Notes - CSI PowerStore v1.4.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/docs/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials   Installation of the driver on Kubernetes v1.20/1.21 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/1.21 requires v1 version of snapshot CRDs. If on Kubernetes 1.20/1.21 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/storage-plugin-docs/docs/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore Pod should be Ready and Running\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller\n Deleting volumes To delete volumes, pod and statefulset run\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 1.1 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerStore driver version 1.1 and later, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1# or v1beta1, depends on your k8s versionkind:VolumeSnapshotClassmetadata:name:powerstore-snapshotdriver:csi-powerstore.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 (or v1beta1) snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1# or v1beta1, depends on your k8s versionkind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:testpowerstorespec:volumeSnapshotClassName:powerstore-snapshotsource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver should override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"This manifest creates a pod and attach newly created ephemeral inline csi volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assinged in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where driver has a connection to PowerStore with IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work.\n For any additional information about topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as Host on storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 adds support for managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIinsecure:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)block-protocol:\"ISCSI\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nas-name:\"nas-server\"# what NAS should be used for NFS volumes- endpoint:\"https://10.0.0.2/api/rest\"username:\"user\"password:\"password\"insecure:trueblock-protocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayIP:\"10.0.0.1\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayIP:\"10.0.0.2\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 adds the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver should detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 adds the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter will be added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This would mean that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/storage-plugin-docs/v1/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  Linux native multipathing requirements Configure Mount propagation on container runtime (i.e. Docker) Volume Snapshot requirements The nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.3 supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI director on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  Configure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerStore. The following is instruction on how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nVolume Snapshot Requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use v3.0.3 version of snapshotter/snapshot-controller when using Kubernetes v1.18, v1.19 When using Kubernetes v1.20 it is recommended to use v4.0.0 version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit helm/secret.yaml, correct namespace field to point to your desired namespace.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. username, password: defines credentials for connecting to array. insecure: defines if we should use insecure connection or not. default: defines if we should treat the current array as a default. block-protocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nas-name: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayIP parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running sed \"s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\" helm/secret.yaml | kubectl apply -f -\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csi”   nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell EMC PowerStore version 1.3 and later, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerStore v1.2 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayIP: specifies what array driver should use to provision volumes, if not specified driver will use array specified as default in helm/config.yaml FsType: specifies what filesystem type driver should use, possible variants ext4, xfs, nfs, if not specified driver will use ext4 by default allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\" or \"-nfs\" at the end to use FC or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/storage-plugin-docs/v1/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIinsecure:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)block-protocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nas-name:\"nas-server\"# what NAS should be used for NFS volumesChange the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false   StorageClass parameters      FsType Specifies what filesystem type driver should use, possible variants ext4, xfs, nfs No “ext4”   arrayIP Specifies what array driver should use to provision volumes No “default”   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the “127.0.0.1-nfs” portion in the key with PowerStore endpoint IP with its value and append -nfs, -fc or -iscsi at the end of it No “127.0.0.1-nfs”      Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.\n After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e    ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/v1/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from helm/samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/storage-plugin-docs/v1/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.3.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Added support for managing multiple PowerStore arrays from one driver Added support for configuring custom IPs/sub-networks for NFS exports Added support for automatic generation of CHAP credentials Changed code structure of the project Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+   Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released you need to manually remove the node labels    ","excerpt":"Release Notes - CSI PowerStore v1.3.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/v1/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to PowerStore API. Check if you’ve created secret with correct credentials   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/storage-plugin-docs/v1/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command will create a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset will be created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore Pod should be Ready and Running\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller\n Deleting volumes To delete volumes, pod and statefulset run\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 1.1 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-3.0 kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerStore driver version 1.1 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:powerstore-snapshotdriver:csi-powerstore.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:testpowerstorespec:volumeSnapshotClassName:powerstore-snapshotsource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.2.0 adds support for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI protocol.\nTo enable CHAP authentication:\n Create secret powerstore-creds with the key chapsecret and chapuser set to base64 values. chapsecret must be between 12 and 60 symbols. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter connection.enableCHAP in my-powerstore-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating new host on powerstore array driver will populate host chap credentials with provided values. When re-using already existing hosts be sure to check that provided credentials in powerstore-creds match earlier preconfigured host credentials.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"This manifest will create a pod and attach newly created ephemeral inline csi volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assinged in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feaure user needs to create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example will match all nodes where driver has a connection to PowerStore with IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work.\n For any additional information about topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as Host on storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/storage-plugin-docs/v2/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  Linux native multipathing requirements Configure Mount propagation on container runtime (i.e. Docker) Volume Snapshot requirements The nonsecure registries are defined in Docker or other container runtime, for CSI drivers that are hosted in a nonsecure location. You can access your cluster with kubectl and helm.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you will use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.2 supports iSCSI connectivity.\nIf you will use the iSCSI protocol, set up the iSCSI initiators as follows:\n Make sure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Make sure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  Configure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerStore. The following is instruction on how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Volume Snapshot requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nAlternately, you can install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script. If you are installing the driver using the Dell CSI Operator, there is a helper script provided to install the snapshot CRDs - scripts/install_snap_crds.sh.\nVolume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 onwards, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell EMC recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository Ensure that you’ve created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image. Edit the helm/secret.yaml, point to correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername \u0026 mypassword are credentials that would be used for accessing PowerStore API. NOTE: If you want to use iSCSI CHAP you need fill chapsecret and chapuser fields in similar manner\n Create the secret by running kubectl create -f helm/secret.yaml Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:     Parameter Description Required Default     powerStoreApi Defines the full URL path to the PowerStore API Yes \" \"   volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csi”   nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   connection.scsiProtocol Defines which transport protocol to use (FC, ISCSI, None, or auto). - By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. - A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. - To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. - For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. Yes “auto”   connection.nfs.enable Enables or disables NFS support No FALSE   connection.nfs.nasServerName Points to the NAS server that would be used - If you have nfs.enabled set to true, it will try to use nfs.nasServerName. This will fail if you do not provide nfs.nasServerName. No “nas-server”   connection.nfs.version Defines version of NFS protocol No “v3”   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No FALSE   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/storage-plugin-docs/v2/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator doesn’t use any Helm charts and the installation \u0026 configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerStore credentials: Create a file called powerstore-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powerstore-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f powerstore-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerStore using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If controller pods is greater than number of available nodes, excess pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_POWERSTORE_ENDPOINT Must provide a PowerStore HTTPS API url Yes https://127.0.0.1/api/rest   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provide list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   StorageClass parameters      allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the “127.0.0.1-nfs” portion in the key with PowerStore endpoint IP with its value and append -nfs, -fc or -iscsi at the end of it No “127.0.0.1-nfs”     Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.  ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell …","ref":"/storage-plugin-docs/v2/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs and nfs storage classes, and automatically mounts them to the pod. Note that nfs storage class is optional and will not be created if you haven’t turned it on in myvalues.yaml.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/storage-plugin-docs/v2/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.2.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Docker EE 3.1 Added support for Controller high availability (multiple-controllers) Added support for Topology Added support for ephemeral volumes Added support for mount options Changed driver base image to UBI 8.x  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+    ","excerpt":"Release Notes - CSI PowerStore v1.2.0 New Features/Changes  Added …","ref":"/storage-plugin-docs/v2/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to PowerStore API. Check if you’ve created secret with correct credentials    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/storage-plugin-docs/v2/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/storage-plugin-docs/docs/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/storage-plugin-docs/v1/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/storage-plugin-docs/v2/grasp/video/","title":"Quick video lessons"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE v1.2.8 supported with Kubernetes client (kubectl) v1.21\nRKE Examples ","excerpt":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE).\nThe …","ref":"/storage-plugin-docs/docs/partners/rancher/","title":"RKE"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/docs/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v1/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/storage-plugin-docs/v2/installation/test/","title":"Testing Drivers"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unity command. The pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and generally available (v1) in Kubernetes version \u003e=1.20.\nThe CSI Unity driver version 1.5 supports v1beta1 snapshots on Kubernetes 1.19 and v1 snapshots on Kubernetes 1.20 and 1.21.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI Unity 1.5 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for a Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver version 1.4 and later supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"arrayId:APM************protocol:iSCSIthinProvisioned:\"true\"isDataReductionEnabled:\"false\"tieringPolicy:\"1\"storagePool:pool_2This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 and later supports the controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, number of replicas is set to 2, you can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change the replicas parameter in the spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to the Unity array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nSupport for SLES 15 SP2 The CSI Driver for Dell EMC Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\nVolume Limit The CSI Driver for Dell EMC Unity allows users to specify the maximum number of Unity volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in secret.json or secret.yaml file.\n NOTE: To reflect the changes after setting the value either via node label or in secret.json/secret.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and secret.json/secret.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the secret.json/secret.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then CO SHALL decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\n Log Levels The CSI Driver for Dell EMC Unity allows users to configure different log levels using logLevel parameter.\nThe logLevel parameter needs to be configured in unity-creds secret created from secret.json or secret.yaml. The supported log levels are Info/Debug/Warn/Error. The default level is Info if logLevel is not configured by the user. The parameter can be changed dynamically without the need of driver re-installation or restart.\n","excerpt":"Creating volumes and consuming them Create a file sample.yaml using …","ref":"/storage-plugin-docs/docs/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used  Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with the command git clone https://github.com/dell/csi-unity.git, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.json or secret.yaml and myvalues.yaml file.\nNote:\n ArrayId corresponds to the serial number of Unity array. Unity Array username must have role as Storage Administrator to be able to perform CRUD operations.    Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1   controllerCount Controller replication count to maintain high availability. yes 2   volumeNamePrefix String to prepend to any volumes created by the driver. false csivol   snapNamePrefix String to prepend to any snapshot created by the driver. false csi-snap   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15    Note:\n  User should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  User can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n Parameters allowRWOMultiPodAccess and syncNodeInfoInterval have been deprecated from myvalues.yaml and are removed from myvalues.yaml in a future releases. They can be configured in secret.json or secret.yaml as they facilitate the user to dynamically change these values without the need for driver re-installation.    Example myvalues.yaml\ncsiDebug:\"true\"volumeNamePrefix :csivolsnapNamePrefix:csi-snapimagePullPolicy:AlwayscertSecretCount:1controllerCount:2allowRWOMultiPodAccess:falsesyncNodeInfoInterval:5  For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file helm/emptysecret.yaml file by running the kubectl create -f helm/emptysecret.yaml command.\n  Prepare the secret.json or secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     storageArrayList.username Username for accessing Unity system true -   storageArrayList.password Password for accessing Unity system true -   storageArrayList.endpoint REST API gateway HTTPS endpoint Unity system true -   storageArrayList.arrayId ArrayID for Unity system true -   storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true   storageArrayList.isDefault An array having isDefault=true or isDefaultArray=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. false false   logLevel This parameter is used to set the logging level in the driver. Log level can be Info/Debug/Warn/Error. false Info   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute. false 15   maxUnityVolumesPerNode The maximum number of volumes that can be provisioned to a single node beyond which the driver would return an error on a provisioning request. Default value 0 stands for unlimited volumes. false 0    Example: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"endpoint\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"skipCertificateValidation\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"endpoint\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"skipCertificateValidation\": true } ], \"logLevel\": \"Info\", \"allowRWOMultiPodAccess\": false, \"syncNodeInfoTimeInterval\": 15, \"maxUnityVolumesPerNode\": 0 } Use the following command to create a new secret unity-creds from secret.json file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of both secret.json and secret.yaml are available in the directory csi-unity/helm/samples.\nExample: secret.yaml\nlogLevel:\"Info\"syncNodeInfoInterval:15allowRWOMultiPodAccess:\"false\"maxUnityVolumesPerNode:0storageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:true``\nUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml --dry-run | kubectl replace -f -\nNote:\n  “restGateway” parameter has been changed to “endpoint” as restgateway is deprecated and will be removed from use in secret.json or secret.yaml in a future release. Users can continue to use any one of “restGateway” or “endpoint” for now. The driver would return an error if both parameters are used.\n  “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in secret.json or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used.\n  “isDefaultArray” parameter has been changed to “isDefault” as isDefaultArray is deprecated and will be removed from use in secret.json or secret.yaml in a future release. Users can continue to use any one of “isDefaultArray” or “isDefault” for now. The driver would return error if both parameters are used.\n  Parameters “allowRWOMultiPodAccess” and “syncNodeInfoTimeInterval” have been enabled for configuration in secret.json or secret.yaml and this helps users to dynamically change these values without the need for driver re-installation. If these parameters are specified in myvalues.yaml as well as here then the values from secret.json / secret.yaml takes precedence and they will reflect in the driver after unity-creds secret is updated.\n    Setup for snapshots.\nThe Kubernetes Volume Snapshot feature is beta in Kubernetes 1.19 and moved to GA in \u003e=v1.20.\n The following section summarizes the changes in the GA Applicable only if you decided to enable snapshot feature in values.yaml  snapshot:enabled:trueIn order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster\nVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  Note:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller Note:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes 1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.20 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.20 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.  The script also creates one or more volumesnapshotclasses based on the number of arrays . “unity-snapclass” will be the volumesnapshotclass for the default array. The output will be similar to the following:\n[root@host ~]# kubectl get volumesnapshotclass NAME AGE unity-apm***********-snapclass 12m unity-snapclass 12m\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n  To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -\n  Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n  “unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\n  User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\n  Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n    Storage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the helm/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the csi-unity/helm/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from CSI Unity v1.5 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver It is strongly recommended to upgrade the earlier versions of CSI Unity to 1.5 before upgrading to 1.6.\nSteps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Pick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml  Note:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n kubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/storage-plugin-docs/docs/installation/helm/unity/","title":"Unity"},{"body":"CSI Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing Unity system true -   password Password for accessing Unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for Unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Ex: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot    Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v5replicas:2common:image:\"dellemc/csi-unity:v1.6.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\"true\"- name:X_CSI_UNITY_ALLOW_MULTI_POD_ACCESSvalue:\"false\"- name:X_CSI_MAX_VOLUMES_PER_NODEvalue:\"0\"sideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"]- name:snapshotterargs:[\"--snapshot-name-prefix=csiunitysnap\"]","excerpt":"CSI Unity Pre-requisites Create secret to store Unity credentials …","ref":"/storage-plugin-docs/docs/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/storage-plugin-docs/docs/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.6.0 New Features/Changes  Added support for Kubernetes v1.21 Added support for Red Hat Enterprise Linux (RHEL) 8.4 Added support for MKE 3.4.0 Added support for RKE v1.2.8 Added support for VMware Tanzu Added support for CSI Spec 1.3 Added Volume limit feature Added support for secret in YAML format Added support for Dynamic log level changes  Fixed Issues  The flag allowRWOMultiPodAccess: false is not applicable for Raw Block volumes and the driver allows the creation of multiple pods on the same node with RWO access mode.  Known Issues    Issue Workaround     Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node  csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it.    ","excerpt":"Release Notes - CSI Unity v1.6.0 New Features/Changes  Added support …","ref":"/storage-plugin-docs/docs/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity - Authentication failure. Check if you have created a secret with correct credentials   Installation of the driver on Kubernetes v1.20/1.21 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/1.21 requires v1 version of snapshot CRDs. If on Kubernetes 1.20/1.21 (v1 snapshots) install CRDs from v4.0.0, see point 6 here   fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes.   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod   On deleting pods sometimes the corresponding ‘volumeattachment’ will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI, or NFS) based storageclasses. This issue occurs in Kubernetes versions 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/storage-plugin-docs/docs/troubleshooting/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unitycommand. Pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and generally available (v1) in Kubernetes version 1.20.\nThe CSI Unity driver version 1.5 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI Unity 1.5 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for a Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver supports Raw Block Volumes from v1.4 onwards. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"This manifest creates a pod and attach newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (Attacher, Provisioner, Resizer and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in the similar way in node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feature, user can install driver by setting createStorageClassesWithTopology to true in the myvalues.yaml which will create default storage classes by adding topology keys (based on the arrays specified in myvalues.yaml) and with WaitForFirstConsumer binding mode.\nAnother option is the user can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example will match all nodes where driver has a connection to Unity array with array ID mentioned via Fiber Channel. Similarly by replacing fc with iscsi in the key will check for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work properly.\n For any additional information about topology, see the Kubernetes Topology documentation.\nSupport for Docker EE The CSI Driver for Dell EMC Unity supports Docker EE and deployment on clusters bootstrapped with UCP (Universal Control Plane).\n*UCP version 3.3.3 supports Kubernetes 1.18 and CSI driver can be installed on UCP 3.3 with Helm.\nThe installation process for the driver on such clusters remains the same as the installation process on upstream clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes in UCP backed clusters may run any of the OSs which we support with upstream clusters.\nSupport for SLES 15 SP2 The CSI Driver for Dell EMC Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with iSCSI protocol to work.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/storage-plugin-docs/v1/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Configure Docker service Install Helm v3 To use FC protocol, the host must be zoned with Unity array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed  Configure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for Unity.\nProcedure   Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows:\n[Service] ... MountFlags=shared   Restart the Docker service with following commands:\nsystemctl daemon-reload systemctl restart docker   Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with command git clone https://github.com/dell/csi-unity.git, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure “unity” namespace exists in kubernetes cluster. Use kubectl create namespace unity command to create the namespace, if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.json and myvalues.yaml file.\n  Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (unity-cert-0..unity-cert-n). Minimum value should be 1 false 1   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute false 15   controllerCount Controller replication count to maintain high availability yes 2   volumeNamePrefix String to prepend to any volumes created by the driver false csivol   snapNamePrefix String to prepend to any snapshot created by the driver false csi-snap   csiDebug To set the debug log policy for CSI driver false “false”   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   createStorageClassesWithTopology Flag to enable or disable topology. true false   allowRWOMultiPodAccess Flag to enable multiple pods use the same pvc on the same node with RWO access mode. false false   Storage Array List Following parameters is a list of parameters to provide multiple storage arrays     storageArrayList[i].name Name of the storage class to be defined. A suffix of ArrayId and protocol will be added to the name. No suffix will be added to default array. false unity   storageArrayList[i].isDefaultArray To handle the existing volumes created in csi-unity v1.0, 1.1 and 1.1.0.1. The user needs to provide “isDefaultArray”: true in secret.json. This entry should be present only for one array and that array will be marked default for existing volumes. false “false”   Storage Class parameters Following parameters are not present in values.yaml     storageArrayList[i].storageClass.storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   storageArrayList[i].storageClass.thinProvisioned To set volume thinProvisioned false “true”   storageArrayList[i].storageClass.isDataReductionEnabled To set volume data reduction false “false”   storageArrayList[i].storageClass.volumeTieringPolicy To set volume tiering policy false 0   storageArrayList[i].storageClass.FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   storageArrayList[i].storageClass.hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \"”   storageArrayList[i].storageClass.nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \"”   storageArrayList[i].storageClass.hostIoSize NFS related parameter. To set filesystem host IO Size. false “8192”   storageArrayList[i].storageClass.reclaimPolicy What should happen when a volume is removed false Delete   Snapshot Class parameters Following parameters are not present in values.yaml     storageArrayList[i] .snapshotClass.retentionDuration TO set snapshot retention duration. Format:“1:23:52:50” (number of days:hours:minutes:sec) false \"”    Note:\n  User should provide all boolean values with double quotes. This applicable only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  ‘createStorageClassesWithTopology’ key is applicable only in the helm based installation but not with the operator based installation. In operator based installation, however user can create custom storage class with topology related key/values.\n  User can create separate storage class (with topology related keys) by referring to existing default storageclasses.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods access the same pvc with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n  Example myvalues.yaml\ncsiDebug:\"true\"volumeNamePrefix :csivolsnapNamePrefix:csi-snapimagePullPolicy:AlwayscertSecretCount:1syncNodeInfoInterval:5controllerCount:2createStorageClassesWithTopology:trueallowRWOMultiPodAccess:falsestorageClassProtocols:- protocol:\"FC\"- protocol:\"iSCSI\"- protocol:\"NFS\"storageArrayList:- name:\"APM00******1\"isDefaultArray:\"true\"storageClass:storagePool:pool_1FsType:ext4nasServer:\"nas_1\"thinProvisioned:\"true\"isDataReductionEnabled:truehostIOLimitName:\"value_from_array\"tieringPolicy:\"2\"snapshotClass:retentionDuration:\"2:2:23:45\"- name:\"APM001******2\"storageClass:storagePool:pool_1reclaimPolicy:DeletehostIoSize:\"8192\"nasServer:\"nasserver_2\"  Create an empty secret with file helm/emptysecret.yaml file by running the kubectl create -f helm/emptysecret.yaml command.\n  Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Example: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nNote: “isDefaultArray” parameter in values.yaml and secret.json should match each other.\n  Setup for snapshots.\nThe Kubernetes Volume Snapshot feature is beta in Kubernetes v1.18 and v1.19 and move to GA in v1.20.\n The following section summarizes the changes in the GA  In order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster.\n  Install Snapshot CRDs:\nFor Kubernetes 1.18 and 1.19, Snapshot CRDs versioned 3.0.3 must be installed. CRDs\nFor Kubernetes 1.20 , Snapshot CRDs versioned 4.0.0 must be installed. CRDs\n  Install Snapshot Controller:\nFor Kubernetes 1.18 and 1.19, Snapshot Controller versioned 3.0.3 must be installed. Controller\nFor Kubernetes 1.20, Snapshot Controller versioned 4.0.0 must be installed.\nController\n    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.20 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.20 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results At the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.  Finally, the script creates storageclasses such as, “unity”. Additional storage classes can be created for different combinations of file system types and Unity storage pools.\nThe script also creates one or more volumesnapshotclasses based on number of arrays . “unity-snapclass” will be the volumesnapshotclass for default array. The output will be similar to following:\n[root@host ~]# kubectl get volumesnapshotclass NAME AGE unity-apm***********-snapclass 12m unity-snapclass 12m\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)  Note: “unity” is the namespace for helm based installation but namespace can be user defined in operator based installation.\nNote: User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to kubernetes secret size limitation.\nNote: Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\nStorage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and cannot be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting from CSI Unity v1.5, an annotation “helm.sh/resource-policy”: keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNote: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\nError: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command. Deleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n`kubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - `  Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/storage-plugin-docs/v1/installation/helm/unity/","title":"Unity"},{"body":"CSI Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Ex: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   GOUNITY_DEBUG To enable debug mode for gounity library No false   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot    StorageClass Parameters    Parameter Description Required Default     storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   thinProvisioned To set volume thinProvisioned false “true”   isDataReductionEnabled To set volume data reduction false “false”   volumeTieringPolicy To set volume tiering policy false 0   FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \"”   nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \"”   hostIoSize NFS related parameter. To set filesystem host IO Size. false “8192”   reclaimPolicy What should happen when a volume is removed false Delete    SnapshotClass parameters Following parameters are not present in values.yaml in the Helm based installer\n   Parameter Description Required Default     snapshotRetentionDuration TO set snapshot retention duration. Format:“1:23:52:50” (number of days:hours:minutes:sec) false \"”    Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v4replicas:2common:image:\"dellemc/csi-unity:v1.5.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\"true\"- name:X_CSI_UNITY_ALLOW_MULTI_POD_ACCESSvalue:\"false\"sideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\"]storageClass:- name:virt2016****-fcdefault:truereclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2016****\"protocol:\"FC\"- name:virt2017****-iscsireclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"- name:virt2017****-nfsreclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"NFS\"hostIoSize:\"8192\"nasServer:nas_1- name:virt2017****-iscsi-topologyreclaimPolicy:\"Delete\"allowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/virt2017****-iscsivalues:- \"true\"parameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"snapshotClass:- name:test-snapparameters:retentionDuration:\"\"","excerpt":"CSI Unity Pre-requisites Create secret to store Unity credentials …","ref":"/storage-plugin-docs/v1/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes, and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/storage-plugin-docs/v1/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.5.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Changed driver base image to UBI 8.x Added support for Red Hat Enterprise Linux (RHEL) 8.3 Qualified with Docker - UCP 3.3.5 Added support for SLES 15SP2  Fixed Issues  Raw-Block volume with accessmode RWX can be mounted to multiple nodes. PVC creation fails on a cluster with only NFS protocol enabled by adding topology keys for NFS protocol.  Known Issues    Issue Workaround     Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released we need to manually remove the node labels mentioned here https://github.com/dell/csi-unity#known-issues (Point 1)   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver should be restarted with the below command only if the topology-based storage classes are used. Otherwise, the driver will automatically detect the newly added or removed arrays https://github.com/dell//csi-unity#known-issues (Point 2)   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in cluster but on array it will still be present and marked for deletion. All the cloned pvc should be deleted in order to delete the source pvc from array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity --no-headers=true   On deleting pods sometimes the corresponding ‘volumeattachment’ will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI or NFS) based storageclasses. This issue occurs across kubernetes versions 1.18 and 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.   The flag allowRWOMultiPodAccess:false is not applicable for Raw Block volumes and the driver allows creation of multiple pods on the same node with RWO access mode. Workaround not necessary as this issue does not block any usecase.   Driver installation warning: “OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6” Ignore this warning and continue with the installation. v1.5.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI Unity v1.5.0 New Features/Changes  Added support …","ref":"/storage-plugin-docs/v1/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to Unity - Authentication failure. Check if you have created secret with correct credentials   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see point 6 here    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/storage-plugin-docs/v1/troubleshooting/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/sample.yaml\nThis command will create a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset will be created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unitycommand. Pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI Unity driver version 1.4 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by copy pasting the following commands (Copy entire thing in one shot and paste it in terminal):\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of the CSI Unity driver version 1.3 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver version 1.3 and later supports managing Raw Block volumes.\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:unitytestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:unityresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nNote: Raw block volume creation supports only for FC and iSCSI protocols\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"This manifest will create a pod and attach newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (Attacher, Provisioner, Resizer and Snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in the similar way in node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feature user can install driver by setting createStorageClassesWithTopology to true in the myvalues.yaml which will create default storage classes by adding topology keys (based on the arrays specified in myvalues.yaml) and with WaitForFirstConsumer binding mode.\nAnother option is the user can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example will match all nodes where driver has a connection to Unity array with array ID mentioned via Fiber Channel. Similarly by replacing fc with iscsi in the key will check for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work properly.\n For any additional information about topology, see the Kubernetes Topology documentation.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/storage-plugin-docs/v2/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes Configure Docker service Install Helm v3 To use FC protocol, host must be zoned with Unity array To use iSCSI and NFS protocol, iSCSI initiator and NFS utility packages need to be installed  Configure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for Unity.\nProcedure   Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows:\n[Service] ... MountFlags=shared   Restart the Docker service with systemctl daemon-reload and\nsystemctl daemon-reload systemctl restart docker   Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart, such as creating Custom Resource Definitions (CRDs), if needed. Make sure “unity” namespace exists in kubernetes cluster. Use kubectl create namespace unity command to create the namespace, if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.json and myvalues.yaml file.\n  Copy the csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (unity-cert-0..unity-cert-n). Minimum value should be 1 false 1   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute false 15   controllerCount Controller replication count to maintain high availability yes 2   volumeNamePrefix String to prepend to any volumes created by the driver false csivol   snapNamePrefix String to prepend to any snapshot created by the driver false csi-snap   csiDebug To set the debug log policy for CSI driver false “false”   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   createStorageClassesWithTopology Flag to enable or disable topology. true false   Storage Array List Following parameters is a list of parameters to provide multiple storage arrays     storageArrayList[i].name Name of the storage class to be defined. A suffix of ArrayId and protocol will be added to the name. No suffix will be added to default array. false unity   storageArrayList[i].isDefaultArray To handle the existing volumes created in csi-unity v1.0, 1.1 and 1.1.0.1. The user needs to provide “isDefaultArray”: true in secret.json. This entry should be present only for one array and that array will be marked default for existing volumes. false “false”   Storage Class parameters Following parameters are not present in values.yaml     storageArrayList[i].storageClass.storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   storageArrayList[i].storageClass.thinProvisioned To set volume thinProvisioned false “true”   storageArrayList[i].storageClass.isDataReductionEnabled To set volume data reduction false “false”   storageArrayList[i].storageClass.volumeTieringPolicy To set volume tiering policy false 0   storageArrayList[i].storageClass.FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   storageArrayList[i].storageClass.hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \"”   storageArrayList[i].storageClass.nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \"”   storageArrayList[i].storageClass.hostIoSize NFS related parameter. To set filesystem host IO Size. false “8192”   storageArrayList[i].storageClass.reclaimPolicy What should happen when a volume is removed false Delete   Snapshot Class parameters Following parameters are not present in values.yaml     storageArrayList[i] .snapshotClass.retentionDuration TO set snapshot retention duration. Format:“1:23:52:50” (number of days:hours:minutes:sec) false \"”    Note: User should provide all boolean values with double quotes. This applicable only for myvalues.yaml. Example: “true”/“false”.\nExample myvalues.yaml\ncsiDebug: \"true\" volumeNamePrefix : csivol snapNamePrefix: csi-snap imagePullPolicy: Always certSecretCount: 1 syncNodeInfoInterval: 5 controllerCount: 2 createStorageClassesWithTopology: true storageClassProtocols: - protocol: \"FC\" - protocol: \"iSCSI\" - protocol: \"NFS\" storageArrayList: - name: \"APM00******1\" isDefaultArray: \"true\" storageClass: storagePool: pool_1 FsType: ext4 nasServer: \"nas_1\" thinProvisioned: \"true\" isDataReductionEnabled: true hostIOLimitName: \"value_from_array\" tieringPolicy: \"2\" snapshotClass: retentionDuration: \"2:2:23:45\" - name: \"APM001******2\" storageClass: storagePool: pool_1 reclaimPolicy: Delete hostIoSize: \"8192\" nasServer: \"nasserver_2\"   Create an empty secret by navigating to helm folder that contains emptysecret.yaml file and running the kubectl create -f emptysecret.yaml command.\n  Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Example: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nNote: “isDefaultArray” parameter in values.yaml and secret.json should match each other.\n  Setup for snapshots\nThe Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17.\n  The following section summarizes the changes in the beta release.\nTo use the Kubernetes Volume Snapshot feature, ensure the following components have been deployed on your Kubernetes cluster.\n Install Snapshot Beta CRDs using the following command  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml   Volume snapshot controller\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2  After executing these commands, a snapshot-controller pod should be up and running.\n      Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation should emit messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.19 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.18 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying snapshot support | |--\u003e Verifying that beta snapshot CRDs are available Success | |--\u003e Verifying that beta snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u003e Verification Complete ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for statefulset unity-controller to be ready Success | |--\u003e Waiting for daemonset unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results: At the end of the script statefulset unity-controller and daemonset unity-node is ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n unity-controller-xxxx with 5/5 containers ready, and status displayed as Running.      Agent pods with 2/2 containers and the status displayed as Running.\nFinally, the script creates storageclasses such as, “unity”. Additional storage classes can be created for different combinations of file system types and Unity storage pools. The script also creates volumesnapshotclass “unity-snapclass”.\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’ kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)  Note: “unity” is the namespace for helm based installation but namespace can be user defined in operator based installation.\nNote: User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to kubernetes secret size limitation.\nNote: Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/storage-plugin-docs/v2/installation/helm/unity/","title":"Unity"},{"body":"Installing Unity CSI Driver via Operator The CSI Driver for Dell EMC Unity can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace run kubectl create namespace test-unity to create the a namespace called test-unity. It can be any user-defined name.\n  Create unity-creds\nCreate secret mentioned in Install csi-driver section. The secret should be created in user-defined namespace (test-unity, in this case)\n  Create certificate secrets\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n in the user-defined namespace (test-unity, in this case) Create certificate procedure explained in the link\nNote: ‘certSecretCount’ parameter is not required for operator. Based on secret name pattern (unity-certs-*) operator reads all the secrets. Secret name suffix should have 0 to N order to read the secrets. Secrets will not be considered, if any number missing in suffix.\nExample: If unity-certs-0, unity-certs-1, unity-certs-3 are present in the namespace, then only first two secrets are considered for SSL verification.\n  Create a CR (Custom Resource) for unity using the sample provided below\n  Create a new file csiunity.yaml by referring the following content. Replace the given sample values according to your environment. You can find may CRDs under deploy/crds folder when you install dell-csi-operator\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v3replicas:2common:image:\"dellemc/csi-unity:v1.4.0.000R\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\"true\"sideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\"]storageClass:- name:virt2016****-fcdefault:truereclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2016****\"protocol:\"FC\"- name:virt2017****-iscsireclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"- name:virt2017****-nfsreclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"NFS\"hostIoSize:\"8192\"nasServer:nas_1- name:virt2017****-iscsi-topologyreclaimPolicy:\"Delete\"allowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/virt2017****-iscsivalues:- \"true\"parameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"snapshotClass:- name:test-snapparameters:retentionDuration:\"\"  Execute the following command to create unity custom resource kubectl create -f csiunity.yaml. This command will deploy the csi-unity driver in the test-unity namespace.\n  Any deployment error can be found out by logging the operator pod which is in default namespace (example, kubectl logs dell-csi-operator-64c58559f6-cbgv7)\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   GOUNITY_DEBUG To enable debug mode for gounity library No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot      ","excerpt":"Installing Unity CSI Driver via Operator The CSI Driver for Dell EMC …","ref":"/storage-plugin-docs/v2/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes, and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/storage-plugin-docs/v2/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.4.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Controller high availability (multiple-controllers) Added support for Ubuntu 20.04 Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Docker EE 3.1 Added support for Topology Added support for ephemeral volumes Added raw-block volume creation capability for iSCSI and FC based volumes. Added support for Mount options Changed driver base image to UBI 8.x  Fixed Issues  Source NFS PVC cannot be deleted if cloned NFS PVC exists.  Known Issues    Issue Workaround     Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released we need to manually remove the node labels mentioned here https://github.com/dell/csi-unity#known-issues (Point 1)   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver should be restarted with the below command only if the topology-based storage classes are used. Otherwise, the driver will automatically detect the newly added or removed arrays https://github.com/dell//csi-unity#known-issues (Point 2)   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in cluster but on array it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity --no-headers=true   PVC creation fails on a cluster with only NFS protocol enabled with error failed to provision volume with StorageClass “unity-nfs”: error generating accessibility requirements: no available topology found. For NFS volume and pod creation to succeed there must be minimum one worker node with iSCSI support and with a successful iSCSI login in to the array. Following commands can be used as a reference (which should be executed on worker node with iSCSI support) iscsiadm -m discovery -t st -p \u003ciscsi-interface-ip\u003e iscsiadm -m node -T \u003ctarget-iqn\u003e -l   On deleting pods sometimes the corresponding volumeattachment will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI or NFS) based StorageClasses. This issue occurs across Kubernetes versions 1.18 and 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.    ","excerpt":"Release Notes - CSI Unity v1.4.0 New Features/Changes  Added support …","ref":"/storage-plugin-docs/v2/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to Unity - Authentication failure. Check if you’ve created secret with correct credentials    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/storage-plugin-docs/v2/troubleshooting/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity and PowerScale supports VMware Tanzu and deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and supervisor namespace.\nCurrently, VMware Tanzu with normal configuration(without NAT) supports Kubernetes 1.19 and 1.20. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5  Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","excerpt":"The CSI Driver for Dell EMC Unity and PowerScale supports VMware Tanzu …","ref":"/storage-plugin-docs/docs/partners/tanzu/","title":"VMware Tanzu"}]